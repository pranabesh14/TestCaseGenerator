"""
LLM Handler with Google Gemini API integration
"""
import os
from typing import List, Dict, Optional
import json
import time
import google.generativeai as genai
from logger import get_app_logger
from config import config

logger = get_app_logger("llm_handler")

class LLMHandler:
    """Handler for LLM interactions using Google Gemini"""
    
    def __init__(self):
        """Initialize LLM handler with Gemini"""
        self.api_key = config.GEMINI_API_KEY
        self.model_name = config.GEMINI_MODEL
        
        if not self.api_key:
            logger.error("Gemini API key not found!")
            raise ValueError("GEMINI_API_KEY not set in environment variables")
        
        # Configure Gemini
        genai.configure(api_key=self.api_key)
        
        # Initialize model
        self.model = genai.GenerativeModel(
            model_name=self.model_name,
            generation_config={
                "temperature": config.GEMINI_TEMPERATURE,
                "max_output_tokens": config.GEMINI_MAX_TOKENS,
            }
        )
        
        logger.info(f"‚úÖ LLM Handler initialized with Gemini model: {self.model_name}")
        
        # System prompt
        self.system_prompt = """You are a specialized AI assistant for test case generation ONLY.

Generate comprehensive test cases in valid JSON format.
Always return: [{"name": "test_name", "description": "desc", "code": "test code", "target": "function_name"}]"""

    def _make_request(self, prompt: str, context: str = "", max_retries: int = 3) -> str:
        """Make request to Gemini API with retry logic"""
        
        full_prompt = f"{self.system_prompt}\n\n"
        
        if context:
            full_prompt += f"CONTEXT:\n{context}\n\n"
        
        full_prompt += f"USER REQUEST:\n{prompt}\n\nRESPONSE:"
        
        logger.info(f"üì§ Making Gemini API request...")
        logger.debug(f"Prompt length: {len(full_prompt)} characters")
        
        retry_delay = 2
        
        for attempt in range(max_retries):
            try:
                start_time = time.time()
                
                response = self.model.generate_content(full_prompt)
                
                elapsed = time.time() - start_time
                
                if response.text:
                    logger.info(f"‚úÖ Gemini response received in {elapsed:.2f}s ({len(response.text)} chars)")
                    logger.debug(f"Response preview: {response.text[:200]}...")
                    return response.text
                else:
                    logger.warning(f"‚ö†Ô∏è Empty response from Gemini (attempt {attempt + 1})")
                    if attempt < max_retries - 1:
                        time.sleep(retry_delay)
                        retry_delay *= 2
                        continue
                    return "Error: Empty response from Gemini"
                    
            except Exception as e:
                logger.error(f"‚ùå Gemini API error (attempt {attempt + 1}/{max_retries}): {str(e)}")
                
                if "quota" in str(e).lower():
                    return "Error: API quota exceeded. Please check your Gemini API usage."
                elif "api key" in str(e).lower():
                    return "Error: Invalid API key. Please check your GEMINI_API_KEY."
                
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    retry_delay *= 2
                    continue
                
                return f"Error: {str(e)}"
        
        return "Error: Max retries exceeded"
    
    def generate_tests_for_chunk(
        self,
        chunk: Dict,
        test_type: str,
        file_name: str = ""
    ) -> List[Dict]:
        """Generate tests for a specific code chunk"""
        
        logger.info(f"üîß Generating {test_type} for chunk: {chunk['name']} ({chunk['type']})")
        
        chunk_code = chunk['code']
        chunk_name = chunk['name']
        chunk_type = chunk['type']
        
        # Build prompt based on test type
        if test_type == "Unit Test":
            prompt = self._build_unit_test_prompt(chunk_code, chunk_name, chunk_type)
        elif test_type == "Functional Test":
            prompt = self._build_functional_test_prompt(chunk_code, chunk_name, chunk_type)
        elif test_type == "Regression Test":
            prompt = self._build_regression_test_prompt(chunk_code, chunk_name, chunk_type)
        else:
            prompt = self._build_generic_test_prompt(chunk_code, chunk_name, test_type)
        
        response = self._make_request(prompt)
        
        if response.startswith("Error:"):
            logger.error(f"‚ùå LLM error for {chunk_name}: {response}")
            return self._generate_fallback_tests(chunk, test_type, file_name)
        
        tests = self._parse_test_response(response, test_type)
        
        # Add metadata
        for test in tests:
            test['file'] = file_name
            test['chunk_name'] = chunk_name
            test['chunk_type'] = chunk_type
            test['line_start'] = chunk.get('line_start', 0)
            test['line_end'] = chunk.get('line_end', 0)
        
        logger.info(f"‚úÖ Generated {len(tests)} tests for chunk {chunk_name}")
        return tests
    
    def _build_unit_test_prompt(self, code: str, chunk_name: str, chunk_type: str) -> str:
        """Build prompt for unit test generation"""
        
        if chunk_type == 'function':
            prompt = f"""Generate unit tests for this function.

FUNCTION: {chunk_name}
```
{code}
```

Generate 2-3 unit tests covering:
1. Normal/happy path
2. Edge cases
3. Error conditions if applicable

Return ONLY JSON array:
[{{"name": "test_name", "description": "what it tests", "code": "complete test function", "target": "{chunk_name}"}}]"""
        
        elif chunk_type == 'class':
            prompt = f"""Generate unit tests for this class.

CLASS: {chunk_name}
```
{code}
```

Generate 3-5 unit tests covering different methods and scenarios.

Return ONLY JSON array:
[{{"name": "test_name", "description": "what it tests", "code": "complete test function", "target": "method_name"}}]"""
        
        else:
            prompt = f"""Generate unit tests for this code.

CODE:
```
{code}
```

Generate 2-4 unit tests.

Return ONLY JSON array:
[{{"name": "test_name", "description": "what it tests", "code": "complete test function", "target": "general"}}]"""
        
        return prompt
    
    def _build_functional_test_prompt(self, code: str, chunk_name: str, chunk_type: str) -> str:
        """Build prompt for functional test generation"""
        
        prompt = f"""Generate functional test cases for this code in PROFESSIONAL TEST CASE FORMAT.

{chunk_type.upper()}: {chunk_name}
```
{code}
```

Generate 3-5 functional test cases covering:
1. Valid/happy path scenarios
2. Invalid input scenarios
3. Edge cases
4. Error handling
5. Integration scenarios

Return test cases in this EXACT JSON format:
[
  {{
    "test_case_id": "TC-XXX-01",
    "description": "Brief description of what is being tested",
    "steps": "Step 1: Do something\\nStep 2: Do something else\\nStep 3: Verify result",
    "expected_result": "Detailed expected outcome of the test"
  }}
]

Return ONLY the JSON array, no other text."""
        
        return prompt
    
    def _build_regression_test_prompt(self, code: str, chunk_name: str, chunk_type: str) -> str:
        """Build prompt for regression test generation"""
        
        prompt = f"""Generate regression tests for this code.

{chunk_type.upper()}: {chunk_name}
```
{code}
```

Generate 2-3 regression tests that ensure:
1. Existing functionality is preserved
2. No breaking changes
3. Backward compatibility

Return ONLY JSON array:
[{{"name": "test_name", "description": "what it tests", "code": "complete test function", "target": "{chunk_name}"}}]"""
        
        return prompt
    
    def _build_generic_test_prompt(self, code: str, chunk_name: str, test_type: str) -> str:
        """Build generic test prompt"""
        
        prompt = f"""Generate {test_type}s for this code.

CODE: {chunk_name}
```
{code}
```

Generate 2-3 {test_type.lower()}s.

Return ONLY JSON array:
[{{"name": "test_name", "description": "what it tests", "code": "complete test function", "target": "{chunk_name}"}}]"""
        
        return prompt
    
    def _parse_test_response(self, response: str, test_type: str) -> List[Dict]:
        """Parse LLM response into structured test cases"""
        
        if not response or response.startswith("Error:"):
            logger.warning(f"‚ö†Ô∏è Empty or error response for {test_type}")
            return []
        
        try:
            # Try to extract JSON from response
            start_idx = response.find('[')
            end_idx = response.rfind(']') + 1
            
            if start_idx != -1 and end_idx > start_idx:
                json_str = response[start_idx:end_idx]
                tests = json.loads(json_str)
                
                # Validate and structure tests
                valid_tests = []
                for i, test in enumerate(tests):
                    if isinstance(test, dict):
                        if 'test_case_id' in test and test_type == 'Functional Test':
                            # Professional format
                            valid_test = {
                                'name': test.get('test_case_id', f'TC-FN-{i+1:02d}'),
                                'test_case_id': test.get('test_case_id', f'TC-FN-{i+1:02d}'),
                                'description': test.get('description', 'Test case'),
                                'steps': test.get('steps', 'No steps provided'),
                                'expected_result': test.get('expected_result', 'No expected result'),
                                'type': test_type,
                                'target': test.get('target', 'general'),
                                'format': 'professional'
                            }
                        else:
                            # Code format
                            valid_test = {
                                'name': test.get('name', f'{test_type.lower().replace(" ", "_")}_{i+1}'),
                                'description': test.get('description', 'Test case'),
                                'code': test.get('code', '# No code generated'),
                                'type': test_type,
                                'target': test.get('target', 'general'),
                                'format': 'code'
                            }
                        valid_tests.append(valid_test)
                
                if valid_tests:
                    logger.info(f"‚úÖ Parsed {len(valid_tests)} tests from JSON")
                    return valid_tests
            
            # Fallback to plain text parsing
            logger.info("‚ö†Ô∏è Attempting plain text parsing")
            return self._parse_plain_text_tests(response, test_type)
                
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå JSON decode error: {e}")
            return self._parse_plain_text_tests(response, test_type)
        except Exception as e:
            logger.error(f"‚ùå Error parsing test response: {e}", exc_info=True)
            return []
    
    def _parse_plain_text_tests(self, response: str, test_type: str) -> List[Dict]:
        """Parse plain text response into test cases"""
        import re
        
        code_blocks = re.findall(r'```(?:python)?\n(.*?)```', response, re.DOTALL)
        
        if code_blocks:
            logger.info(f"üìù Found {len(code_blocks)} code blocks")
            tests = []
            for i, code in enumerate(code_blocks, 1):
                tests.append({
                    'name': f'{test_type.lower().replace(" ", "_")}_{i}',
                    'description': f'Generated {test_type}',
                    'code': code.strip(),
                    'type': test_type,
                    'target': 'general',
                    'format': 'code'
                })
            return tests
        
        return []
    
    def _generate_fallback_tests(self, chunk: Dict, test_type: str, file_name: str) -> List[Dict]:
        """Generate fallback tests when LLM fails"""
        logger.warning(f"‚ö†Ô∏è Generating fallback tests for {chunk['name']}")
        
        chunk_name = chunk['name']
        chunk_type = chunk['type']
        
        if test_type == "Functional Test":
            return [{
                'name': f'TC-FN-01',
                'test_case_id': f'TC-FN-01',
                'description': f'Functional test for {chunk_name} ({chunk_type})',
                'steps': f'Step 1: Initialize {chunk_name}\nStep 2: Execute main functionality\nStep 3: Verify expected behavior',
                'expected_result': f'{chunk_name} should execute successfully and return expected output',
                'type': test_type,
                'target': chunk_name,
                'file': file_name,
                'fallback': True,
                'format': 'professional'
            }]
        else:
            test_name = f"test_{chunk_name}_{test_type.lower().replace(' ', '_')}"
            return [{
                'name': test_name,
                'description': f'{test_type} for {chunk_name} (fallback)',
                'code': f"""def {test_name}():
    \"\"\"
    {test_type} for {chunk_name} ({chunk_type})
    File: {file_name}
    
    TODO: LLM generation failed. Implement test manually.
    \"\"\"
    pass""",
                'type': test_type,
                'target': chunk_name,
                'file': file_name,
                'fallback': True,
                'format': 'code'
            }]
    
    def generate_chat_response(
        self,
        user_message: str,
        context: str = "",
        chat_history: List[Dict] = None
    ) -> str:
        """Generate response for chat interface"""
        
        logger.info(f"üí¨ Generating chat response for: {user_message[:50]}...")
        
        # Build conversation context
        history_text = ""
        if chat_history:
            history_text = "\n".join([
                f"{msg['role'].upper()}: {msg['content']}"
                for msg in chat_history[-5:]  # Last 5 messages
            ])
        
        prompt = f"""You are a helpful AI assistant for test case generation.

Previous conversation:
{history_text}

Context (if any):
{context}

Current question: {user_message}

Provide a helpful, concise response focused on test case generation, code analysis, or testing strategies."""
        
        response = self._make_request(prompt)
        logger.info(f"‚úÖ Chat response generated ({len(response)} chars)")
        
        return response