{
  "code_documents": {
    "e1ff932aca5433c0e1544c59d1e72036": {
      "filename": "config.py",
      "code": "# config.py\r\nimport os\r\nfrom typing import Dict, Any\r\nfrom pathlib import Path\r\n\r\nclass MedicalAssistantConfig:\r\n    \"\"\"Configuration settings for the Multimodal Medical Assistant\"\"\"\r\n    \r\n    # Base directories\r\n    BASE_DIR = Path(__file__).parent\r\n    DATA_DIR = BASE_DIR / \"data\"\r\n    MODELS_DIR = BASE_DIR / \"models\"\r\n    LOGS_DIR = BASE_DIR / \"logs\"\r\n    TEMP_DIR = BASE_DIR / \"temp\"\r\n    \r\n    # Model configurations\r\n    MODELS = {\r\n        'llama': {\r\n            'base_url': os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434'),\r\n            'model_name': os.getenv('LLAMA_MODEL', 'llama2'),\r\n            'timeout': 120,\r\n            'max_tokens': 2000,\r\n            'temperature': 0.3\r\n        },\r\n        'biomedclip': {\r\n            'model_name': os.getenv('BIOMEDCLIP_MODEL', 'openai/clip-vit-base-patch32'),\r\n            'cache_dir': MODELS_DIR / \"biomedclip\",\r\n            'device': 'auto'  # 'auto', 'cpu', 'cuda'\r\n        },\r\n        'biobert': {\r\n            'model_name': os.getenv('BIOBERT_MODEL', 'sentence-transformers/all-MiniLM-L6-v2'),\r\n            'cache_dir': MODELS_DIR / \"biobert\",\r\n            'max_length': 512\r\n        },\r\n        'clinical_bert': {\r\n            'model_name': os.getenv('CLINICAL_BERT_MODEL', 'emilyalsentzer/Bio_ClinicalBERT'),\r\n            'cache_dir': MODELS_DIR / \"clinical_bert\",\r\n            'max_length': 512\r\n        },\r\n        'medical_ner': {\r\n            'model_name': os.getenv('MEDICAL_NER_MODEL', 'd4data/biomedical-ner-all'),\r\n            'cache_dir': MODELS_DIR / \"medical_ner\"\r\n        }\r\n    }\r\n    \r\n    # File processing settings\r\n    FILE_PROCESSING = {\r\n        'max_file_size': {\r\n            'image': 50 * 1024 * 1024,  # 50MB\r\n            'document': 100 * 1024 * 1024,  # 100MB\r\n            'dicom': 200 * 1024 * 1024  # 200MB\r\n        },\r\n        'supported_formats': {\r\n            'images': ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.dcm'],\r\n            'documents': ['.pdf', '.txt', '.doc', '.docx']\r\n        },\r\n        'image_preprocessing': {\r\n            'resize_dimensions': (224, 224),\r\n            'normalize': True,\r\n            'enhance_contrast': True\r\n        },\r\n        'temp_file_cleanup_hours': 24\r\n    }\r\n    \r\n    # RAG system settings\r\n    RAG_SETTINGS = {\r\n        'chunk_size': 1000,\r\n        'chunk_overlap': 200,\r\n        'max_results': 10,\r\n        'similarity_threshold': 0.7,\r\n        'use_hybrid_search': True,\r\n        'vector_store_type': 'faiss',  \r\n        'embedding_model': 'sentence-transformers/all-MiniLM-L6-v2'\r\n    }\r\n    \r\n    # Privacy and de-identification settings\r\n    PRIVACY_SETTINGS = {\r\n        'auto_deidentify': True,\r\n        'preserve_clinical_context': True,\r\n        'phi_detection_confidence': 0.8,\r\n        'date_shift_range_days': 365,\r\n        'audit_logging': True,\r\n        'cache_replacements': True\r\n    }\r\n    \r\n    # UI/UX settings\r\n    STREAMLIT_CONFIG = {\r\n        'page_title': 'Multimodal Medical Assistant',\r\n        'page_icon': '\ud83c\udfe5',\r\n        'layout': 'wide',\r\n        'sidebar_state': 'expanded',\r\n        'theme': 'light',\r\n        'max_upload_size': 200  # MB\r\n    }\r\n    \r\n    # Medical analysis settings\r\n    MEDICAL_ANALYSIS = {\r\n        'clinical_significance_threshold': 0.6,\r\n        'urgency_keywords': [\r\n            'emergency', 'urgent', 'critical', 'severe', 'acute', \r\n            'stat', 'immediate', 'life-threatening', 'cardiac arrest'\r\n        ],\r\n        'medical_specialties': [\r\n            'cardiology', 'radiology', 'pathology', 'emergency',\r\n            'internal_medicine', 'surgery', 'neurology', 'oncology'\r\n        ],\r\n        'confidence_thresholds': {\r\n            'high': 0.8,\r\n            'medium': 0.6,\r\n            'low': 0.4\r\n        }\r\n    }\r\n    \r\n    # Logging configuration\r\n    LOGGING = {\r\n        'level': os.getenv('LOG_LEVEL', 'INFO'),\r\n        'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\r\n        'file_rotation': 'midnight',\r\n        'backup_count': 7,\r\n        'max_file_size': '10MB'\r\n    }\r\n    \r\n    # Database settings (for future expansion)\r\n    DATABASE = {\r\n        'type': os.getenv('DB_TYPE', 'sqlite'),\r\n        'host': os.getenv('DB_HOST', 'localhost'),\r\n        'port': os.getenv('DB_PORT', '5432'),\r\n        'name': os.getenv('DB_NAME', 'medical_assistant'),\r\n        'user': os.getenv('DB_USER', ''),\r\n        'password': os.getenv('DB_PASSWORD', ''),\r\n        'connection_timeout': 30\r\n    }\r\n    \r\n    # API settings (for future API deployment)\r\n    API_SETTINGS = {\r\n        'host': os.getenv('API_HOST', '0.0.0.0'),\r\n        'port': int(os.getenv('API_PORT', '8000')),\r\n        'debug': os.getenv('DEBUG', 'False').lower() == 'true',\r\n        'cors_origins': os.getenv('CORS_ORIGINS', '*').split(','),\r\n        'rate_limit': {\r\n            'requests_per_minute': 60,\r\n            'burst_limit': 100\r\n        }\r\n    }\r\n    \r\n    # Security settings\r\n    SECURITY = {\r\n        'secret_key': os.getenv('SECRET_KEY', 'your-secret-key-here'),\r\n        'encryption_algorithm': 'AES-256-GCM',\r\n        'session_timeout_minutes': 60,\r\n        'max_login_attempts': 3,\r\n        'require_https': os.getenv('REQUIRE_HTTPS', 'False').lower() == 'true'\r\n    }\r\n    \r\n    # Performance settings\r\n    PERFORMANCE = {\r\n        'max_concurrent_requests': 10,\r\n        'request_timeout_seconds': 300,\r\n        'memory_limit_mb': 4096,\r\n        'gpu_memory_fraction': 0.8,\r\n        'batch_size': {\r\n            'text_processing': 32,\r\n            'image_processing': 8,\r\n            'embedding_generation': 16\r\n        }\r\n    }\r\n    \r\n    @classmethod\r\n    def create_directories(cls):\r\n        \"\"\"Create necessary directories\"\"\"\r\n        directories = [\r\n            cls.DATA_DIR,\r\n            cls.MODELS_DIR,\r\n            cls.LOGS_DIR,\r\n            cls.TEMP_DIR,\r\n            cls.DATA_DIR / \"documents\",\r\n            cls.DATA_DIR / \"embeddings\",\r\n            cls.DATA_DIR / \"medical_kb\",\r\n            cls.LOGS_DIR / \"application\",\r\n            cls.LOGS_DIR / \"audit\"\r\n        ]\r\n        \r\n        for directory in directories:\r\n            directory.mkdir(parents=True, exist_ok=True)\r\n    \r\n    @classmethod\r\n    def get_model_config(cls, model_name: str) -> Dict[str, Any]:\r\n        \"\"\"Get configuration for a specific model\"\"\"\r\n        return cls.MODELS.get(model_name, {})\r\n    \r\n    @classmethod\r\n    def get_file_config(cls) -> Dict[str, Any]:\r\n        \"\"\"Get file processing configuration\"\"\"\r\n        return cls.FILE_PROCESSING\r\n    \r\n    @classmethod\r\n    def get_rag_config(cls) -> Dict[str, Any]:\r\n        \"\"\"Get RAG system configuration\"\"\"\r\n        return cls.RAG_SETTINGS\r\n    \r\n    @classmethod\r\n    def get_privacy_config(cls) -> Dict[str, Any]:\r\n        \"\"\"Get privacy and de-identification configuration\"\"\"\r\n        return cls.PRIVACY_SETTINGS\r\n    \r\n    @classmethod\r\n    def validate_config(cls) -> Dict[str, Any]:\r\n        \"\"\"Validate configuration settings\"\"\"\r\n        validation_results = {\r\n            'valid': True,\r\n            'errors': [],\r\n            'warnings': []\r\n        }\r\n        \r\n        # Check if required environment variables are set\r\n        required_env_vars = []\r\n        for var in required_env_vars:\r\n            if not os.getenv(var):\r\n                validation_results['errors'].append(f\"Required environment variable {var} not set\")\r\n                validation_results['valid'] = False\r\n        \r\n        # Check if directories can be created\r\n        try:\r\n            cls.create_directories()\r\n        except Exception as e:\r\n            validation_results['errors'].append(f\"Cannot create directories: {str(e)}\")\r\n            validation_results['valid'] = False\r\n        \r\n        # Validate file size limits\r\n        for file_type, size in cls.FILE_PROCESSING['max_file_size'].items():\r\n            if size <= 0:\r\n                validation_results['warnings'].append(f\"Invalid file size limit for {file_type}: {size}\")\r\n        \r\n        # Check Ollama connection (if configured)\r\n        try:\r\n            import requests\r\n            ollama_url = cls.MODELS['llama']['base_url']\r\n            response = requests.get(f\"{ollama_url}/api/tags\", timeout=5)\r\n            if response.status_code != 200:\r\n                validation_results['warnings'].append(\"Ollama service not accessible\")\r\n        except Exception:\r\n            validation_results['warnings'].append(\"Cannot verify Ollama connection\")\r\n        \r\n        return validation_results\r\n    \r\n    @classmethod\r\n    def get_environment_info(cls) -> Dict[str, Any]:\r\n        \"\"\"Get environment information\"\"\"\r\n        import platform\r\n        import torch\r\n        \r\n        return {\r\n            'platform': platform.platform(),\r\n            'python_version': platform.python_version(),\r\n            'torch_version': torch.__version__,\r\n            'cuda_available': torch.cuda.is_available(),\r\n            'cuda_device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,\r\n            'base_directory': str(cls.BASE_DIR),\r\n            'config_valid': cls.validate_config()['valid']\r\n        }\r\n\r\n# Create directories on import\r\nMedicalAssistantConfig.create_directories()",
      "language": "python",
      "functions": [
        {
          "name": "create_directories",
          "line": 169,
          "args": [
            "cls"
          ],
          "docstring": "Create necessary directories",
          "decorators": [
            "classmethod"
          ]
        },
        {
          "name": "get_model_config",
          "line": 187,
          "args": [
            "cls",
            "model_name"
          ],
          "docstring": "Get configuration for a specific model",
          "decorators": [
            "classmethod"
          ]
        },
        {
          "name": "get_file_config",
          "line": 192,
          "args": [
            "cls"
          ],
          "docstring": "Get file processing configuration",
          "decorators": [
            "classmethod"
          ]
        },
        {
          "name": "get_rag_config",
          "line": 197,
          "args": [
            "cls"
          ],
          "docstring": "Get RAG system configuration",
          "decorators": [
            "classmethod"
          ]
        },
        {
          "name": "get_privacy_config",
          "line": 202,
          "args": [
            "cls"
          ],
          "docstring": "Get privacy and de-identification configuration",
          "decorators": [
            "classmethod"
          ]
        },
        {
          "name": "validate_config",
          "line": 207,
          "args": [
            "cls"
          ],
          "docstring": "Validate configuration settings",
          "decorators": [
            "classmethod"
          ]
        },
        {
          "name": "get_environment_info",
          "line": 247,
          "args": [
            "cls"
          ],
          "docstring": "Get environment information",
          "decorators": [
            "classmethod"
          ]
        }
      ],
      "classes": [
        {
          "name": "MedicalAssistantConfig",
          "line": 6,
          "methods": [
            "create_directories",
            "get_model_config",
            "get_file_config",
            "get_rag_config",
            "get_privacy_config",
            "validate_config",
            "get_environment_info"
          ],
          "docstring": "Configuration settings for the Multimodal Medical Assistant",
          "bases": []
        }
      ],
      "imports": [
        "os",
        "typing.Dict",
        "typing.Any",
        "pathlib.Path",
        "platform",
        "torch",
        "requests"
      ],
      "complexity": "medium",
      "timestamp": "2025-10-20T09:01:08.426542"
    },
    "22b3c9dae6596bc00887f277c24aa3ce": {
      "filename": "main.py",
      "code": "import streamlit as st\r\nimport os\r\nimport tempfile\r\nimport pandas as pd\r\nfrom pathlib import Path\r\nimport time\r\nimport json\r\nfrom datetime import datetime\r\n\r\n# Import custom modules\r\nfrom src.models.llama_processor import LlamaProcessor\r\nfrom src.models.image_processor import MedicalImageProcessor\r\nfrom src.models.text_processor import MedicalTextProcessor\r\nfrom src.rag.retrieval_system import MedicalRAGSystem\r\nfrom src.utils.privacy_utils import DataDeidentifier\r\nfrom src.utils.file_handler import FileHandler\r\n\r\nclass MultimodalMedicalAssistant:\r\n    def __init__(self):\r\n        self.initialize_components()\r\n        \r\n    def initialize_components(self):\r\n        \"\"\"Initialize all the required components\"\"\"\r\n        try:\r\n            # Initialize processors\r\n            self.llama_processor = LlamaProcessor()\r\n            self.image_processor = MedicalImageProcessor()\r\n            self.text_processor = MedicalTextProcessor()\r\n            self.rag_system = MedicalRAGSystem()\r\n            self.deidentifier = DataDeidentifier()\r\n            self.file_handler = FileHandler()\r\n            \r\n            # Initialize session state\r\n            if 'medical_data' not in st.session_state:\r\n                st.session_state.medical_data = {\r\n                    'uploaded_files': [],\r\n                    'processed_images': [],\r\n                    'processed_texts': [],\r\n                    'conversation_history': []\r\n                }\r\n        except Exception as e:\r\n            st.error(f\"Error initializing components: {str(e)}\")\r\n\r\ndef main():\r\n    st.set_page_config(\r\n        page_title=\"Multimodal Medical Assistant\",\r\n        page_icon=\"\ud83c\udfe5\",\r\n        layout=\"wide\",\r\n        initial_sidebar_state=\"expanded\"\r\n    )\r\n    \r\n    # Initialize the assistant\r\n    if 'assistant' not in st.session_state:\r\n        st.session_state.assistant = MultimodalMedicalAssistant()\r\n    \r\n    assistant = st.session_state.assistant\r\n    \r\n    # Main header\r\n    st.title(\"Multimodal Medical Assistant\")\r\n    st.markdown(\"**AI-powered clinical decision support with multimodal data processing**\")\r\n    \r\n    # Sidebar for navigation\r\n    with st.sidebar:\r\n        st.header(\"Navigation\")\r\n        tab = st.selectbox(\r\n            \"Choose Function\",\r\n            [\"Medical Chat\", \"System Status\"]\r\n        )\r\n        \r\n        # Add information about the simplified interface\r\n        st.markdown(\"---\")\r\n        st.markdown(\"### \ud83c\udfe5 Medical Chat Features\")\r\n        st.markdown(\"\"\"\r\n        **The Medical Chat now handles everything:**\r\n        \r\n        \ud83e\udd16 **Intelligent Analysis**\r\n        - Text medical questions\r\n        - X-ray image analysis  \r\n        - Skin lesion evaluation\r\n        - Medical report review\r\n        - Cross-modal reasoning\r\n        \r\n        \ud83e\udde0 **Smart Features**\r\n        - Auto-detects content type\r\n        - Routes to appropriate AI models\r\n        - Context-aware responses\r\n        - Topic change detection\r\n        \r\n        \ud83d\udcac **Simple to Use**\r\n        - Single chat interface\r\n        - Upload any medical file\r\n        - Ask any medical question\r\n        - Get comprehensive answers\r\n        \"\"\")\r\n        \r\n        st.markdown(\"---\")\r\n        st.markdown(\"### \u2699\ufe0f System Status\")\r\n        st.markdown(\"\"\"\r\n        Check AI model availability and system performance.\r\n        \"\"\")\r\n    \r\n    if tab == \"Medical Chat\":\r\n        render_chat_tab(assistant)\r\n    elif tab == \"System Status\":\r\n        render_status_tab(assistant)\r\n\r\ndef render_data_upload_tab(assistant):\r\n    \"\"\"Render the data upload interface\"\"\"\r\n    st.header(\"Medical Data Upload & Processing\")\r\n    \r\n    col1, col2 = st.columns(2)\r\n    \r\n    with col1:\r\n        st.subheader(\"Upload Medical Images\")\r\n        uploaded_images = st.file_uploader(\r\n            \"Upload X-rays, MRIs, CT scans (DICOM, PNG, JPEG)\",\r\n            type=['dcm', 'png', 'jpg', 'jpeg'],\r\n            accept_multiple_files=True,\r\n            key=\"image_uploader\"\r\n        )\r\n        \r\n        if uploaded_images:\r\n            for uploaded_file in uploaded_images:\r\n                if st.button(f\"Process {uploaded_file.name}\", key=f\"process_img_{uploaded_file.name}\"):\r\n                    with st.spinner(\"Processing medical image...\"):\r\n                        try:\r\n                            # Save uploaded file temporarily\r\n                            temp_path = assistant.file_handler.save_temp_file(uploaded_file)\r\n                            \r\n                            # Process the image\r\n                            result = assistant.image_processor.process_medical_image(temp_path)\r\n                            \r\n                            # Store in session state\r\n                            st.session_state.medical_data['processed_images'].append({\r\n                                'filename': uploaded_file.name,\r\n                                'analysis': result,\r\n                                'timestamp': datetime.now()\r\n                            })\r\n                            \r\n                            st.success(f\"Successfully processed {uploaded_file.name}\")\r\n                            st.json(result)\r\n                            \r\n                        except Exception as e:\r\n                            st.error(f\"Error processing image: {str(e)}\")\r\n    \r\n    with col2:\r\n        st.subheader(\"Upload Medical Documents\")\r\n        uploaded_docs = st.file_uploader(\r\n            \"Upload EHRs, discharge summaries, clinical notes (PDF, TXT)\",\r\n            type=['pdf', 'txt', 'doc', 'docx'],\r\n            accept_multiple_files=True,\r\n            key=\"doc_uploader\"\r\n        )\r\n        \r\n        if uploaded_docs:\r\n            for uploaded_file in uploaded_docs:\r\n                if st.button(f\"Process {uploaded_file.name}\", key=f\"process_doc_{uploaded_file.name}\"):\r\n                    with st.spinner(\"Processing medical document...\"):\r\n                        try:\r\n                            # Extract text from document\r\n                            text_content = assistant.file_handler.extract_text(uploaded_file)\r\n                            \r\n                            # De-identify the text\r\n                            deidentified_text = assistant.deidentifier.deidentify_text(text_content)\r\n                            \r\n                            # Process with medical NLP\r\n                            processed_result = assistant.text_processor.process_medical_text(deidentified_text)\r\n                            \r\n                            # Add to RAG system\r\n                            assistant.rag_system.add_document(deidentified_text, uploaded_file.name)\r\n                            \r\n                            # Store in session state\r\n                            st.session_state.medical_data['processed_texts'].append({\r\n                                'filename': uploaded_file.name,\r\n                                'content': deidentified_text,\r\n                                'analysis': processed_result,\r\n                                'timestamp': datetime.now()\r\n                            })\r\n                            \r\n                            st.success(f\"Successfully processed {uploaded_file.name}\")\r\n                            st.json(processed_result)\r\n                            \r\n                        except Exception as e:\r\n                            st.error(f\"Error processing document: {str(e)}\")\r\n\r\ndef render_analysis_tab(assistant):\r\n    \"\"\"Render the cross-modal analysis interface\"\"\"\r\n    st.header(\"Cross-Modal Medical Analysis\")\r\n    \r\n    # Check if we have both images and text data\r\n    if not st.session_state.medical_data['processed_images']:\r\n        st.warning(\"Please upload and process medical images first.\")\r\n        return\r\n    \r\n    if not st.session_state.medical_data['processed_texts']:\r\n        st.warning(\"Please upload and process medical documents first.\")\r\n        return\r\n    \r\n    st.subheader(\"Available Data\")\r\n    \r\n    col1, col2 = st.columns(2)\r\n    \r\n    with col1:\r\n        st.write(\"**Processed Images:**\")\r\n        for img in st.session_state.medical_data['processed_images']:\r\n            st.write(f\"- {img['filename']} ({img['timestamp'].strftime('%H:%M:%S')})\")\r\n    \r\n    with col2:\r\n        st.write(\"**Processed Documents:**\")\r\n        for doc in st.session_state.medical_data['processed_texts']:\r\n            st.write(f\"- {doc['filename']} ({doc['timestamp'].strftime('%H:%M:%S')})\")\r\n    \r\n    st.subheader(\"Cross-Modal Query\")\r\n    \r\n    # Predefined query templates\r\n    query_templates = [\r\n        \"Interpret this X-ray and correlate with patient symptoms\",\r\n        \"Compare imaging findings with clinical notes\",\r\n        \"Provide diagnostic suggestions based on multimodal evidence\",\r\n        \"Summarize all available patient data\",\r\n        \"Custom query\"\r\n    ]\r\n    \r\n    selected_template = st.selectbox(\"Select query template:\", query_templates)\r\n    \r\n    if selected_template == \"Custom query\":\r\n        user_query = st.text_area(\"Enter your medical query:\")\r\n    else:\r\n        user_query = selected_template\r\n        st.write(f\"Query: {user_query}\")\r\n    \r\n    if st.button(\"Analyze\", key=\"cross_modal_analyze\"):\r\n        if user_query:\r\n            with st.spinner(\"Performing cross-modal analysis...\"):\r\n                try:\r\n                    # Gather all data for analysis\r\n                    multimodal_data = {\r\n                        'images': st.session_state.medical_data['processed_images'],\r\n                        'texts': st.session_state.medical_data['processed_texts'],\r\n                        'query': user_query\r\n                    }\r\n                    \r\n                    # Perform cross-modal analysis\r\n                    result = assistant.llama_processor.cross_modal_analysis(multimodal_data)\r\n                    \r\n                    st.subheader(\"Analysis Results\")\r\n                    st.write(result['response'])\r\n                    \r\n                    if 'evidence' in result:\r\n                        st.subheader(\"Supporting Evidence\")\r\n                        for evidence in result['evidence']:\r\n                            st.write(f\"- **{evidence['type']}**: {evidence['content']}\")\r\n                    \r\n                    if 'recommendations' in result:\r\n                        st.subheader(\"Clinical Recommendations\")\r\n                        for rec in result['recommendations']:\r\n                            st.write(f\"- {rec}\")\r\n                            \r\n                except Exception as e:\r\n                    st.error(f\"Error during analysis: {str(e)}\")\r\n\r\ndef render_rag_tab(assistant):\r\n    \"\"\"Render the RAG query interface\"\"\"\r\n    st.header(\"Medical Knowledge Retrieval (RAG)\")\r\n    \r\n    st.subheader(\"Query Medical Knowledge Base\")\r\n    \r\n    # Query input\r\n    query = st.text_input(\"Enter your medical question:\")\r\n    \r\n    # Query type selection\r\n    query_type = st.selectbox(\r\n        \"Query Type:\",\r\n        [\"General Medical\", \"Diagnostic\", \"Treatment\", \"Drug Information\", \"Procedure\"]\r\n    )\r\n    \r\n    # Advanced options\r\n    with st.expander(\"Advanced Options\"):\r\n        max_results = st.slider(\"Maximum results:\", 1, 20, 5)\r\n        similarity_threshold = st.slider(\"Similarity threshold:\", 0.1, 1.0, 0.7)\r\n    \r\n    if st.button(\"Search Knowledge Base\"):\r\n        if query:\r\n            with st.spinner(\"Searching medical knowledge base...\"):\r\n                try:\r\n                    # Perform RAG search\r\n                    results = assistant.rag_system.query(\r\n                        query, \r\n                        max_results=max_results,\r\n                        threshold=similarity_threshold\r\n                    )\r\n                    \r\n                    if results:\r\n                        st.subheader(\"Search Results\")\r\n                        \r\n                        for i, result in enumerate(results, 1):\r\n                            with st.expander(f\"Result {i} (Score: {result['score']:.3f})\"):\r\n                                st.write(\"**Source:**\", result['source'])\r\n                                st.write(\"**Content:**\", result['content'])\r\n                                \r\n                        # Generate comprehensive answer using Llama\r\n                        comprehensive_answer = assistant.llama_processor.generate_rag_response(query, results)\r\n                        \r\n                        st.subheader(\"AI-Generated Summary\")\r\n                        st.write(comprehensive_answer)\r\n                        \r\n                    else:\r\n                        st.warning(\"No relevant results found. Try adjusting your query or similarity threshold.\")\r\n                        \r\n                except Exception as e:\r\n                    st.error(f\"Error during search: {str(e)}\")\r\n\r\ndef render_chat_tab(assistant):\r\n    \"\"\"Render the simplified medical chat interface\"\"\"\r\n    st.header(\"Medical Assistant Chat\")\r\n    st.markdown(\"**Ask medical questions or upload images/documents for analysis**\")\r\n    \r\n    # Chat history display\r\n    if st.session_state.medical_data['conversation_history']:\r\n        st.subheader(\"Conversation History\")\r\n        for i, exchange in enumerate(st.session_state.medical_data['conversation_history']):\r\n            with st.container():\r\n                # Show new topic indicator\r\n                if exchange.get('new_topic'):\r\n                    st.info(\"\ud83c\udd95 **New Medical Topic Started**\")\r\n                \r\n                st.write(f\"**You ({exchange['timestamp']}):** {exchange['user_message']}\")\r\n                \r\n                # Display uploaded content if present\r\n                if 'uploaded_image' in exchange:\r\n                    st.image(exchange['uploaded_image'], caption=f\"\ud83d\udcf8 {exchange.get('detected_type', 'Medical Image')}\", width=300)\r\n                \r\n                if 'uploaded_document' in exchange:\r\n                    st.info(f\"\ud83d\udcc4 Document: {exchange['document_name']} ({exchange.get('detected_type', 'Medical Document')})\")\r\n                \r\n                st.write(f\"**Assistant:** {exchange['assistant_response']}\")\r\n                \r\n                # Show analysis details if available\r\n                if 'analysis_details' in exchange:\r\n                    with st.expander(\"\ud83d\udcca Analysis Details\"):\r\n                        details = exchange['analysis_details']\r\n                        if 'detected_type' in details:\r\n                            st.write(f\"**Detected Type:** {details['detected_type']}\")\r\n                        if 'confidence' in details:\r\n                            st.write(f\"**Confidence:** {details['confidence']:.1%}\")\r\n                        if 'key_findings' in details:\r\n                            st.write(f\"**Key Findings:** {', '.join(details['key_findings'][:3])}\")\r\n                \r\n                st.divider()\r\n    \r\n    # Simplified input section\r\n    st.subheader(\"Ask or Upload\")\r\n    \r\n    # Context management buttons\r\n    col1, col2, col3 = st.columns([2, 1, 1])\r\n    with col1:\r\n        st.write(\"**Conversation Context:**\")\r\n    with col2:\r\n        if st.button(\"\ud83d\udd04 New Topic\", help=\"Start a new medical topic (clears previous context)\"):\r\n            # Mark that user wants to start a new topic\r\n            st.session_state.new_topic_requested = True\r\n            st.success(\"\u2705 Starting new medical topic - previous context cleared\")\r\n    with col3:\r\n        if st.button(\"\ud83d\uddd1\ufe0f Clear Chat\", help=\"Clear entire conversation history\"):\r\n            st.session_state.medical_data['conversation_history'] = []\r\n            st.success(\"\u2705 Chat history cleared\")\r\n            st.rerun()\r\n    \r\n    # Single text input area\r\n    user_input = st.text_area(\r\n        \"Ask a medical question or describe what you're uploading:\",\r\n        height=100,\r\n        placeholder=\"Examples:\\n\u2022 What are the symptoms of pneumonia?\\n\u2022 Please analyze this X-ray\\n\u2022 Is this skin lesion concerning?\\n\u2022 Review this medical report\\n\\n\ud83d\udca1 Tip: Click 'New Topic' above if asking about a different condition\"\r\n    )\r\n    \r\n    # Single file uploader for all types\r\n    uploaded_file = st.file_uploader(\r\n        \"\ud83d\udcce Upload any medical file (optional):\",\r\n        type=['jpg', 'jpeg', 'png', 'dcm', 'pdf', 'txt', 'doc', 'docx'],\r\n        help=\"Supported: X-rays, CT scans, MRIs, skin photos, medical reports, clinical notes\"\r\n    )\r\n    \r\n    # Show file preview\r\n    if uploaded_file:\r\n        file_type = uploaded_file.type\r\n        if file_type.startswith('image/'):\r\n            st.image(uploaded_file, caption=\"\ud83d\udcf8 Uploaded Image\", width=400)\r\n        else:\r\n            st.info(f\"\ud83d\udcc4 Document uploaded: {uploaded_file.name}\")\r\n    \r\n    # Send button\r\n    if st.button(\"Send\", type=\"primary\", use_container_width=True):\r\n        if user_input or uploaded_file:\r\n            with st.spinner(\"\ud83d\udd0d Analyzing and generating response...\"):\r\n                try:\r\n                    response_data = {}\r\n                    detected_type = \"General Query\"\r\n                    \r\n                    # Automatic file processing and type detection\r\n                    if uploaded_file:\r\n                        st.info(\"\ud83d\udccb Step 1/3: Processing uploaded file...\")\r\n                        \r\n                        # Auto-detect file type and content\r\n                        detection_result = assistant._auto_detect_content_type(uploaded_file, user_input)\r\n                        detected_type = detection_result['type']\r\n                        confidence = detection_result['confidence']\r\n                        \r\n                        st.success(f\"\u2705 Detected: {detected_type} (confidence: {confidence:.1%})\")\r\n                        \r\n                        # Process based on detected type\r\n                        if detection_result['category'] == 'image':\r\n                            st.info(\"\ud83d\udccb Step 2/3: Analyzing medical image...\")\r\n                            \r\n                            # Save and process image\r\n                            temp_path = assistant.file_handler.save_temp_file(uploaded_file)\r\n                            image_analysis = assistant.image_processor.process_medical_image(temp_path)\r\n                            \r\n                            response_data['image_analysis'] = image_analysis\r\n                            response_data['uploaded_image'] = uploaded_file\r\n                            response_data['detected_type'] = detected_type\r\n                            response_data['confidence'] = confidence\r\n                            \r\n                        elif detection_result['category'] == 'document':\r\n                            st.info(\"\ud83d\udccb Step 2/3: Processing medical document...\")\r\n                            \r\n                            # Extract and process document text\r\n                            extracted_text = assistant.file_handler.extract_text(uploaded_file)\r\n                            deidentified_text = assistant.deidentifier.deidentify_text(extracted_text)\r\n                            text_analysis = assistant.text_processor.process_medical_text(deidentified_text)\r\n                            \r\n                            response_data['document_analysis'] = text_analysis\r\n                            response_data['document_content'] = deidentified_text[:1000] + \"...\" if len(deidentified_text) > 1000 else deidentified_text\r\n                            response_data['uploaded_document'] = uploaded_file\r\n                            response_data['detected_type'] = detected_type\r\n                            response_data['confidence'] = confidence\r\n                    \r\n                    st.info(\"\ud83d\udccb Step 3/3: Generating intelligent response...\")\r\n                    \r\n                    # Generate contextual query if none provided\r\n                    if not user_input:\r\n                        user_input = assistant._generate_contextual_query(detected_type, response_data)\r\n                    \r\n                    # Generate appropriate response based on content type\r\n                    response = assistant._generate_intelligent_response(\r\n                        user_input, \r\n                        detected_type, \r\n                        response_data,\r\n                        st.session_state.medical_data['conversation_history']\r\n                    )\r\n                    \r\n                    # Prepare conversation entry\r\n                    conversation_entry = {\r\n                        'user_message': user_input,\r\n                        'assistant_response': response,\r\n                        'timestamp': datetime.now().strftime('%H:%M:%S'),\r\n                        'detected_type': detected_type\r\n                    }\r\n                    \r\n                    # Mark if this is a new topic\r\n                    if getattr(st.session_state, 'new_topic_requested', False):\r\n                        conversation_entry['new_topic'] = True\r\n                        st.session_state.new_topic_requested = False\r\n                    \r\n                    # Add file data if uploaded\r\n                    if uploaded_file:\r\n                        if response_data.get('image_analysis'):\r\n                            conversation_entry['uploaded_image'] = uploaded_file\r\n                            conversation_entry['analysis_details'] = {\r\n                                'detected_type': detected_type,\r\n                                'confidence': response_data.get('confidence', 0),\r\n                                'key_findings': assistant._extract_key_findings(response_data['image_analysis'])\r\n                            }\r\n                        elif response_data.get('document_analysis'):\r\n                            conversation_entry['uploaded_document'] = True\r\n                            conversation_entry['document_name'] = uploaded_file.name\r\n                            conversation_entry['analysis_details'] = {\r\n                                'detected_type': detected_type,\r\n                                'confidence': response_data.get('confidence', 0),\r\n                                'key_findings': assistant._extract_key_findings(response_data['document_analysis'])\r\n                            }\r\n                    \r\n                    # Add to conversation history\r\n                    st.session_state.medical_data['conversation_history'].append(conversation_entry)\r\n                    \r\n                    # Display success and response\r\n                    st.success(\"\u2705 Analysis complete!\")\r\n                    \r\n                    # Show detection summary if file was uploaded\r\n                    if uploaded_file:\r\n                        col1, col2, col3 = st.columns(3)\r\n                        with col1:\r\n                            st.metric(\"Content Type\", detected_type)\r\n                        with col2:\r\n                            st.metric(\"Confidence\", f\"{response_data.get('confidence', 0):.1%}\")\r\n                        with col3:\r\n                            processing_time = \"< 30s\" if 'skin' not in detected_type.lower() else \"< 90s\"\r\n                            st.metric(\"Processing Time\", processing_time)\r\n                    \r\n                    # Display response\r\n                    st.write(\"**Assistant Response:**\")\r\n                    st.write(response)\r\n                    \r\n                    # Rerun to update chat history\r\n                    st.rerun()\r\n                    \r\n                except Exception as e:\r\n                    st.error(f\"\u274c Error processing request: {str(e)}\")\r\n                    \r\n                    # Add error to conversation for debugging\r\n                    st.session_state.medical_data['conversation_history'].append({\r\n                        'user_message': user_input if user_input else \"[File uploaded]\",\r\n                        'assistant_response': f\"I apologize, but I encountered an error: {str(e)}. Please try again with a simpler question or different file format.\",\r\n                        'timestamp': datetime.now().strftime('%H:%M:%S'),\r\n                        'error': True\r\n                    })\r\n    \r\n    # Quick example buttons (simplified)\r\n    st.subheader(\"\ud83d\udca1 Example Questions\")\r\n    col1, col2, col3, col4 = st.columns(4)\r\n    \r\n    with col1:\r\n        if st.button(\"\ud83e\ude7a General Medical\"):\r\n            example_question = \"What are the common symptoms of pneumonia and how is it diagnosed?\"\r\n            st.session_state.medical_data['conversation_history'].append({\r\n                'user_message': example_question,\r\n                'assistant_response': assistant.llama_processor.generate_medical_response(example_question),\r\n                'timestamp': datetime.now().strftime('%H:%M:%S'),\r\n                'detected_type': 'General Medical Query'\r\n            })\r\n            st.rerun()\r\n    \r\n    with col2:\r\n        if st.button(\"\ud83d\udcf8 Upload X-ray\"):\r\n            st.info(\"\ud83d\udc46 Use the file uploader above to upload your X-ray image, then ask: 'Please analyze this X-ray'\")\r\n    \r\n    with col3:\r\n        if st.button(\"\ud83d\udd0d Upload Skin Photo\"):\r\n            st.info(\"\ud83d\udc46 Use the file uploader above to upload your skin photo, then ask: 'Is this skin lesion concerning?'\")\r\n    \r\n    with col4:\r\n        if st.button(\"\ud83d\udcc4 Upload Report\"):\r\n            st.info(\"\ud83d\udc46 Use the file uploader above to upload your medical report, then ask: 'Summarize this report'\")\r\n    \r\n    # Help section\r\n    with st.expander(\"\u2139\ufe0f How to Use This Medical Assistant\"):\r\n        st.markdown(\"\"\"\r\n        **This AI assistant automatically detects and analyzes:**\r\n        \r\n        \ud83d\udcf8 **Medical Images:**\r\n        - X-rays (chest, limb, etc.)\r\n        - Skin lesions and injuries\r\n        - CT scans and MRIs\r\n        - Other medical photos\r\n        \r\n        \ud83d\udcc4 **Medical Documents:**\r\n        - Lab reports\r\n        - Radiology reports  \r\n        - Clinical notes\r\n        - Discharge summaries\r\n        \r\n        \ud83d\udcac **Text Questions:**\r\n        - Symptom explanations\r\n        - Medical conditions\r\n        - Treatment information\r\n        - Drug interactions\r\n        \r\n        **Simply upload any file and ask your question - the AI will automatically:**\r\n        1. \u2705 Detect what type of content you uploaded\r\n        2. \u2705 Choose the right analysis method\r\n        3. \u2705 Provide appropriate medical insights\r\n        4. \u2705 Include relevant disclaimers and recommendations\r\n        \"\"\")\r\n    \r\n    # Medical disclaimer (always visible)\r\n    st.error(\"\"\"\r\n    \u26a0\ufe0f **IMPORTANT MEDICAL DISCLAIMER**\r\n    \r\n    This AI assistant is for **educational purposes only**. It should NOT be used for medical diagnosis, treatment decisions, or emergency situations. Always consult qualified healthcare professionals for medical advice.\r\n    \r\n    **For emergencies:** Call your local emergency number immediately.\r\n    \"\"\")\r\n\r\n\r\n# Add these helper methods to the MultimodalMedicalAssistant class\r\ndef add_helper_methods_to_assistant():\r\n    \"\"\"Add the helper methods to the main assistant class\"\"\"\r\n    \r\n    def _auto_detect_content_type(self, uploaded_file, user_query=None):\r\n        \"\"\"Automatically detect the type of uploaded content\"\"\"\r\n        try:\r\n            file_name = uploaded_file.name.lower()\r\n            file_type = uploaded_file.type\r\n            \r\n            # Image detection\r\n            if file_type.startswith('image/') or file_name.endswith(('.jpg', '.jpeg', '.png', '.dcm')):\r\n                # Try to detect medical image type from filename or user query\r\n                if any(keyword in file_name for keyword in ['xray', 'x-ray', 'chest', 'lung']):\r\n                    return {'type': 'Chest X-ray', 'category': 'image', 'confidence': 0.9}\r\n                elif any(keyword in file_name for keyword in ['skin', 'lesion', 'mole', 'rash']):\r\n                    return {'type': 'Skin Lesion', 'category': 'image', 'confidence': 0.9}\r\n                elif any(keyword in file_name for keyword in ['ct', 'scan']):\r\n                    return {'type': 'CT Scan', 'category': 'image', 'confidence': 0.8}\r\n                elif any(keyword in file_name for keyword in ['mri']):\r\n                    return {'type': 'MRI Scan', 'category': 'image', 'confidence': 0.8}\r\n                elif file_name.endswith('.dcm'):\r\n                    return {'type': 'DICOM Medical Image', 'category': 'image', 'confidence': 0.95}\r\n                else:\r\n                    # Analyze user query for hints\r\n                    if user_query:\r\n                        query_lower = user_query.lower()\r\n                        if any(keyword in query_lower for keyword in ['x-ray', 'chest', 'lung', 'pneumonia']):\r\n                            return {'type': 'X-ray Image', 'category': 'image', 'confidence': 0.7}\r\n                        elif any(keyword in query_lower for keyword in ['skin', 'lesion', 'mole', 'melanoma', 'rash']):\r\n                            return {'type': 'Skin Lesion', 'category': 'image', 'confidence': 0.7}\r\n                    \r\n                    return {'type': 'Medical Image', 'category': 'image', 'confidence': 0.6}\r\n            \r\n            # Document detection\r\n            elif file_type.startswith('text/') or file_name.endswith(('.pdf', '.txt', '.doc', '.docx')):\r\n                if any(keyword in file_name for keyword in ['lab', 'blood', 'test', 'result']):\r\n                    return {'type': 'Lab Report', 'category': 'document', 'confidence': 0.9}\r\n                elif any(keyword in file_name for keyword in ['radiology', 'imaging', 'scan']):\r\n                    return {'type': 'Radiology Report', 'category': 'document', 'confidence': 0.9}\r\n                elif any(keyword in file_name for keyword in ['discharge', 'summary']):\r\n                    return {'type': 'Discharge Summary', 'category': 'document', 'confidence': 0.9}\r\n                else:\r\n                    return {'type': 'Medical Document', 'category': 'document', 'confidence': 0.7}\r\n            \r\n            else:\r\n                return {'type': 'Unknown File Type', 'category': 'unknown', 'confidence': 0.3}\r\n                \r\n        except Exception as e:\r\n            return {'type': 'File Processing Error', 'category': 'error', 'confidence': 0.0}\r\n    \r\n    def _generate_contextual_query(self, detected_type, response_data):\r\n        \"\"\"Generate appropriate query based on detected content type\"\"\"\r\n        \r\n        if 'x-ray' in detected_type.lower() or 'chest' in detected_type.lower():\r\n            return \"Please analyze this X-ray image. What are the key findings and any areas of concern?\"\r\n        \r\n        elif 'skin' in detected_type.lower() or 'lesion' in detected_type.lower():\r\n            return \"Please analyze this skin lesion. Assess using ABCDE criteria and indicate the level of concern.\"\r\n        \r\n        elif 'ct' in detected_type.lower():\r\n            return \"Please analyze this CT scan image. What structures are visible and any abnormal findings?\"\r\n        \r\n        elif 'mri' in detected_type.lower():\r\n            return \"Please analyze this MRI image. Describe the anatomy visible and any pathological findings.\"\r\n        \r\n        elif 'lab' in detected_type.lower():\r\n            return \"Please review this lab report. Highlight any abnormal values and their clinical significance.\"\r\n        \r\n        elif 'radiology' in detected_type.lower():\r\n            return \"Please summarize this radiology report. What are the key findings and recommendations?\"\r\n        \r\n        elif 'discharge' in detected_type.lower():\r\n            return \"Please summarize this discharge summary. What was the diagnosis, treatment, and follow-up plan?\"\r\n        \r\n        elif 'document' in detected_type.lower():\r\n            return \"Please analyze this medical document. Summarize the key medical information and findings.\"\r\n        \r\n        else:\r\n            return \"Please analyze the uploaded content and provide relevant medical insights.\"\r\n    \r\n    def _generate_intelligent_response(self, user_query, detected_type, response_data, conversation_history):\r\n        \"\"\"Generate intelligent response based on content type and context\"\"\"\r\n        \r\n        try:\r\n            # Check if user requested new topic\r\n            new_topic = getattr(st.session_state, 'new_topic_requested', False)\r\n            if new_topic:\r\n                # Clear the flag and use empty conversation history\r\n                st.session_state.new_topic_requested = False\r\n                conversation_history = []\r\n            \r\n            # Handle image analysis\r\n            if response_data.get('image_analysis'):\r\n                if 'skin' in detected_type.lower():\r\n                    # Use optimized skin lesion processing\r\n                    return self._generate_skin_lesion_response(user_query, response_data, conversation_history)\r\n                else:\r\n                    # Standard image analysis\r\n                    return self._generate_image_response(user_query, detected_type, response_data, conversation_history)\r\n            \r\n            # Handle document analysis\r\n            elif response_data.get('document_analysis'):\r\n                return self._generate_document_response(user_query, detected_type, response_data, conversation_history)\r\n            \r\n            # Handle text-only queries\r\n            else:\r\n                return self.llama_processor.generate_medical_response(user_query, conversation_history)\r\n                \r\n        except Exception as e:\r\n            return f\"I encountered an error while analyzing your content. Please try asking a more specific question or contact support. Error: {str(e)}\"\r\n    \r\n    def _generate_skin_lesion_response(self, user_query, response_data, conversation_history):\r\n        \"\"\"Generate optimized response for skin lesions\"\"\"\r\n        \r\n        try:\r\n            # Create focused skin lesion prompt\r\n            image_analysis = response_data['image_analysis']\r\n            \r\n            skin_prompt = f\"\"\"Skin lesion analysis request: {user_query}\r\n\r\nImage Analysis Summary:\r\n- Detected as: {response_data['detected_type']}\r\n- Processing Status: {image_analysis.get('processing_status', 'completed')}\r\n- Findings: {image_analysis.get('medical_findings', {})}\r\n\r\nProvide focused response (under 250 words):\r\n1. Visual assessment based on analysis\r\n2. ABCDE criteria evaluation\r\n3. Concern level (low/medium/high)\r\n4. Immediate recommendations\r\n\r\nInclude appropriate medical disclaimers.\"\"\"\r\n\r\n            return self.llama_processor.generate_response(\r\n                skin_prompt,\r\n                temperature=0.2,\r\n                max_tokens=500,\r\n                timeout=90\r\n            )\r\n            \r\n        except Exception:\r\n            # Fallback response for skin lesions\r\n            return \"\"\"**Skin Lesion Analysis**\r\n\r\nI've processed your skin lesion image. Here's important guidance:\r\n\r\n**ABCDE Criteria Assessment:**\r\n- **A**symmetry: Check if one half differs from the other\r\n- **B**order: Look for irregular or poorly defined edges\r\n- **C**olor: Note multiple colors or uneven distribution  \r\n- **D**iameter: Measure if larger than 6mm (pencil eraser)\r\n- **E**volving: Track any recent changes in size, shape, or color\r\n\r\n**Recommendations:**\r\n- Consult a dermatologist for professional evaluation\r\n- Monitor for any changes and take photos for comparison\r\n- Protect the area from sun exposure\r\n- Seek immediate care if bleeding, rapid growth, or other concerning changes occur\r\n\r\n**IMPORTANT:** This analysis is for educational purposes only. Skin lesion evaluation requires professional dermatological assessment. Any concerning features warrant immediate medical attention.\"\"\"\r\n    \r\n    def _generate_image_response(self, user_query, detected_type, response_data, conversation_history):\r\n        \"\"\"Generate response for medical images (non-skin)\"\"\"\r\n        \r\n        image_analysis = response_data['image_analysis']\r\n        \r\n        multimodal_prompt = f\"\"\"Medical Image Analysis Request: {user_query}\r\n\r\nImage Details:\r\n- Type: {detected_type}\r\n- Modality: {image_analysis.get('modality', 'Unknown')}\r\n- Findings: {image_analysis.get('medical_findings', {})}\r\n- Quality: {image_analysis.get('characteristics', {}).get('quality_category', 'Unknown')}\r\n\r\nProvide comprehensive analysis including:\r\n1. Image interpretation\r\n2. Key findings and abnormalities\r\n3. Clinical significance\r\n4. Recommendations for next steps\r\n\r\nInclude appropriate medical disclaimers.\"\"\"\r\n\r\n        return self.llama_processor.generate_medical_response(\r\n            multimodal_prompt,\r\n            conversation_history,\r\n            image_type=detected_type\r\n        )\r\n    \r\n    def _generate_document_response(self, user_query, detected_type, response_data, conversation_history):\r\n        \"\"\"Generate response for medical documents\"\"\"\r\n        \r\n        document_analysis = response_data['document_analysis']\r\n        document_content = response_data['document_content']\r\n        \r\n        document_prompt = f\"\"\"Medical Document Analysis Request: {user_query}\r\n\r\nDocument Type: {detected_type}\r\nContent Preview: {document_content}\r\n\r\nDocument Analysis Summary:\r\n- Entities Found: {document_analysis.get('entities', {})}\r\n- Medical Concepts: {document_analysis.get('medical_concepts', {})}\r\n- Clinical Assessment: {document_analysis.get('clinical_assessment', {})}\r\n\r\nPlease provide:\r\n1. Document summary\r\n2. Key medical findings\r\n3. Important values or results\r\n4. Clinical significance and recommendations\r\n\r\nInclude appropriate medical disclaimers.\"\"\"\r\n\r\n        return self.llama_processor.generate_medical_response(document_prompt, conversation_history)\r\n    \r\n    def _extract_key_findings(self, analysis_data):\r\n        \"\"\"Extract key findings from analysis data for display\"\"\"\r\n        \r\n        findings = []\r\n        \r\n        try:\r\n            if 'medical_findings' in analysis_data:\r\n                medical_findings = analysis_data['medical_findings'].get('findings', [])\r\n                for finding in medical_findings[:3]:  # Top 3 findings\r\n                    findings.append(finding.get('finding', 'Unknown finding'))\r\n            \r\n            elif 'entities' in analysis_data:\r\n                entities = analysis_data['entities']\r\n                for category, entity_list in entities.items():\r\n                    if entity_list and len(findings) < 3:\r\n                        findings.append(f\"{category}: {entity_list[0].get('text', 'Unknown')}\")\r\n            \r\n            return findings if findings else ['Analysis completed']\r\n            \r\n        except Exception:\r\n            return ['Processing completed']\r\n    \r\n    # Add methods to the MultimodalMedicalAssistant class\r\n    MultimodalMedicalAssistant._auto_detect_content_type = _auto_detect_content_type\r\n    MultimodalMedicalAssistant._generate_contextual_query = _generate_contextual_query\r\n    MultimodalMedicalAssistant._generate_intelligent_response = _generate_intelligent_response\r\n    MultimodalMedicalAssistant._generate_skin_lesion_response = _generate_skin_lesion_response\r\n    MultimodalMedicalAssistant._generate_image_response = _generate_image_response\r\n    MultimodalMedicalAssistant._generate_document_response = _generate_document_response\r\n    MultimodalMedicalAssistant._extract_key_findings = _extract_key_findings\r\n\r\n# Call this function to add the methods\r\nadd_helper_methods_to_assistant()\r\n\r\ndef render_status_tab(assistant):\r\n    \"\"\"Render the system status interface\"\"\"\r\n    st.header(\"System Status & Configuration\")\r\n    \r\n    col1, col2 = st.columns(2)\r\n    \r\n    with col1:\r\n        st.subheader(\"Model Status\")\r\n        \r\n        # Check model availability\r\n        models_status = {\r\n            \"Llama (via Ollama)\": assistant.llama_processor.check_status(),\r\n            \"MedCLIP\": assistant.image_processor.check_medclip_status(),\r\n            \"BioBERT\": assistant.text_processor.check_biobert_status(),\r\n            \"RAG System\": assistant.rag_system.check_status()\r\n        }\r\n        \r\n        for model, status in models_status.items():\r\n            if status:\r\n                st.success(f\"{model}: Available\")\r\n            else:\r\n                st.error(f\"{model}: Not Available\")\r\n    \r\n    with col2:\r\n        st.subheader(\"Data Statistics\")\r\n        \r\n        stats = {\r\n            \"Uploaded Images\": len(st.session_state.medical_data['processed_images']),\r\n            \"Uploaded Documents\": len(st.session_state.medical_data['processed_texts']),\r\n            \"Chat Messages\": len(st.session_state.medical_data['conversation_history']),\r\n            \"RAG Documents\": assistant.rag_system.get_document_count()\r\n        }\r\n        \r\n        for stat, value in stats.items():\r\n            st.metric(stat, value)\r\n    \r\n    # Clear data options\r\n    st.subheader(\"Data Management\")\r\n    if st.button(\"Clear All Data\", type=\"secondary\"):\r\n        st.session_state.medical_data = {\r\n            'uploaded_files': [],\r\n            'processed_images': [],\r\n            'processed_texts': [],\r\n            'conversation_history': []\r\n        }\r\n        assistant.rag_system.clear_documents()\r\n        st.success(\"All data cleared!\")\r\n        st.rerun()\r\n\r\nif __name__ == \"__main__\":\r\n    main()",
      "language": "python",
      "functions": [
        {
          "name": "main",
          "line": 44,
          "args": [],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "render_data_upload_tab",
          "line": 107,
          "args": [
            "assistant"
          ],
          "docstring": "Render the data upload interface",
          "decorators": []
        },
        {
          "name": "render_analysis_tab",
          "line": 186,
          "args": [
            "assistant"
          ],
          "docstring": "Render the cross-modal analysis interface",
          "decorators": []
        },
        {
          "name": "render_rag_tab",
          "line": 262,
          "args": [
            "assistant"
          ],
          "docstring": "Render the RAG query interface",
          "decorators": []
        },
        {
          "name": "render_chat_tab",
          "line": 313,
          "args": [
            "assistant"
          ],
          "docstring": "Render the simplified medical chat interface",
          "decorators": []
        },
        {
          "name": "add_helper_methods_to_assistant",
          "line": 585,
          "args": [],
          "docstring": "Add the helper methods to the main assistant class",
          "decorators": []
        },
        {
          "name": "render_status_tab",
          "line": 833,
          "args": [
            "assistant"
          ],
          "docstring": "Render the system status interface",
          "decorators": []
        },
        {
          "name": "__init__",
          "line": 19,
          "args": [
            "self"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "initialize_components",
          "line": 22,
          "args": [
            "self"
          ],
          "docstring": "Initialize all the required components",
          "decorators": []
        },
        {
          "name": "_auto_detect_content_type",
          "line": 588,
          "args": [
            "self",
            "uploaded_file",
            "user_query"
          ],
          "docstring": "Automatically detect the type of uploaded content",
          "decorators": []
        },
        {
          "name": "_generate_contextual_query",
          "line": 635,
          "args": [
            "self",
            "detected_type",
            "response_data"
          ],
          "docstring": "Generate appropriate query based on detected content type",
          "decorators": []
        },
        {
          "name": "_generate_intelligent_response",
          "line": 665,
          "args": [
            "self",
            "user_query",
            "detected_type",
            "response_data",
            "conversation_history"
          ],
          "docstring": "Generate intelligent response based on content type and context",
          "decorators": []
        },
        {
          "name": "_generate_skin_lesion_response",
          "line": 696,
          "args": [
            "self",
            "user_query",
            "response_data",
            "conversation_history"
          ],
          "docstring": "Generate optimized response for skin lesions",
          "decorators": []
        },
        {
          "name": "_generate_image_response",
          "line": 746,
          "args": [
            "self",
            "user_query",
            "detected_type",
            "response_data",
            "conversation_history"
          ],
          "docstring": "Generate response for medical images (non-skin)",
          "decorators": []
        },
        {
          "name": "_generate_document_response",
          "line": 773,
          "args": [
            "self",
            "user_query",
            "detected_type",
            "response_data",
            "conversation_history"
          ],
          "docstring": "Generate response for medical documents",
          "decorators": []
        },
        {
          "name": "_extract_key_findings",
          "line": 799,
          "args": [
            "self",
            "analysis_data"
          ],
          "docstring": "Extract key findings from analysis data for display",
          "decorators": []
        }
      ],
      "classes": [
        {
          "name": "MultimodalMedicalAssistant",
          "line": 18,
          "methods": [
            "__init__",
            "initialize_components"
          ],
          "docstring": null,
          "bases": []
        }
      ],
      "imports": [
        "streamlit",
        "os",
        "tempfile",
        "pandas",
        "pathlib.Path",
        "time",
        "json",
        "datetime.datetime",
        "src.models.llama_processor.LlamaProcessor",
        "src.models.image_processor.MedicalImageProcessor",
        "src.models.text_processor.MedicalTextProcessor",
        "src.rag.retrieval_system.MedicalRAGSystem",
        "src.utils.privacy_utils.DataDeidentifier",
        "src.utils.file_handler.FileHandler"
      ],
      "complexity": "high",
      "timestamp": "2025-10-20T09:01:08.426542"
    },
    "c6ee1dc677572dc360c1da2498e4d449": {
      "filename": "setup.py",
      "code": "# setup.py\r\n\"\"\"\r\nSetup script for the Multimodal Medical Assistant\r\nThis script installs dependencies and configures the environment\r\n\"\"\"\r\n\r\nimport subprocess\r\nimport sys\r\nimport os\r\nimport platform\r\nfrom pathlib import Path\r\nimport requests\r\nimport zipfile\r\nimport shutil\r\n\r\nclass MedicalAssistantSetup:\r\n    def __init__(self):\r\n        self.base_dir = Path(__file__).parent\r\n        self.python_executable = sys.executable\r\n        self.platform = platform.system().lower()\r\n        \r\n    def print_header(self):\r\n        \"\"\"Print setup header\"\"\"\r\n        print(\"=\" * 60)\r\n        print(\" MULTIMODAL MEDICAL ASSISTANT SETUP\")\r\n        print(\"=\" * 60)\r\n        print(f\"Platform: {platform.platform()}\")\r\n        print(f\"Python: {sys.version}\")\r\n        print(f\"Base Directory: {self.base_dir}\")\r\n        print(\"=\" * 60)\r\n    \r\n    def check_python_version(self):\r\n        \"\"\"Check if Python version is compatible\"\"\"\r\n        print(\"\\n\ud83d\udccb Checking Python version...\")\r\n        \r\n        version = sys.version_info\r\n        if version.major != 3 or version.minor < 8:\r\n            print(\" ERROR: Python 3.8 or higher is required\")\r\n            print(f\"   Current version: {version.major}.{version.minor}.{version.micro}\")\r\n            return False\r\n        \r\n        print(f\" Python {version.major}.{version.minor}.{version.micro} is compatible\")\r\n        return True\r\n    \r\n    def install_base_requirements(self):\r\n        \"\"\"Install base Python requirements\"\"\"\r\n        print(\"\\n Installing Python dependencies...\")\r\n        \r\n        requirements_file = self.base_dir / \"requirements.txt\"\r\n        if not requirements_file.exists():\r\n            print(\" ERROR: requirements.txt not found\")\r\n            return False\r\n        \r\n        try:\r\n            # Update pip first\r\n            subprocess.check_call([\r\n                self.python_executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"\r\n            ])\r\n            \r\n            # Install requirements\r\n            subprocess.check_call([\r\n                self.python_executable, \"-m\", \"pip\", \"install\", \"-r\", str(requirements_file)\r\n            ])\r\n            \r\n            print(\" Python dependencies installed successfully\")\r\n            return True\r\n            \r\n        except subprocess.CalledProcessError as e:\r\n            print(f\" ERROR installing dependencies: {e}\")\r\n            return False\r\n    \r\n    def setup_ollama(self):\r\n        \"\"\"Setup Ollama for Llama model\"\"\"\r\n        print(\"\\n Setting up Ollama for Llama models...\")\r\n        \r\n        # Check if Ollama is already installed\r\n        try:\r\n            result = subprocess.run([\"ollama\", \"--version\"], \r\n                                  capture_output=True, text=True)\r\n            if result.returncode == 0:\r\n                print(\" Ollama is already installed\")\r\n                return self.pull_llama_model()\r\n        except FileNotFoundError:\r\n            pass\r\n        \r\n        print(\" Ollama not found. Installation required...\")\r\n        \r\n        if self.platform == \"windows\":\r\n            print(\" Windows detected - Please download Ollama from: https://ollama.ai/download\")\r\n            print(\"   After installation, run this setup script again.\")\r\n            return False\r\n        elif self.platform == \"darwin\":  # macOS\r\n            print(\" macOS detected - Installing Ollama...\")\r\n            try:\r\n                subprocess.check_call([\"brew\", \"install\", \"ollama\"])\r\n                print(\" Ollama installed via Homebrew\")\r\n                return self.pull_llama_model()\r\n            except (subprocess.CalledProcessError, FileNotFoundError):\r\n                print(\" Failed to install via Homebrew\")\r\n                print(\"   Please install manually from: https://ollama.ai/download\")\r\n                return False\r\n        else:  # Linux\r\n            print(\" Linux detected - Installing Ollama...\")\r\n            try:\r\n                # Download and install Ollama\r\n                install_script = \"curl -fsSL https://ollama.ai/install.sh | sh\"\r\n                subprocess.check_call(install_script, shell=True)\r\n                print(\" Ollama installed successfully\")\r\n                return self.pull_llama_model()\r\n            except subprocess.CalledProcessError:\r\n                print(\" Failed to install Ollama\")\r\n                print(\"   Please install manually from: https://ollama.ai/download\")\r\n                return False\r\n    \r\n    def pull_llama_model(self):\r\n        \"\"\"Pull the Llama model using Ollama\"\"\"\r\n        print(\" Downloading Llama2 model (this may take a while)...\")\r\n        \r\n        try:\r\n            # Start Ollama service\r\n            subprocess.Popen([\"ollama\", \"serve\"], \r\n                           stdout=subprocess.DEVNULL, \r\n                           stderr=subprocess.DEVNULL)\r\n            \r\n            # Wait a moment for service to start\r\n            import time\r\n            time.sleep(3)\r\n            \r\n            # Pull the model\r\n            subprocess.check_call([\"ollama\", \"pull\", \"llama2\"])\r\n            print(\" Llama2 model downloaded successfully\")\r\n            return True\r\n            \r\n        except subprocess.CalledProcessError as e:\r\n            print(f\" ERROR downloading Llama model: {e}\")\r\n            return False\r\n    \r\n    def download_spacy_models(self):\r\n        \"\"\"Download required spaCy models\"\"\"\r\n        print(\"\\n Downloading spaCy models...\")\r\n        \r\n        models = [\r\n            \"en_core_web_sm\",\r\n            \"en_core_sci_sm\"  # Scientific model\r\n        ]\r\n        \r\n        for model in models:\r\n            try:\r\n                print(f\" Downloading {model}...\")\r\n                subprocess.check_call([\r\n                    self.python_executable, \"-m\", \"spacy\", \"download\", model\r\n                ])\r\n                print(f\" {model} downloaded successfully\")\r\n            except subprocess.CalledProcessError:\r\n                print(f\"  Warning: Could not download {model}\")\r\n                if model == \"en_core_sci_sm\":\r\n                    print(\"   Scientific model is optional but recommended\")\r\n                    print(\"   Install manually: pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.0/en_core_sci_sm-0.5.0.tar.gz\")\r\n    \r\n    def setup_medical_models(self):\r\n        \"\"\"Setup medical AI models\"\"\"\r\n        print(\"\\n Setting up medical AI models...\")\r\n        \r\n        # This will download models on first use\r\n        models_to_verify = [\r\n            \"microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\",\r\n            \"dmis-lab/biobert-base-cased-v1.1\",\r\n            \"emilyalsentzer/Bio_ClinicalBERT\",\r\n            \"d4data/biomedical-ner-all\"\r\n        ]\r\n        \r\n        print(\" Medical models will be downloaded automatically on first use:\")\r\n        for model in models_to_verify:\r\n            print(f\"   - {model}\")\r\n        \r\n        print(\" Medical models configured\")\r\n        return True\r\n    \r\n    def create_directories(self):\r\n        \"\"\"Create necessary directories\"\"\"\r\n        print(\"\\n Creating directory structure...\")\r\n        \r\n        directories = [\r\n            \"data\",\r\n            \"data/documents\",\r\n            \"data/embeddings\", \r\n            \"data/medical_kb\",\r\n            \"models\",\r\n            \"logs\",\r\n            \"logs/application\",\r\n            \"logs/audit\",\r\n            \"temp\"\r\n        ]\r\n        \r\n        for dir_name in directories:\r\n            dir_path = self.base_dir / dir_name\r\n            dir_path.mkdir(parents=True, exist_ok=True)\r\n            print(f\" Created: {dir_path}\")\r\n        \r\n        return True\r\n    \r\n    def create_sample_data(self):\r\n        \"\"\"Create sample medical data for testing\"\"\"\r\n        print(\"\\n Creating sample data...\")\r\n        \r\n        sample_text = \"\"\"\r\n        MEDICAL RECORD - SAMPLE DATA FOR TESTING\r\n        =======================================\r\n        \r\n        Patient: [SAMPLE_PATIENT]\r\n        DOB: [DATE]\r\n        MRN: [MRN_12345]\r\n        \r\n        CHIEF COMPLAINT:\r\n        Patient presents with chest pain and shortness of breath.\r\n        \r\n        HISTORY OF PRESENT ILLNESS:\r\n        45-year-old male presents to the emergency department with acute onset chest pain \r\n        that started 2 hours ago. Pain is described as crushing, substernal, radiating \r\n        to left arm. Associated with diaphoresis and nausea. No previous cardiac history.\r\n        \r\n        PHYSICAL EXAMINATION:\r\n        Vital Signs: BP 150/90, HR 110, RR 22, O2 Sat 95% on room air\r\n        General: Anxious appearing male in moderate distress\r\n        Cardiovascular: Tachycardic, regular rhythm, no murmurs\r\n        Pulmonary: Clear to auscultation bilaterally\r\n        \r\n        DIAGNOSTIC TESTS:\r\n        EKG: ST elevation in leads II, III, aVF\r\n        Chest X-ray: Clear lung fields, normal cardiac silhouette\r\n        Troponin: Elevated at 2.5 ng/mL (normal <0.04)\r\n        \r\n        ASSESSMENT AND PLAN:\r\n        Acute ST-elevation myocardial infarction (STEMI)\r\n        - Activate cardiac catheterization lab\r\n        - Administer aspirin, clopidogrel, atorvastatin\r\n        - Heparin per protocol\r\n        - Serial cardiac enzymes\r\n        \r\n        This is sample data for testing the medical assistant.\r\n        \"\"\"\r\n        \r\n        sample_file = self.base_dir / \"data\" / \"sample_medical_record.txt\"\r\n        with open(sample_file, 'w') as f:\r\n            f.write(sample_text)\r\n        \r\n        print(f\" Created sample medical record: {sample_file}\")\r\n        return True\r\n    \r\n    def create_environment_file(self):\r\n        \"\"\"Create .env file with default settings\"\"\"\r\n        print(\"\\n  Creating environment configuration...\")\r\n        \r\n        env_content = \"\"\"# Multimodal Medical Assistant Environment Configuration\r\n\r\n# Ollama Configuration\r\nOLLAMA_BASE_URL=http://localhost:11434\r\nLLAMA_MODEL=llama2\r\n\r\n# Model Configuration\r\nBIOMEDCLIP_MODEL=microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\r\nBIOBERT_MODEL=dmis-lab/biobert-base-cased-v1.1\r\nCLINICAL_BERT_MODEL=emilyalsentzer/Bio_ClinicalBERT\r\nMEDICAL_NER_MODEL=d4data/biomedical-ner-all\r\n\r\n# Logging\r\nLOG_LEVEL=INFO\r\n\r\n# Security (Change these in production!)\r\nSECRET_KEY=your-secret-key-change-in-production\r\nREQUIRE_HTTPS=False\r\n\r\n# Development Settings\r\nDEBUG=True\r\n\"\"\"\r\n        \r\n        env_file = self.base_dir / \".env\"\r\n        if not env_file.exists():\r\n            with open(env_file, 'w') as f:\r\n                f.write(env_content)\r\n            print(f\" Created environment file: {env_file}\")\r\n        else:\r\n            print(\"  .env file already exists, skipping...\")\r\n        \r\n        return True\r\n    \r\n    def verify_installation(self):\r\n        \"\"\"Verify that everything is installed correctly\"\"\"\r\n        print(\"\\n Verifying installation...\")\r\n        \r\n        # Check Python packages\r\n        required_packages = [\r\n            'streamlit', 'torch', 'transformers', 'sentence_transformers',\r\n            'langchain', 'faiss', 'opencv-python', 'pydicom', 'spacy'\r\n        ]\r\n        \r\n        missing_packages = []\r\n        for package in required_packages:\r\n            try:\r\n                __import__(package.replace('-', '_'))\r\n                print(f\" {package}\")\r\n            except ImportError:\r\n                missing_packages.append(package)\r\n                print(f\" {package}\")\r\n        \r\n        # Check Ollama\r\n        try:\r\n            response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\r\n            if response.status_code == 200:\r\n                print(\" Ollama service\")\r\n            else:\r\n                print(\" Ollama service (not running)\")\r\n        except:\r\n            print(\" Ollama service (not accessible)\")\r\n        \r\n        # Check spaCy models\r\n        try:\r\n            import spacy\r\n            try:\r\n                spacy.load(\"en_core_web_sm\")\r\n                print(\" spaCy English model\")\r\n            except OSError:\r\n                print(\" spaCy English model\")\r\n        except ImportError:\r\n            print(\" spaCy not installed\")\r\n        \r\n        if missing_packages:\r\n            print(f\"\\n Missing packages: {', '.join(missing_packages)}\")\r\n            return False\r\n        \r\n        print(\"\\n Installation verification complete!\")\r\n        return True\r\n    \r\n    def print_next_steps(self):\r\n        \"\"\"Print next steps for the user\"\"\"\r\n        print(\"\\n\" + \"=\" * 60)\r\n        print(\" SETUP COMPLETE!\")\r\n        print(\"=\" * 60)\r\n        print(\"\\nNEXT STEPS:\")\r\n        print(\"1. Ensure Ollama is running: 'ollama serve'\")\r\n        print(\"2. Start the medical assistant: 'streamlit run main.py'\")\r\n        print(\"3. Open your browser to: http://localhost:8501\")\r\n        print(\"\\n DOCUMENTATION:\")\r\n        print(\"- Upload medical images (X-rays, CT scans, MRIs)\")\r\n        print(\"- Upload medical documents (PDFs, text files)\")\r\n        print(\"- Ask questions about medical data\")\r\n        print(\"- Perform cross-modal analysis\")\r\n        print(\"\\n  IMPORTANT DISCLAIMERS:\")\r\n        print(\"- This is for educational/research purposes only\")\r\n        print(\"- Not for clinical decision making\")\r\n        print(\"- Always consult healthcare professionals\")\r\n        print(\"- Ensure patient data is properly de-identified\")\r\n        print(\"\\n TROUBLESHOOTING:\")\r\n        print(\"- Check logs in the 'logs' directory\")\r\n        print(\"- Ensure all models are downloaded\")\r\n        print(\"- Verify Ollama is running on port 11434\")\r\n        print(\"=\" * 60)\r\n    \r\n    def run_setup(self):\r\n        \"\"\"Run the complete setup process\"\"\"\r\n        self.print_header()\r\n        \r\n        steps = [\r\n            (\"Checking Python version\", self.check_python_version),\r\n            (\"Creating directories\", self.create_directories),\r\n            (\"Installing requirements\", self.install_base_requirements),\r\n            (\"Setting up Ollama\", self.setup_ollama),\r\n            (\"Downloading spaCy models\", self.download_spacy_models),\r\n            (\"Setting up medical models\", self.setup_medical_models),\r\n            (\"Creating sample data\", self.create_sample_data),\r\n            (\"Creating environment file\", self.create_environment_file),\r\n            (\"Verifying installation\", self.verify_installation)\r\n        ]\r\n        \r\n        failed_steps = []\r\n        \r\n        for step_name, step_function in steps:\r\n            try:\r\n                if not step_function():\r\n                    failed_steps.append(step_name)\r\n            except Exception as e:\r\n                print(f\" ERROR in {step_name}: {e}\")\r\n                failed_steps.append(step_name)\r\n        \r\n        if failed_steps:\r\n            print(f\"\\n Setup completed with warnings. Failed steps: {', '.join(failed_steps)}\")\r\n            print(\"   You may need to complete these steps manually.\")\r\n        \r\n        self.print_next_steps()\r\n        return len(failed_steps) == 0\r\n\r\ndef main():\r\n    \"\"\"Main setup function\"\"\"\r\n    if len(sys.argv) > 1 and sys.argv[1] == \"--verify-only\":\r\n        # Only run verification\r\n        setup = MedicalAssistantSetup()\r\n        setup.verify_installation()\r\n    else:\r\n        # Run full setup\r\n        setup = MedicalAssistantSetup()\r\n        success = setup.run_setup()\r\n        sys.exit(0 if success else 1)\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n",
      "language": "python",
      "functions": [
        {
          "name": "main",
          "line": 392,
          "args": [],
          "docstring": "Main setup function",
          "decorators": []
        },
        {
          "name": "__init__",
          "line": 17,
          "args": [
            "self"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "print_header",
          "line": 22,
          "args": [
            "self"
          ],
          "docstring": "Print setup header",
          "decorators": []
        },
        {
          "name": "check_python_version",
          "line": 32,
          "args": [
            "self"
          ],
          "docstring": "Check if Python version is compatible",
          "decorators": []
        },
        {
          "name": "install_base_requirements",
          "line": 45,
          "args": [
            "self"
          ],
          "docstring": "Install base Python requirements",
          "decorators": []
        },
        {
          "name": "setup_ollama",
          "line": 72,
          "args": [
            "self"
          ],
          "docstring": "Setup Ollama for Llama model",
          "decorators": []
        },
        {
          "name": "pull_llama_model",
          "line": 115,
          "args": [
            "self"
          ],
          "docstring": "Pull the Llama model using Ollama",
          "decorators": []
        },
        {
          "name": "download_spacy_models",
          "line": 138,
          "args": [
            "self"
          ],
          "docstring": "Download required spaCy models",
          "decorators": []
        },
        {
          "name": "setup_medical_models",
          "line": 160,
          "args": [
            "self"
          ],
          "docstring": "Setup medical AI models",
          "decorators": []
        },
        {
          "name": "create_directories",
          "line": 179,
          "args": [
            "self"
          ],
          "docstring": "Create necessary directories",
          "decorators": []
        },
        {
          "name": "create_sample_data",
          "line": 202,
          "args": [
            "self"
          ],
          "docstring": "Create sample medical data for testing",
          "decorators": []
        },
        {
          "name": "create_environment_file",
          "line": 250,
          "args": [
            "self"
          ],
          "docstring": "Create .env file with default settings",
          "decorators": []
        },
        {
          "name": "verify_installation",
          "line": 287,
          "args": [
            "self"
          ],
          "docstring": "Verify that everything is installed correctly",
          "decorators": []
        },
        {
          "name": "print_next_steps",
          "line": 334,
          "args": [
            "self"
          ],
          "docstring": "Print next steps for the user",
          "decorators": []
        },
        {
          "name": "run_setup",
          "line": 359,
          "args": [
            "self"
          ],
          "docstring": "Run the complete setup process",
          "decorators": []
        }
      ],
      "classes": [
        {
          "name": "MedicalAssistantSetup",
          "line": 16,
          "methods": [
            "__init__",
            "print_header",
            "check_python_version",
            "install_base_requirements",
            "setup_ollama",
            "pull_llama_model",
            "download_spacy_models",
            "setup_medical_models",
            "create_directories",
            "create_sample_data",
            "create_environment_file",
            "verify_installation",
            "print_next_steps",
            "run_setup"
          ],
          "docstring": null,
          "bases": []
        }
      ],
      "imports": [
        "subprocess",
        "sys",
        "os",
        "platform",
        "pathlib.Path",
        "requests",
        "zipfile",
        "shutil",
        "time",
        "spacy"
      ],
      "complexity": "high",
      "timestamp": "2025-10-20T09:01:08.426542"
    },
    "d3e206534c409ee71071ccb97a3cde4f": {
      "filename": "skin_lesion_optimizer.py",
      "code": "# skin_lesion_optimizer.py - \r\n\r\nimport streamlit as st\r\nimport time\r\nfrom datetime import datetime\r\nimport asyncio\r\nimport threading\r\n\r\nclass SkinLesionOptimizer:\r\n    \"\"\"Optimizes skin lesion analysis to prevent timeouts and improve performance\"\"\"\r\n    \r\n    def __init__(self, assistant):\r\n        self.assistant = assistant\r\n        \r\n    def quick_skin_analysis(self, image_path: str, user_query: str = None) -> dict:\r\n        \"\"\"Perform quick skin lesion analysis with timeout prevention\"\"\"\r\n        \r\n        try:\r\n            # Step 1: Quick image processing (usually fast)\r\n            st.info(\"Step 1/3: Processing image...\")\r\n            image_analysis = self.assistant.image_processor.process_medical_image(image_path)\r\n            \r\n            # Step 2: Extract key features without full AI analysis\r\n            st.info(\"Step 2/3: Extracting features...\")\r\n            quick_features = self._extract_quick_features(image_analysis)\r\n            \r\n            # Step 3: Generate focused response\r\n            st.info(\"Step 3/3: Generating analysis...\")\r\n            if user_query:\r\n                simplified_query = self._simplify_skin_query(user_query, quick_features)\r\n            else:\r\n                simplified_query = self._create_default_skin_query(quick_features)\r\n            \r\n            # Use shorter, more focused prompt\r\n            response = self._generate_quick_skin_response(simplified_query, quick_features)\r\n            \r\n            return {\r\n                'success': True,\r\n                'image_analysis': image_analysis,\r\n                'quick_features': quick_features,\r\n                'response': response,\r\n                'processing_time': 'Optimized for speed'\r\n            }\r\n            \r\n        except Exception as e:\r\n            return {\r\n                'success': False,\r\n                'error': str(e),\r\n                'fallback_response': self._generate_fallback_response()\r\n            }\r\n    \r\n    def _extract_quick_features(self, image_analysis: dict) -> dict:\r\n        \"\"\"Extract quick visual features without complex AI processing\"\"\"\r\n        \r\n        features = {\r\n            'image_quality': 'good',  # Default assumption\r\n            'has_findings': False,\r\n            'basic_description': 'skin lesion image uploaded'\r\n        }\r\n        \r\n        try:\r\n            # Extract basic info from image analysis\r\n            if image_analysis.get('processing_status') == 'success':\r\n                characteristics = image_analysis.get('characteristics', {})\r\n                \r\n                features['image_quality'] = characteristics.get('quality_category', 'fair')\r\n                features['has_findings'] = len(image_analysis.get('medical_findings', {}).get('findings', [])) > 0\r\n                \r\n                # Basic description based on findings\r\n                findings = image_analysis.get('medical_findings', {}).get('findings', [])\r\n                if findings:\r\n                    top_finding = findings[0].get('finding', 'lesion')\r\n                    features['basic_description'] = f\"possible {top_finding}\"\r\n                \r\n        except Exception:\r\n            pass  # Use defaults\r\n            \r\n        return features\r\n    \r\n    def _simplify_skin_query(self, user_query: str, features: dict) -> str:\r\n        \"\"\"Simplify user query to prevent timeout\"\"\"\r\n        \r\n        # Keep only essential parts of the query\r\n        simplified = user_query[:200]  # Limit length\r\n        \r\n        # Add quick context\r\n        context = f\"Image quality: {features['image_quality']}. \"\r\n        if features['has_findings']:\r\n            context += f\"Description: {features['basic_description']}. \"\r\n        \r\n        return context + simplified\r\n    \r\n    def _create_default_skin_query(self, features: dict) -> str:\r\n        \"\"\"Create default query for skin lesion analysis\"\"\"\r\n        \r\n        base_query = \"Analyze this skin lesion image. \"\r\n        \r\n        if features['image_quality'] == 'poor':\r\n            base_query += \"Note: Image quality may limit analysis. \"\r\n        \r\n        base_query += \"Provide ABCDE assessment and recommendations.\"\r\n        \r\n        return base_query\r\n    \r\n    def _generate_quick_skin_response(self, query: str, features: dict) -> str:\r\n        \"\"\"Generate quick response optimized for speed\"\"\"\r\n        \r\n        try:\r\n            # Very short, focused prompt\r\n            prompt = f\"\"\"Skin lesion analysis request: {query}\r\n\r\nProvide a brief response (under 200 words) with:\r\n1. Visual assessment\r\n2. ABCDE criteria (if applicable) \r\n3. Concern level (low/medium/high)\r\n4. Next steps\r\n\r\nBe concise and include medical disclaimer.\"\"\"\r\n\r\n            # Use shortest timeout and smaller response\r\n            response = self.assistant.llama_processor.generate_response(\r\n                prompt, \r\n                temperature=0.2, \r\n                max_tokens=400,  # Smaller response\r\n                timeout=90       # Shorter timeout\r\n            )\r\n            \r\n            return response\r\n            \r\n        except Exception as e:\r\n            return self._generate_fallback_response()\r\n    \r\n    def _generate_fallback_response(self) -> str:\r\n        \"\"\"Generate fallback response when AI analysis fails\"\"\"\r\n        \r\n        return \"\"\"**Skin Lesion Analysis - Quick Assessment**\r\n\r\nI've received your skin lesion image but encountered processing difficulties. Here's general guidance:\r\n\r\n**ABCDE Criteria to Consider:**\r\n- **A**symmetry: Is one half different from the other?\r\n- **B**order: Are edges irregular, scalloped, or poorly defined?\r\n- **C**olor: Are there multiple colors or uneven distribution?\r\n- **D**iameter: Is it larger than 6mm (pencil eraser size)?\r\n- **E**volving: Has it changed in size, shape, or color?\r\n\r\n**Immediate Action Needed If:**\r\n- Any ABCDE criteria are present\r\n- Bleeding, itching, or tenderness\r\n- Recent changes in appearance\r\n- Irregular or changing borders\r\n\r\n**Next Steps:**\r\n1. **Consult a dermatologist immediately** for professional evaluation\r\n2. **Take photos** to track any changes\r\n3. **Avoid sun exposure** to the area\r\n4. **Don't delay** if you have concerns\r\n\r\n**IMPORTANT:** This is a general guide only. Skin lesion evaluation requires professional dermatological assessment. Any concerning features warrant immediate medical attention.\r\n\r\n**Emergency:** If bleeding, rapid growth, or severe changes occur, seek immediate medical care.\"\"\"\r\n\r\ndef optimize_skin_lesion_chat():\r\n    \"\"\"Add to main.py to optimize skin lesion processing\"\"\"\r\n    \r\n    st.markdown(\"\"\"\r\n    ###  Skin Lesion Analysis Tips\r\n    \r\n    **For faster processing:**\r\n    - Use clear, well-lit images\r\n    - Keep questions short and specific\r\n    - Focus on specific concerns (color, size, shape)\r\n    \r\n    **If analysis times out:**\r\n    - Try simpler questions like \"Is this concerning?\"\r\n    - Upload smaller image files\r\n    - Ask about specific ABCDE criteria only\r\n    \"\"\")\r\n\r\n# Usage in main.py chat function\r\ndef handle_skin_lesion_timeout(assistant, image_path, user_query, image_type):\r\n    \"\"\"Handle skin lesion analysis with timeout prevention\"\"\"\r\n    \r\n    if \"skin\" in image_type.lower():\r\n        st.warning(\"\ud83d\udd04 Skin lesion analysis detected. Using optimized processing...\")\r\n        \r\n        optimizer = SkinLesionOptimizer(assistant)\r\n        result = optimizer.quick_skin_analysis(image_path, user_query)\r\n        \r\n        if result['success']:\r\n            return result['response']\r\n        else:\r\n            st.error(\"Analysis timed out. Using fallback response.\")\r\n            return result['fallback_response']\r\n    \r\n    # For non-skin images, use standard processing\r\n    return None\r\n\r\nif __name__ == \"__main__\":\r\n    print(\"Skin Lesion Optimizer loaded successfully!\")\r\n    print(\"This module optimizes skin lesion analysis to prevent timeouts.\")",
      "language": "python",
      "functions": [
        {
          "name": "optimize_skin_lesion_chat",
          "line": 163,
          "args": [],
          "docstring": "Add to main.py to optimize skin lesion processing",
          "decorators": []
        },
        {
          "name": "handle_skin_lesion_timeout",
          "line": 181,
          "args": [
            "assistant",
            "image_path",
            "user_query",
            "image_type"
          ],
          "docstring": "Handle skin lesion analysis with timeout prevention",
          "decorators": []
        },
        {
          "name": "__init__",
          "line": 12,
          "args": [
            "self",
            "assistant"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "quick_skin_analysis",
          "line": 15,
          "args": [
            "self",
            "image_path",
            "user_query"
          ],
          "docstring": "Perform quick skin lesion analysis with timeout prevention",
          "decorators": []
        },
        {
          "name": "_extract_quick_features",
          "line": 52,
          "args": [
            "self",
            "image_analysis"
          ],
          "docstring": "Extract quick visual features without complex AI processing",
          "decorators": []
        },
        {
          "name": "_simplify_skin_query",
          "line": 80,
          "args": [
            "self",
            "user_query",
            "features"
          ],
          "docstring": "Simplify user query to prevent timeout",
          "decorators": []
        },
        {
          "name": "_create_default_skin_query",
          "line": 93,
          "args": [
            "self",
            "features"
          ],
          "docstring": "Create default query for skin lesion analysis",
          "decorators": []
        },
        {
          "name": "_generate_quick_skin_response",
          "line": 105,
          "args": [
            "self",
            "query",
            "features"
          ],
          "docstring": "Generate quick response optimized for speed",
          "decorators": []
        },
        {
          "name": "_generate_fallback_response",
          "line": 133,
          "args": [
            "self"
          ],
          "docstring": "Generate fallback response when AI analysis fails",
          "decorators": []
        }
      ],
      "classes": [
        {
          "name": "SkinLesionOptimizer",
          "line": 9,
          "methods": [
            "__init__",
            "quick_skin_analysis",
            "_extract_quick_features",
            "_simplify_skin_query",
            "_create_default_skin_query",
            "_generate_quick_skin_response",
            "_generate_fallback_response"
          ],
          "docstring": "Optimizes skin lesion analysis to prevent timeouts and improve performance",
          "bases": []
        }
      ],
      "imports": [
        "streamlit",
        "time",
        "datetime.datetime",
        "asyncio",
        "threading"
      ],
      "complexity": "medium",
      "timestamp": "2025-10-20T09:01:08.426542"
    },
    "7af70ab94ac0a14e91d47c8f89ff768a": {
      "filename": "test_medical_assistant.py",
      "code": "# test_medical_assistant.py\r\n\"\"\"\r\nTest and demonstration script for the Multimodal Medical Assistant\r\nThis script validates functionality and provides usage examples\r\n\"\"\"\r\n\r\nimport os\r\nimport sys\r\nimport tempfile\r\nimport logging\r\nfrom pathlib import Path\r\nfrom datetime import datetime\r\nimport json\r\n\r\n# Add src to path for imports\r\nsys.path.append(str(Path(__file__).parent / \"src\"))\r\n\r\nfrom src.models.llama_processor import LlamaProcessor\r\nfrom src.models.image_processor import MedicalImageProcessor\r\nfrom src.models.text_processor import MedicalTextProcessor\r\nfrom src.rag.retrieval_system import MedicalRAGSystem\r\nfrom src.utils.privacy_utils import DataDeidentifier\r\nfrom src.utils.file_handler import FileHandler\r\nfrom config import MedicalAssistantConfig\r\n\r\nclass MedicalAssistantTester:\r\n    \"\"\"Test suite for the Medical Assistant\"\"\"\r\n    \r\n    def __init__(self):\r\n        self.logger = self._setup_logging()\r\n        self.config = MedicalAssistantConfig()\r\n        self.test_results = {}\r\n        self.sample_data_dir = Path(\"data/sample_data\")\r\n        self.sample_data_dir.mkdir(parents=True, exist_ok=True)\r\n        \r\n    def _setup_logging(self):\r\n        \"\"\"Setup logging for tests\"\"\"\r\n        logging.basicConfig(\r\n            level=logging.INFO,\r\n            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\r\n        )\r\n        return logging.getLogger(__name__)\r\n    \r\n    def create_sample_data(self):\r\n        \"\"\"Create sample medical data for testing\"\"\"\r\n        print(\" Creating sample medical data...\")\r\n        \r\n        # Sample medical text\r\n        sample_medical_text = \"\"\"\r\n        MEDICAL RECORD - SAMPLE CASE\r\n        ============================\r\n        \r\n        Patient: John Smith\r\n        DOB: 01/15/1975\r\n        MRN: 123456789\r\n        Date: 09/27/2025\r\n        \r\n        CHIEF COMPLAINT:\r\n        45-year-old male presents with acute chest pain and shortness of breath.\r\n        \r\n        HISTORY OF PRESENT ILLNESS:\r\n        Patient reports sudden onset of severe chest pain approximately 2 hours prior to arrival.\r\n        Pain is described as crushing, substernal, 8/10 severity, radiating to left arm and jaw.\r\n        Associated with diaphoresis, nausea, and mild dyspnea. Denies previous cardiac events.\r\n        \r\n        PAST MEDICAL HISTORY:\r\n        - Hypertension, controlled on lisinopril 10mg daily\r\n        - Hyperlipidemia, on atorvastatin 40mg daily\r\n        - Type 2 diabetes mellitus, diet controlled\r\n        - No previous hospitalizations\r\n        \r\n        MEDICATIONS:\r\n        - Lisinopril 10mg PO daily\r\n        - Atorvastatin 40mg PO daily\r\n        - Metformin 500mg PO BID\r\n        \r\n        ALLERGIES:\r\n        NKDA (No Known Drug Allergies)\r\n        \r\n        PHYSICAL EXAMINATION:\r\n        Vital Signs: BP 160/95, HR 110, RR 24, O2 Sat 94% on room air, Temp 98.6\u00b0F\r\n        General: Anxious appearing male in moderate acute distress\r\n        HEENT: PERRLA, no JVD\r\n        Cardiovascular: Tachycardic, regular rhythm, S1 S2 present, no murmurs\r\n        Pulmonary: Bilateral crackles at bases, otherwise clear\r\n        Abdomen: Soft, non-tender, non-distended\r\n        Extremities: No peripheral edema\r\n        \r\n        DIAGNOSTIC STUDIES:\r\n        EKG: ST elevation in leads II, III, aVF consistent with inferior STEMI\r\n        Chest X-ray: Mild pulmonary edema, normal cardiac silhouette\r\n        Laboratory Results:\r\n        - Troponin I: 3.2 ng/mL (elevated, normal <0.04)\r\n        - CK-MB: 45 ng/mL (elevated)\r\n        - BNP: 1200 pg/mL (elevated)\r\n        - CBC: WBC 12.5, Hgb 14.2, Plt 285\r\n        - BMP: Na 138, K 4.2, Cl 102, CO2 22, BUN 18, Cr 1.1, Glu 165\r\n        \r\n        ASSESSMENT AND PLAN:\r\n        1. Acute ST-elevation myocardial infarction (STEMI) - inferior wall\r\n           - Emergent cardiac catheterization\r\n           - Dual antiplatelet therapy: ASA 325mg, Clopidogrel 600mg loading dose\r\n           - Anticoagulation with heparin per protocol\r\n           - Atorvastatin 80mg daily\r\n           - Metoprolol 25mg BID when stable\r\n        \r\n        2. Acute heart failure\r\n           - Furosemide 40mg IV PRN\r\n           - Monitor I/O, daily weights\r\n           - ACE inhibitor when stable\r\n        \r\n        3. Diabetes mellitus\r\n           - Continue current regimen\r\n           - Monitor blood glucose closely\r\n        \r\n        DISPOSITION:\r\n        Patient admitted to cardiac ICU for post-catheterization monitoring.\r\n        Cardiology consulted. Family notified.\r\n        \r\n        Dr. Sarah Johnson, MD\r\n        Emergency Medicine\r\n        Phone: (555) 123-4567\r\n        \"\"\"\r\n        \r\n        # Save sample text\r\n        with open(self.sample_data_dir / \"sample_medical_record.txt\", 'w') as f:\r\n            f.write(sample_medical_text)\r\n        \r\n        # Create sample image description (since we can't create actual medical images)\r\n        sample_image_info = \"\"\"\r\n        SAMPLE CHEST X-RAY REPORT\r\n        =========================\r\n        \r\n        Study: Chest X-ray, PA and Lateral\r\n        Date: 09/27/2025\r\n        Indication: Chest pain, rule out pneumonia\r\n        \r\n        FINDINGS:\r\n        - Heart size is mildly enlarged\r\n        - Bilateral lower lobe opacities consistent with pulmonary edema\r\n        - No pneumothorax or pleural effusion\r\n        - Bony structures intact\r\n        - No acute infiltrates\r\n        \r\n        IMPRESSION:\r\n        Mild cardiomegaly with pulmonary edema. Correlate clinically.\r\n        \"\"\"\r\n        \r\n        with open(self.sample_data_dir / \"sample_xray_report.txt\", 'w') as f:\r\n            f.write(sample_image_info)\r\n        \r\n        print(\" Sample data created successfully\")\r\n        return True\r\n    \r\n    def test_configuration(self):\r\n        \"\"\"Test system configuration\"\"\"\r\n        print(\"\\n\ud83d\udd27 Testing system configuration...\")\r\n        \r\n        try:\r\n            # Validate configuration\r\n            validation_result = self.config.validate_config()\r\n            \r\n            if validation_result['valid']:\r\n                print(\" Configuration validation passed\")\r\n                self.test_results['configuration'] = 'PASS'\r\n            else:\r\n                print(\"\u274c Configuration validation failed:\")\r\n                for error in validation_result['errors']:\r\n                    print(f\"   - {error}\")\r\n                self.test_results['configuration'] = 'FAIL'\r\n                \r\n            # Test environment info\r\n            env_info = self.config.get_environment_info()\r\n            print(f\" Environment: {env_info['platform']}\")\r\n            print(f\" Python: {env_info['python_version']}\")\r\n            print(f\" PyTorch: {env_info['torch_version']}\")\r\n            print(f\" CUDA: {'Available' if env_info['cuda_available'] else 'Not Available'}\")\r\n            \r\n            return validation_result['valid']\r\n            \r\n        except Exception as e:\r\n            print(f\"\u274c Configuration test failed: {e}\")\r\n            self.test_results['configuration'] = 'ERROR'\r\n            return False\r\n    \r\n    def test_llama_processor(self):\r\n        \"\"\"Test Llama processor functionality\"\"\"\r\n        print(\"\\n\ud83e\udd99 Testing Llama processor...\")\r\n        \r\n        try:\r\n            processor = LlamaProcessor()\r\n            \r\n            # Test connection\r\n            if not processor.check_status():\r\n                print(\"\u274c Ollama service not available\")\r\n                self.test_results['llama'] = 'SKIP'\r\n                return False\r\n            \r\n            # Test basic response generation\r\n            test_prompt = \"What is pneumonia? Provide a brief medical explanation.\"\r\n            response = processor.generate_response(test_prompt, max_tokens=200)\r\n            \r\n            if response and len(response) > 10:\r\n                print(\" Basic response generation working\")\r\n                print(f\" Sample response: {response[:100]}...\")\r\n                \r\n                # Test medical response\r\n                medical_query = \"A patient presents with chest pain and elevated troponin. What could this indicate?\"\r\n                medical_response = processor.generate_medical_response(medical_query)\r\n                \r\n                if \"disclaimer\" in medical_response.lower():\r\n                    print(\" Medical disclaimer included\")\r\n                    self.test_results['llama'] = 'PASS'\r\n                    return True\r\n                else:\r\n                    print(\"  Medical disclaimer missing\")\r\n                    self.test_results['llama'] = 'PARTIAL'\r\n                    return True\r\n            else:\r\n                print(\"\u274c Response generation failed\")\r\n                self.test_results['llama'] = 'FAIL'\r\n                return False\r\n                \r\n        except Exception as e:\r\n            print(f\"\u274c Llama processor test failed: {e}\")\r\n            self.test_results['llama'] = 'ERROR'\r\n            return False\r\n    \r\n    def test_text_processor(self):\r\n        \"\"\"Test medical text processor\"\"\"\r\n        print(\"\\n Testing medical text processor...\")\r\n        \r\n        try:\r\n            processor = MedicalTextProcessor()\r\n            \r\n            # Test with sample medical text\r\n            sample_text = \"\"\"\r\n            Patient presents with chest pain, elevated troponin levels, and EKG changes \r\n            consistent with myocardial infarction. Started on aspirin and heparin.\r\n            Blood pressure 150/90, heart rate 110 bpm.\r\n            \"\"\"\r\n            \r\n            # Process the text\r\n            result = processor.process_medical_text(sample_text)\r\n            \r\n            if result.get('processing_status') == 'success':\r\n                print(\" Text processing successful\")\r\n                \r\n                # Check for medical entities\r\n                entities = result.get('entities', {})\r\n                if any(entities.values()):\r\n                    print(f\" Medical entities detected: {len(entities)} categories\")\r\n                    \r\n                # Check embeddings\r\n                embeddings = result.get('embeddings')\r\n                if embeddings:\r\n                    print(\" Text embeddings generated\")\r\n                    \r\n                # Check clinical assessment\r\n                assessment = result.get('clinical_assessment', {})\r\n                if assessment:\r\n                    print(f\" Clinical assessment: complexity={assessment.get('complexity_score', 0):.2f}\")\r\n                \r\n                self.test_results['text_processor'] = 'PASS'\r\n                return True\r\n            else:\r\n                print(\"\u274c Text processing failed\")\r\n                self.test_results['text_processor'] = 'FAIL'\r\n                return False\r\n                \r\n        except Exception as e:\r\n            print(f\"\u274c Text processor test failed: {e}\")\r\n            self.test_results['text_processor'] = 'ERROR'\r\n            return False\r\n    \r\n    def test_image_processor(self):\r\n        \"\"\"Test medical image processor\"\"\"\r\n        print(\"\\n  Testing medical image processor...\")\r\n        \r\n        try:\r\n            processor = MedicalImageProcessor()\r\n            \r\n            if not processor.check_medclip_status():\r\n                print(\"  MedCLIP model not available, skipping image tests\")\r\n                self.test_results['image_processor'] = 'SKIP'\r\n                return True\r\n            \r\n            # Since we can't create actual medical images in this test,\r\n            # we'll test the processor initialization and methods\r\n            print(\" MedCLIP model loaded successfully\")\r\n            \r\n            # Test supported formats\r\n            supported_formats = ['.dcm', '.png', '.jpg', '.jpeg']\r\n            print(f\" Supported formats: {supported_formats}\")\r\n            \r\n            self.test_results['image_processor'] = 'PASS'\r\n            return True\r\n            \r\n        except Exception as e:\r\n            print(f\"\u274c Image processor test failed: {e}\")\r\n            self.test_results['image_processor'] = 'ERROR'\r\n            return False\r\n    \r\n    def test_rag_system(self):\r\n        \"\"\"Test RAG system functionality\"\"\"\r\n        print(\"\\n Testing RAG system...\")\r\n        \r\n        try:\r\n            rag_system = MedicalRAGSystem()\r\n            \r\n            if not rag_system.check_status():\r\n                print(\"\u274c RAG system not ready\")\r\n                self.test_results['rag_system'] = 'FAIL'\r\n                return False\r\n            \r\n            # Add sample document\r\n            sample_doc = \"\"\"\r\n            Myocardial infarction (MI), commonly known as a heart attack, occurs when \r\n            blood flow to the heart muscle is blocked. Symptoms include chest pain, \r\n            shortness of breath, and nausea. Treatment includes aspirin, anticoagulants, \r\n            and emergency revascularization procedures.\r\n            \"\"\"\r\n            \r\n            doc_id = rag_system.add_document(sample_doc, \"test_document.txt\")\r\n            \r\n            if doc_id:\r\n                print(\" Document added to RAG system\")\r\n                \r\n                # Test query\r\n                query_results = rag_system.query(\"What is myocardial infarction?\", max_results=3)\r\n                \r\n                if query_results:\r\n                    print(f\" Query returned {len(query_results)} results\")\r\n                    print(f\" Top result score: {query_results[0]['score']:.3f}\")\r\n                    \r\n                    # Test statistics\r\n                    stats = rag_system.get_statistics()\r\n                    print(f\" RAG statistics: {stats['total_documents']} docs, {stats['total_chunks']} chunks\")\r\n                    \r\n                    self.test_results['rag_system'] = 'PASS'\r\n                    return True\r\n                else:\r\n                    print(\"\u274c Query returned no results\")\r\n                    self.test_results['rag_system'] = 'PARTIAL'\r\n                    return False\r\n            else:\r\n                print(\"\u274c Failed to add document\")\r\n                self.test_results['rag_system'] = 'FAIL'\r\n                return False\r\n                \r\n        except Exception as e:\r\n            print(f\"\u274c RAG system test failed: {e}\")\r\n            self.test_results['rag_system'] = 'ERROR'\r\n            return False\r\n    \r\n    def test_privacy_utils(self):\r\n        \"\"\"Test privacy and de-identification utilities\"\"\"\r\n        print(\"\\n  Testing privacy utilities...\")\r\n        \r\n        try:\r\n            deidentifier = DataDeidentifier()\r\n            \r\n            # Test text with PHI\r\n            phi_text = \"\"\"\r\n            Patient: John Smith\r\n            DOB: 01/15/1975\r\n            Phone: (555) 123-4567\r\n            SSN: 123-45-6789\r\n            Email: john.smith@email.com\r\n            Address: 123 Main St, Anytown, CA 90210\r\n            MRN: MR123456\r\n            \r\n            Mr. Smith is a 48-year-old male who presents with chest pain.\r\n            \"\"\"\r\n            \r\n            # De-identify the text\r\n            deidentified = deidentifier.deidentify_text(phi_text)\r\n            \r\n            # Validate de-identification\r\n            validation = deidentifier.validate_deidentification(phi_text, deidentified)\r\n            \r\n            print(f\" De-identification completed\")\r\n            print(f\" Confidence score: {validation['confidence_score']:.2f}\")\r\n            print(f\" Validation passed: {validation['validation_passed']}\")\r\n            \r\n            if validation['confidence_score'] > 0.8:\r\n                print(\" High confidence de-identification\")\r\n                self.test_results['privacy'] = 'PASS'\r\n                return True\r\n            else:\r\n                print(\"  Low confidence de-identification\")\r\n                self.test_results['privacy'] = 'PARTIAL'\r\n                return True\r\n                \r\n        except Exception as e:\r\n            print(f\"\u274c Privacy utilities test failed: {e}\")\r\n            self.test_results['privacy'] = 'ERROR'\r\n            return False\r\n    \r\n    def test_file_handler(self):\r\n        \"\"\"Test file handling utilities\"\"\"\r\n        print(\"\\n\ud83d\udcc1 Testing file handler...\")\r\n        \r\n        try:\r\n            file_handler = FileHandler()\r\n            \r\n            # Test supported formats\r\n            formats = file_handler.get_supported_formats()\r\n            print(f\" Supported formats: {len(formats['all'])} total\")\r\n            \r\n            # Create a temporary text file\r\n            with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as temp_file:\r\n                temp_file.write(\"This is a test medical document for validation.\")\r\n                temp_path = temp_file.name\r\n            \r\n            try:\r\n                # Test text extraction\r\n                extracted_text = file_handler.extract_text(temp_path)\r\n                \r\n                if extracted_text and len(extracted_text) > 0:\r\n                    print(\" Text extraction working\")\r\n                    \r\n                    # Test metadata extraction\r\n                    metadata = file_handler.get_file_metadata(temp_path)\r\n                    \r\n                    if metadata and 'filename' in metadata:\r\n                        print(\" Metadata extraction working\")\r\n                        self.test_results['file_handler'] = 'PASS'\r\n                        return True\r\n                    else:\r\n                        print(\"\u274c Metadata extraction failed\")\r\n                        self.test_results['file_handler'] = 'PARTIAL'\r\n                        return False\r\n                else:\r\n                    print(\"\u274c Text extraction failed\")\r\n                    self.test_results['file_handler'] = 'FAIL'\r\n                    return False\r\n                    \r\n            finally:\r\n                # Cleanup\r\n                os.unlink(temp_path)\r\n                \r\n        except Exception as e:\r\n            print(f\"\u274c File handler test failed: {e}\")\r\n            self.test_results['file_handler'] = 'ERROR'\r\n            return False\r\n    \r\n    def test_integration(self):\r\n        \"\"\"Test integration between components\"\"\"\r\n        print(\"\\n Testing component integration...\")\r\n        \r\n        try:\r\n            # Test cross-modal analysis simulation\r\n            print(\" Simulating cross-modal analysis...\")\r\n            \r\n            # Create sample multimodal data\r\n            sample_image_analysis = {\r\n                'modality': 'chest x-ray',\r\n                'findings': [\r\n                    {'finding': 'cardiomegaly', 'confidence': 0.85},\r\n                    {'finding': 'pulmonary edema', 'confidence': 0.72}\r\n                ],\r\n                'overall_assessment': 'abnormal'\r\n            }\r\n            \r\n            sample_text_analysis = {\r\n                'entities': {\r\n                    'symptoms': [{'text': 'chest pain'}, {'text': 'shortness of breath'}],\r\n                    'diagnoses': [{'text': 'myocardial infarction'}]\r\n                },\r\n                'clinical_assessment': {\r\n                    'complexity_score': 0.8,\r\n                    'urgency_indicators': ['acute']\r\n                }\r\n            }\r\n            \r\n            multimodal_data = {\r\n                'images': [{'filename': 'chest_xray.png', 'analysis': sample_image_analysis}],\r\n                'texts': [{'filename': 'clinical_note.txt', 'analysis': sample_text_analysis}],\r\n                'query': 'Correlate the chest X-ray findings with the clinical presentation'\r\n            }\r\n            \r\n            # Test with Llama processor if available\r\n            llama_processor = LlamaProcessor()\r\n            if llama_processor.check_status():\r\n                result = llama_processor.cross_modal_analysis(multimodal_data)\r\n                \r\n                if result.get('response'):\r\n                    print(\" Cross-modal analysis successful\")\r\n                    print(f\" Generated {len(result.get('evidence', []))} evidence items\")\r\n                    self.test_results['integration'] = 'PASS'\r\n                    return True\r\n                else:\r\n                    print(\"\u274c Cross-modal analysis failed\")\r\n                    self.test_results['integration'] = 'FAIL'\r\n                    return False\r\n            else:\r\n                print(\"  Llama not available, skipping integration test\")\r\n                self.test_results['integration'] = 'SKIP'\r\n                return True\r\n                \r\n        except Exception as e:\r\n            print(f\"\u274c Integration test failed: {e}\")\r\n            self.test_results['integration'] = 'ERROR'\r\n            return False\r\n    \r\n    def generate_test_report(self):\r\n        \"\"\"Generate comprehensive test report\"\"\"\r\n        print(\"\\n\" + \"=\" * 60)\r\n        print(\" TEST REPORT\")\r\n        print(\"=\" * 60)\r\n        \r\n        total_tests = len(self.test_results)\r\n        passed_tests = sum(1 for result in self.test_results.values() if result == 'PASS')\r\n        failed_tests = sum(1 for result in self.test_results.values() if result == 'FAIL')\r\n        error_tests = sum(1 for result in self.test_results.values() if result == 'ERROR')\r\n        skipped_tests = sum(1 for result in self.test_results.values() if result == 'SKIP')\r\n        partial_tests = sum(1 for result in self.test_results.values() if result == 'PARTIAL')\r\n        \r\n        print(f\" Total Tests: {total_tests}\")\r\n        print(f\" Passed: {passed_tests}\")\r\n        print(f\"  Partial: {partial_tests}\")\r\n        print(f\"\u274c Failed: {failed_tests}\")\r\n        print(f\" Error: {error_tests}\")\r\n        print(f\"  Skipped: {skipped_tests}\")\r\n        \r\n        print(\"\\n Detailed Results:\")\r\n        for component, result in self.test_results.items():\r\n            status_icon = {\r\n                'PASS': '',\r\n                'FAIL': '\u274c',\r\n                'ERROR': '',\r\n                'SKIP': '',\r\n                'PARTIAL': ''\r\n            }.get(result, '\u2753')\r\n            \r\n            print(f\"{status_icon} {component}: {result}\")\r\n        \r\n        # Calculate overall score\r\n        score = (passed_tests + (partial_tests * 0.5)) / total_tests * 100\r\n        print(f\"\\n Overall Score: {score:.1f}%\")\r\n        \r\n        # Generate recommendations\r\n        print(\"\\n\ud83d\udca1 Recommendations:\")\r\n        \r\n        if failed_tests > 0 or error_tests > 0:\r\n            print(\"- Review failed components and check dependencies\")\r\n            print(\"- Ensure all required models are downloaded\")\r\n            print(\"- Verify Ollama service is running\")\r\n        \r\n        if skipped_tests > 0:\r\n            print(\"- Install missing dependencies for skipped tests\")\r\n            print(\"- Check model availability\")\r\n        \r\n        if score >= 80:\r\n            print(\" System is ready for use!\")\r\n        elif score >= 60:\r\n            print(\"  System partially functional - some features may not work\")\r\n        else:\r\n            print(\"\u274c System needs significant fixes before use\")\r\n        \r\n        # Save report to file\r\n        report_file = Path(\"test_report.json\")\r\n        report_data = {\r\n            'timestamp': datetime.now().isoformat(),\r\n            'results': self.test_results,\r\n            'summary': {\r\n                'total': total_tests,\r\n                'passed': passed_tests,\r\n                'failed': failed_tests,\r\n                'error': error_tests,\r\n                'skipped': skipped_tests,\r\n                'partial': partial_tests,\r\n                'score': score\r\n            }\r\n        }\r\n        \r\n        with open(report_file, 'w') as f:\r\n            json.dump(report_data, f, indent=2)\r\n        \r\n        print(f\"\\n Detailed report saved to: {report_file}\")\r\n        \r\n        return score >= 60  # Return True if system is functional\r\n    \r\n    def run_all_tests(self):\r\n        \"\"\"Run all test suites\"\"\"\r\n        print(\" Starting Medical Assistant Test Suite\")\r\n        print(\"=\" * 60)\r\n        \r\n        # Create sample data first\r\n        self.create_sample_data()\r\n        \r\n        # Run all tests\r\n        test_functions = [\r\n            self.test_configuration,\r\n            self.test_file_handler,\r\n            self.test_privacy_utils,\r\n            self.test_text_processor,\r\n            self.test_image_processor,\r\n            self.test_rag_system,\r\n            self.test_llama_processor,\r\n            self.test_integration\r\n        ]\r\n        \r\n        for test_func in test_functions:\r\n            try:\r\n                test_func()\r\n            except Exception as e:\r\n                component_name = test_func.__name__.replace('test_', '')\r\n                print(f\" Unexpected error in {component_name}: {e}\")\r\n                self.test_results[component_name] = 'ERROR'\r\n        \r\n        # Generate final report\r\n        return self.generate_test_report()\r\n\r\ndef main():\r\n    \"\"\"Main test function\"\"\"\r\n    print(\" Multimodal Medical Assistant - Test Suite\")\r\n    print(\"=\" * 60)\r\n    \r\n    tester = MedicalAssistantTester()\r\n    \r\n    if len(sys.argv) > 1:\r\n        test_name = sys.argv[1].replace('--', '').replace('-', '_')\r\n        test_method = getattr(tester, f'test_{test_name}', None)\r\n        \r\n        if test_method:\r\n            print(f\"Running single test: {test_name}\")\r\n            test_method()\r\n            tester.generate_test_report()\r\n        else:\r\n            print(f\"Unknown test: {test_name}\")\r\n            print(\"Available tests: configuration, file_handler, privacy_utils, text_processor, image_processor, rag_system, llama_processor, integration\")\r\n    else:\r\n        # Run all tests\r\n        success = tester.run_all_tests()\r\n        sys.exit(0 if success else 1)\r\n\r\nif __name__ == \"__main__\":\r\n    main()",
      "language": "python",
      "functions": [
        {
          "name": "main",
          "line": 616,
          "args": [],
          "docstring": "Main test function",
          "decorators": []
        },
        {
          "name": "__init__",
          "line": 29,
          "args": [
            "self"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "_setup_logging",
          "line": 36,
          "args": [
            "self"
          ],
          "docstring": "Setup logging for tests",
          "decorators": []
        },
        {
          "name": "create_sample_data",
          "line": 44,
          "args": [
            "self"
          ],
          "docstring": "Create sample medical data for testing",
          "decorators": []
        },
        {
          "name": "test_configuration",
          "line": 155,
          "args": [
            "self"
          ],
          "docstring": "Test system configuration",
          "decorators": []
        },
        {
          "name": "test_llama_processor",
          "line": 186,
          "args": [
            "self"
          ],
          "docstring": "Test Llama processor functionality",
          "decorators": []
        },
        {
          "name": "test_text_processor",
          "line": 229,
          "args": [
            "self"
          ],
          "docstring": "Test medical text processor",
          "decorators": []
        },
        {
          "name": "test_image_processor",
          "line": 276,
          "args": [
            "self"
          ],
          "docstring": "Test medical image processor",
          "decorators": []
        },
        {
          "name": "test_rag_system",
          "line": 304,
          "args": [
            "self"
          ],
          "docstring": "Test RAG system functionality",
          "decorators": []
        },
        {
          "name": "test_privacy_utils",
          "line": 356,
          "args": [
            "self"
          ],
          "docstring": "Test privacy and de-identification utilities",
          "decorators": []
        },
        {
          "name": "test_file_handler",
          "line": 400,
          "args": [
            "self"
          ],
          "docstring": "Test file handling utilities",
          "decorators": []
        },
        {
          "name": "test_integration",
          "line": 448,
          "args": [
            "self"
          ],
          "docstring": "Test integration between components",
          "decorators": []
        },
        {
          "name": "generate_test_report",
          "line": 507,
          "args": [
            "self"
          ],
          "docstring": "Generate comprehensive test report",
          "decorators": []
        },
        {
          "name": "run_all_tests",
          "line": 585,
          "args": [
            "self"
          ],
          "docstring": "Run all test suites",
          "decorators": []
        }
      ],
      "classes": [
        {
          "name": "MedicalAssistantTester",
          "line": 26,
          "methods": [
            "__init__",
            "_setup_logging",
            "create_sample_data",
            "test_configuration",
            "test_llama_processor",
            "test_text_processor",
            "test_image_processor",
            "test_rag_system",
            "test_privacy_utils",
            "test_file_handler",
            "test_integration",
            "generate_test_report",
            "run_all_tests"
          ],
          "docstring": "Test suite for the Medical Assistant",
          "bases": []
        }
      ],
      "imports": [
        "os",
        "sys",
        "tempfile",
        "logging",
        "pathlib.Path",
        "datetime.datetime",
        "json",
        "src.models.llama_processor.LlamaProcessor",
        "src.models.image_processor.MedicalImageProcessor",
        "src.models.text_processor.MedicalTextProcessor",
        "src.rag.retrieval_system.MedicalRAGSystem",
        "src.utils.privacy_utils.DataDeidentifier",
        "src.utils.file_handler.FileHandler",
        "config.MedicalAssistantConfig"
      ],
      "complexity": "high",
      "timestamp": "2025-10-20T09:01:08.427548"
    },
    "0f32d523c0e87884b26c69c256d1a617": {
      "filename": "test_app.py",
      "code": "# test_app.py\r\nimport faiss\r\nimport json\r\nimport numpy as np\r\nfrom sentence_transformers import SentenceTransformer\r\n\r\n# -------------------\r\n# CONFIG\r\n# -------------------\r\nBUGS_FILE = \"data/dummy_bugs.jsonl\"   # your bug dataset\r\nMODEL_NAME = \"all-MiniLM-L6-v2\"       # same model used during indexing\r\nFAISS_DIM = 384                       # embedding dimension\r\nTOP_K = 10                            # number of results to retrieve\r\n\r\n# -------------------\r\n# Load dataset\r\n# -------------------\r\nbugs = []\r\nwith open(BUGS_FILE, \"r\", encoding=\"utf-8\") as f:\r\n    for line in f:\r\n        bugs.append(json.loads(line))\r\n\r\n# -------------------\r\n# Load model & FAISS index\r\n# -------------------\r\nprint(\"\ud83d\udce5 Loading model & building FAISS index...\")\r\nmodel = SentenceTransformer(MODEL_NAME)\r\n\r\nbug_texts = [f\"{b['summary']} - {b['description']} ({b['platform']})\" for b in bugs]\r\nembeddings = model.encode(bug_texts, convert_to_numpy=True)\r\n\r\nindex = faiss.IndexFlatL2(FAISS_DIM)\r\nindex.add(embeddings)\r\nprint(f\"\u2705 FAISS index built with {len(bugs)} bug reports\")\r\n\r\n# -------------------\r\n# Search function\r\n# -------------------\r\ndef search_bugs(query, k=TOP_K, keyword_match=True):\r\n    query_vec = model.encode([query], convert_to_numpy=True)\r\n    D, I = index.search(np.array(query_vec), k)\r\n\r\n    results = [(bugs[idx], D[0][rank]) for rank, idx in enumerate(I[0])]\r\n\r\n    if keyword_match:\r\n        keyword_hits = [\r\n            (bug, 0.0) for bug in bugs\r\n            if query.lower() in bug[\"summary\"].lower() or query.lower() in bug[\"description\"].lower()\r\n        ]\r\n        results_dict = {r[0][\"summary\"]: r for r in results}\r\n        for bug, score in keyword_hits:\r\n            results_dict[bug[\"summary\"]] = (bug, score)\r\n        results = list(results_dict.values())\r\n\r\n    return results\r\n\r\n# -------------------\r\n# Severity & Priority heuristics\r\n# -------------------\r\ndef suggest_severity_priority(bug):\r\n    text = (bug[\"summary\"] + \" \" + bug[\"description\"]).lower()\r\n\r\n    if any(word in text for word in [\"crash\", \"data loss\", \"corruption\", \"unresponsive\", \"failover\"]):\r\n        return \"Severity: Critical\", \"Priority: High\"\r\n\r\n    if any(word in text for word in [\"latency\", \"delay\", \"slow\", \"high cpu\", \"memory leak\", \"timeout\"]):\r\n        return \"Severity: Major\", \"Priority: Medium\"\r\n\r\n    if any(word in text for word in [\"ui\", \"format\", \"glitch\", \"display\", \"alignment\"]):\r\n        return \"Severity: Minor\", \"Priority: Low\"\r\n\r\n    return \"Severity: Major\", \"Priority: Medium\"  # default\r\n\r\n# -------------------\r\n# Interactive loop\r\n# -------------------\r\nprint(\"\\n\ud83d\udca1 Type a query (or 'exit' to quit):\")\r\nwhile True:\r\n    query = input(\"Query: \")\r\n    if query.lower() in [\"exit\", \"quit\"]:\r\n        break\r\n\r\n    results = search_bugs(query, k=TOP_K, keyword_match=True)\r\n\r\n    if not results:\r\n        print(\"\u26a0\ufe0f No matches found.\")\r\n        continue\r\n\r\n    print(\"\\n\ud83d\udd0e Matches:\")\r\n    for i, (bug, score) in enumerate(results, 1):\r\n        print(f\"{i}. {bug['summary']} ({bug['platform']})\")\r\n        print(f\"   {bug['description'][:70]}...\")\r\n        print(f\"   Score: {score:.4f}\\n\")\r\n\r\n    try:\r\n        choice = int(input(\"Select issue number to view full defect: \")) - 1\r\n        if choice < 0 or choice >= len(results):\r\n            print(\"\u26a0\ufe0f Invalid choice.\")\r\n            continue\r\n    except ValueError:\r\n        print(\"\u26a0\ufe0f Please enter a valid number.\")\r\n        continue\r\n\r\n    selected = results[choice][0]\r\n\r\n    print(\"\\n\ud83d\udccc Full Defect Description:\")\r\n    print(selected[\"description\"])\r\n\r\n    severity, priority = suggest_severity_priority(selected)\r\n    print(severity)\r\n    print(priority)\r\n    print(\"-\" * 80)\r\n",
      "language": "python",
      "functions": [
        {
          "name": "search_bugs",
          "line": 39,
          "args": [
            "query",
            "k",
            "keyword_match"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "suggest_severity_priority",
          "line": 60,
          "args": [
            "bug"
          ],
          "docstring": null,
          "decorators": []
        }
      ],
      "classes": [],
      "imports": [
        "faiss",
        "json",
        "numpy",
        "sentence_transformers.SentenceTransformer"
      ],
      "complexity": "low",
      "timestamp": "2025-10-20T09:05:39.754577"
    },
    "190471e9f58719cc2d1232c96bbe014a": {
      "filename": "reporting.py",
      "code": "import pandas as pd\r\nimport matplotlib.pyplot as plt\r\nimport os\r\nimport numpy as np\r\nimport glob\r\nimport subprocess\r\nimport csv\r\nfrom datetime import datetime\r\nimport matplotlib.dates as mdates\r\nimport logging\r\nimport tkinter as tk\r\nfrom tkinter import ttk\r\nfrom matplotlib.backends.backend_tkagg import FigureCanvasTkAgg, NavigationToolbar2Tk\r\n\r\n# Set up logging\r\nlogging.basicConfig(filename='csv_parsing.log', level=logging.INFO,\r\n                    format='%(asctime)s - %(levelname)s - %(message)s')\r\n\r\n# Function to extract .gz files to CSV\r\ndef extract_gzip_to_csv(directory, seven_zip=\"C:\\\\Program Files\\\\7-Zip\\\\7z.exe\"):\r\n    for file in os.listdir(directory):\r\n        if file.endswith('.gz'):\r\n            gz_file_path = os.path.join(directory, file)\r\n            output_file = file[:-3]\r\n            output_path = os.path.join(directory, output_file)\r\n            # Construct 7Zip Command\r\n            command = [seven_zip, 'e', gz_file_path, f\"-o{directory}\", \"-y\"]\r\n            try:\r\n                # Use subprocess.run instead of Popen to wait for each extraction to complete\r\n                startupinfo = subprocess.STARTUPINFO()\r\n                startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\r\n                result = subprocess.run(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, startupinfo=startupinfo)\r\n                if result.returncode != 0:\r\n                    logging.error(f\"Error extracting {gz_file_path}\")\r\n            except subprocess.CalledProcessError as e:\r\n                logging.error(f\"Error extracting {gz_file_path}: {e}\")\r\n\r\n# Function to delete lines above 'Timestamp' header dynamically\r\ndef delete_lines_before_timestamp(input_file, output_file):\r\n    with open(input_file, 'r', newline='', encoding='utf-8') as infile, open(output_file, 'w', newline='', encoding='utf-8') as outfile:\r\n        reader = csv.reader(infile)\r\n        writer = csv.writer(outfile)\r\n\r\n        found_timestamp = False\r\n        for row in reader:\r\n            if row and row[0] == \"Timestamp\":\r\n                found_timestamp = True\r\n                writer.writerow(row)\r\n            elif found_timestamp:\r\n                writer.writerow(row)\r\n\r\n# Function to clean up mixed data types in columns\r\ndef clean_numeric_columns(df, cpu_columns):\r\n    for col in cpu_columns:\r\n        df[col] = pd.to_numeric(df[col], errors='coerce')  # Convert to numeric, coercing errors to NaN\r\n    return df\r\n\r\n# Function to format 'Timestamp' column and filter invalid rows\r\ndef cleanse_data(df):\r\n    # Convert 'Timestamp' column to datetime format\r\n    df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%d/%m/%Y %H:%M:%S', errors='coerce', dayfirst=True)\r\n    df = df.dropna(subset=['Timestamp'])  # Drop rows with invalid timestamps\r\n    return df\r\n\r\n# Function to inspect problematic files\r\ndef inspect_problematic_file(file_path):\r\n    try:\r\n        with open(file_path, 'r', encoding='utf-8') as f:\r\n            first_few_lines = [next(f) for _ in range(5)]\r\n        logging.info(f\"First few lines of {file_path}:\\n{''.join(first_few_lines)}\")\r\n    except Exception as e:\r\n        logging.error(f\"Error inspecting file {file_path}: {e}\")\r\n\r\n\r\ndef process_server_data(server_name, directory, threshold, file_type):\r\n    csv_files = glob.glob(os.path.join(directory, f\"*{server_name}*_edited.csv\"))\r\n    logging.info(f\"Processing {len(csv_files)} files for server {server_name}\")\r\n    dataframes = []\r\n\r\n    for csv_file in csv_files:\r\n        try:\r\n            df = pd.read_csv(csv_file, low_memory=False)\r\n            logging.info(f\"Successfully read file: {csv_file}\")\r\n\r\n            if df.empty:\r\n                logging.warning(f\"Empty DataFrame for file: {csv_file}\")\r\n                continue\r\n\r\n            cpu_columns = [col for col in df.columns if col.startswith(\"Processor\") and col.endswith(\"_Percent_Usage_1s_peak\")]\r\n            if not cpu_columns:\r\n                logging.warning(f\"No CPU usage columns found in {csv_file}\")\r\n                continue\r\n\r\n            df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%d/%m/%Y %H:%M:%S', errors='coerce')\r\n            df = df.dropna(subset=['Timestamp'])\r\n\r\n            df[cpu_columns] = df[cpu_columns].apply(pd.to_numeric, errors='coerce')\r\n\r\n            df['Filename'] = os.path.basename(csv_file)\r\n            df['Avg_CPU_Usage'] = df[cpu_columns].mean(axis=1)\r\n\r\n            df_filtered = df[df['Avg_CPU_Usage'] > threshold]\r\n\r\n            if df_filtered.empty:\r\n                logging.warning(f\"No data above {threshold}% CPU usage in {csv_file}\")\r\n                continue\r\n\r\n            columns_to_keep = ['Timestamp', 'Avg_CPU_Usage', 'Filename'] + cpu_columns\r\n            df_filtered = df_filtered[columns_to_keep]\r\n\r\n            dataframes.append(df_filtered)\r\n\r\n        except Exception as e:\r\n            logging.error(f\"Error processing file {csv_file}: {e}\")\r\n            logging.exception(\"Exception details:\")\r\n\r\n    if dataframes:\r\n        combined_df = pd.concat(dataframes).sort_values('Timestamp').reset_index(drop=True)\r\n        return combined_df, cpu_columns\r\n    else:\r\n        return None, []\r\n\r\ndef plot_server_graph(ax, server_name, data, cpu_columns):\r\n    for cpu in cpu_columns:\r\n        line = ax.plot(data['Timestamp'], data[cpu], label=cpu, linewidth=0.5)[0]\r\n        line.set_pickradius(5)\r\n        \r\n        # Store data as a custom attribute\r\n        line_data = data[['Timestamp', cpu, 'Filename']].to_dict('records')\r\n        line.set_picker(5) # Enable picking on the line\r\n        line.data_annotations = line_data # Store data as a custom attribute\r\n\r\n    ax.set_title(f\"{server_name}\", fontsize=8)\r\n    ax.tick_params(axis='both', which='major', labelsize=6)\r\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\r\n\r\ndef create_detailed_view(server_name, data, cpu_columns, root):\r\n    detail_window = tk.Toplevel(root)\r\n    detail_window.title(f\"Detailed View: {server_name}\")\r\n    detail_window.geometry(\"1000x600\")\r\n\r\n    fig, ax = plt.subplots(figsize=(10, 6))\r\n    plot_server_graph(ax, server_name, data, cpu_columns)\r\n    ax.set_xlabel('Timestamp')\r\n    ax.set_ylabel('CPU Usage (%)')\r\n    ax.legend(loc='upper left', bbox_to_anchor=(1, 1), title=\"Processors\")\r\n\r\n    canvas = FigureCanvasTkAgg(fig, master=detail_window)\r\n    canvas_widget = canvas.get_tk_widget()\r\n    canvas_widget.pack(fill=tk.BOTH, expand=True)\r\n\r\n    toolbar = NavigationToolbar2Tk(canvas, detail_window)\r\n    toolbar.update()\r\n    canvas_widget.pack(fill=tk.BOTH, expand=True)\r\n\r\n    current_annotation = None\r\n\r\n    # Function to handle hover text on clicking a point in the graph\r\n    def on_pick(event):\r\n        nonlocal current_annotation\r\n        logging.info(\"Pick event triggered\")  # Log when pick event occurs\r\n\r\n        # Only proceed for left mouse button click\r\n        if event.mouseevent.button != 1:\r\n            return\r\n\r\n        # Get the line and the corresponding data\r\n        line = event.artist\r\n        if hasattr(line, 'data_annotations'):\r\n            data = line.data_annotations\r\n            ind = event.ind[0]  # Index of the picked point\r\n            logging.info(f\"Picked point index: {ind}\")  # Log the index\r\n\r\n            if ind < len(data):\r\n                # Remove the previous annotation if it exists\r\n                if current_annotation:\r\n                    current_annotation.remove()\r\n\r\n                # Get data from the picked point\r\n                point = data[ind]\r\n                timestamp = point['Timestamp']\r\n                cpu_usage = point[line.get_label()]\r\n                filename = point['Filename']\r\n\r\n                # Create annotation text\r\n                annotation_text = f\"File: {filename}\\nTime: {timestamp}\\nCPU Usage: {cpu_usage:.2f}%\"\r\n\r\n                # Create new annotation on the graph\r\n                current_annotation = ax.annotate(\r\n                    annotation_text,\r\n                    xy=(mdates.date2num(pd.Timestamp(timestamp)), cpu_usage),\r\n                    xytext=(15, 15),\r\n                    textcoords='offset points',\r\n                    bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\r\n                    arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0')\r\n                )\r\n               \r\n                logging.info(f\"Annotation created for point: {annotation_text}\")  # Log annotation details\r\n                fig.canvas.draw_idle()\r\n\r\n    fig.canvas.mpl_connect('pick_event', on_pick)\r\n\r\n    # Add a click event to remove annotation when clicking outside of data points\r\n    def on_click(event):\r\n        nonlocal current_annotation\r\n        if event.inaxes is None or (event.xdata is None or event.ydata is None):\r\n            #If it's a pick event (on a point), we do not remove the annotation\r\n            if current_annotation:\r\n                current_annotation.remove()\r\n                current_annotation = None\r\n                fig.canvas.draw_idle()\r\n\r\n    fig.canvas.mpl_connect('button_press_event', on_click)\r\n    \r\ndef create_main_window(server_data, threshold, file_type):\r\n    root = tk.Tk()\r\n    root.title(\"CPU Usage Visualization\")\r\n    root.geometry(\"1200x800\")\r\n\r\n    # Create a frame for the graphs\r\n    main_frame = ttk.Frame(root)\r\n    main_frame.pack(fill=tk.BOTH, expand=True)\r\n\r\n    # Create a canvas with both vertical and horizontal scrollbars\r\n    canvas = tk.Canvas(main_frame)\r\n    v_scrollbar = ttk.Scrollbar(main_frame, orient=\"vertical\", command=canvas.yview)\r\n    h_scrollbar = ttk.Scrollbar(main_frame, orient=\"horizontal\", command=canvas.xview)\r\n    scrollable_frame = ttk.Frame(canvas)\r\n\r\n    scrollable_frame.bind(\r\n        \"<Configure>\",\r\n        lambda e: canvas.configure(\r\n            scrollregion=canvas.bbox(\"all\")\r\n        )\r\n    )\r\n\r\n    canvas.create_window((0, 0), window=scrollable_frame, anchor=\"nw\")\r\n    canvas.configure(yscrollcommand=v_scrollbar.set, xscrollcommand=h_scrollbar.set)\r\n\r\n    # Calculate the number of columns and rows\r\n    num_columns = 3\r\n    num_rows = len(server_data) // num_columns + (1 if len(server_data) % num_columns else 0)\r\n    \r\n    fig, axes = plt.subplots(nrows=num_rows, ncols=num_columns, figsize=(15, 5 * num_rows))\r\n    axes = axes.flatten()\r\n\r\n    for i, (server_name, (data, cpu_columns)) in enumerate(server_data.items()):\r\n        if data is not None and not data.empty:\r\n            plot_server_graph(axes[i], server_name, data, cpu_columns)\r\n\r\n            def create_click_handler(sn, d, cc):\r\n                return lambda event: create_detailed_view(sn, d, cc, root)\r\n\r\n            axes[i].figure.canvas.mpl_connect('button_press_event', create_click_handler(server_name, data, cpu_columns))\r\n\r\n    # Remove any unused subplots\r\n    for j in range(i + 1, len(axes)):\r\n        fig.delaxes(axes[j])\r\n\r\n    plt.tight_layout()\r\n\r\n    canvas_widget = FigureCanvasTkAgg(fig, scrollable_frame)\r\n    canvas_widget.draw()\r\n    canvas_widget.get_tk_widget().pack(fill=tk.BOTH, expand=True)\r\n\r\n    canvas.pack(side=\"left\", fill=\"both\", expand=True)\r\n    v_scrollbar.pack(side=\"right\", fill=\"y\")\r\n    h_scrollbar.pack(side=\"bottom\", fill=\"x\")\r\n\r\n    return root\r\n    \r\n# Function to delete unnecessary files\r\ndef delete_unwanted_csv_files(directory):\r\n    csv_files = glob.glob(os.path.join(directory, \"*.csv\"))\r\n    for file in csv_files:\r\n        if \"_edited.csv\" not in os.path.basename(file):\r\n            try:\r\n                os.remove(file)\r\n            except Exception as e:\r\n                logging.error(f\"Error deleting {file}: {e}\")\r\n\r\ndef process_directory():\r\n    file_type = input(\"Enter 'ADS' or 'ADH': \").strip().upper()\r\n    if file_type not in ['ADS', 'ADH']:\r\n        print(\"Invalid input. Please enter 'ADS' or 'ADH'.\")\r\n        return\r\n\r\n    region = input(\"Enter 'APAC' or 'BON': \").strip().upper()\r\n    if region not in ['APAC', 'BON']:\r\n        print(\"Invalid input. Please enter 'APAC' or 'BON'.\")\r\n        return\r\n\r\n    base_directory = os.getcwd()\r\n    directory = os.path.join(base_directory, f\"{region}_{file_type}\")\r\n\r\n    if not os.path.exists(directory):\r\n        print(f\"Directory {directory} does not exist.\")\r\n        return\r\n\r\n    # Extract gzip files first\r\n    extract_gzip_to_csv(directory)\r\n\r\n    # Process each CSV in the directory\r\n    server_names = set()\r\n    csv_files = glob.glob(os.path.join(directory, \"*.csv\"))\r\n\r\n    # Dynamically clean CSV files by deleting lines before 'Timestamp'\r\n    for csv_file in csv_files:\r\n        output_file = csv_file.replace(\".csv\", \"_edited.csv\")\r\n        delete_lines_before_timestamp(csv_file, output_file)\r\n        try:\r\n            server_name = os.path.basename(csv_file).split('_')[1] # Extract server name\r\n            server_names.add(server_name)\r\n        except IndexError:\r\n            logging.error(f\"Error extracting server name from file: {csv_file}\")\r\n\r\n    if not server_names:\r\n        print(\"No valid server names extracted. Exiting.\")\r\n        return\r\n\r\n    threshold = 10 # CPU usage threshold\r\n    server_data = {}\r\n\r\n    for server_name in server_names:\r\n        data, cpu_columns = process_server_data(server_name, directory, threshold, file_type)\r\n        if data is not None and not data.empty:\r\n            server_data[server_name] = (data, cpu_columns)\r\n\r\n    # Create and run the main window\r\n    root = create_main_window(server_data, threshold, file_type)\r\n    \r\n    # Set up a proper exit mechanism\r\n    def on_closing():\r\n        root.quit()\r\n        root.destroy()\r\n\r\n    root.protocol(\"WM_DELETE_WINDOW\", on_closing)\r\n    root.mainloop()\r\n\r\n    # Delete all unwanted CSV files after processing\r\n    delete_unwanted_csv_files(directory)\r\n\r\nif __name__ == '__main__':\r\n    process_directory()",
      "language": "python",
      "functions": [
        {
          "name": "extract_gzip_to_csv",
          "line": 20,
          "args": [
            "directory",
            "seven_zip"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "delete_lines_before_timestamp",
          "line": 39,
          "args": [
            "input_file",
            "output_file"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "clean_numeric_columns",
          "line": 53,
          "args": [
            "df",
            "cpu_columns"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "cleanse_data",
          "line": 59,
          "args": [
            "df"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "inspect_problematic_file",
          "line": 66,
          "args": [
            "file_path"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "process_server_data",
          "line": 75,
          "args": [
            "server_name",
            "directory",
            "threshold",
            "file_type"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "plot_server_graph",
          "line": 123,
          "args": [
            "ax",
            "server_name",
            "data",
            "cpu_columns"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "create_detailed_view",
          "line": 137,
          "args": [
            "server_name",
            "data",
            "cpu_columns",
            "root"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "create_main_window",
          "line": 215,
          "args": [
            "server_data",
            "threshold",
            "file_type"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "delete_unwanted_csv_files",
          "line": 273,
          "args": [
            "directory"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "process_directory",
          "line": 282,
          "args": [],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "on_pick",
          "line": 159,
          "args": [
            "event"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "on_click",
          "line": 204,
          "args": [
            "event"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "on_closing",
          "line": 333,
          "args": [],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "create_click_handler",
          "line": 251,
          "args": [
            "sn",
            "d",
            "cc"
          ],
          "docstring": null,
          "decorators": []
        }
      ],
      "classes": [],
      "imports": [
        "pandas",
        "matplotlib.pyplot",
        "os",
        "numpy",
        "glob",
        "subprocess",
        "csv",
        "datetime.datetime",
        "matplotlib.dates",
        "logging",
        "tkinter",
        "tkinter.ttk",
        "matplotlib.backends.backend_tkagg.FigureCanvasTkAgg",
        "matplotlib.backends.backend_tkagg.NavigationToolbar2Tk"
      ],
      "complexity": "high",
      "timestamp": "2025-10-20T09:14:45.974021"
    },
    "947b6a089925a763355ebf51a881ae7d": {
      "filename": "reporting_n.py",
      "code": "import pandas as pd\r\nimport matplotlib.pyplot as plt\r\nimport os\r\nimport numpy as np\r\nimport glob\r\nimport subprocess\r\nimport csv\r\nfrom datetime import datetime\r\nimport matplotlib.dates as mdates\r\nimport logging\r\nimport tkinter as tk\r\nfrom tkinter import ttk\r\nfrom matplotlib.backends.backend_tkagg import FigureCanvasTkAgg, NavigationToolbar2Tk\r\n\r\n# Set up logging\r\nlogging.basicConfig(filename='csv_parsing.log', level=logging.INFO,\r\n                    format='%(asctime)s - %(levelname)s - %(message)s')\r\n\r\n# Function to extract .gz files to CSV\r\ndef extract_gzip_to_csv(directory, seven_zip=\"C:\\\\Program Files\\\\7-Zip\\\\7z.exe\"):\r\n    for file in os.listdir(directory):\r\n        if file.endswith('.gz'):\r\n            gz_file_path = os.path.join(directory, file)\r\n            output_file = file[:-3]\r\n            output_path = os.path.join(directory, output_file)\r\n            # Construct 7Zip Command\r\n            command = [seven_zip, 'e', gz_file_path, f\"-o{directory}\", \"-y\"]\r\n            try:\r\n                # Use subprocess.run instead of Popen to wait for each extraction to complete\r\n                startupinfo = subprocess.STARTUPINFO()\r\n                startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\r\n                result = subprocess.run(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, startupinfo=startupinfo)\r\n                if result.returncode != 0:\r\n                    logging.error(f\"Error extracting {gz_file_path}\")\r\n            except subprocess.CalledProcessError as e:\r\n                logging.error(f\"Error extracting {gz_file_path}: {e}\")\r\n\r\n# Function to delete lines above 'Timestamp' header dynamically\r\ndef delete_lines_before_timestamp(input_file, output_file):\r\n    with open(input_file, 'r', newline='', encoding='utf-8') as infile, open(output_file, 'w', newline='', encoding='utf-8') as outfile:\r\n        reader = csv.reader(infile)\r\n        writer = csv.writer(outfile)\r\n\r\n        found_timestamp = False\r\n        for row in reader:\r\n            if row and row[0] == \"Timestamp\":\r\n                found_timestamp = True\r\n                writer.writerow(row)\r\n            elif found_timestamp:\r\n                writer.writerow(row)\r\n\r\n# Function to clean up mixed data types in columns\r\ndef clean_numeric_columns(df, cpu_columns):\r\n    for col in cpu_columns:\r\n        df[col] = pd.to_numeric(df[col], errors='coerce')  # Convert to numeric, coercing errors to NaN\r\n    return df\r\n\r\n# Function to format 'Timestamp' column and filter invalid rows\r\ndef cleanse_data(df):\r\n    # Convert 'Timestamp' column to datetime format\r\n    df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%d/%m/%Y %H:%M:%S', errors='coerce', dayfirst=True)\r\n    df = df.dropna(subset=['Timestamp'])  # Drop rows with invalid timestamps\r\n    return df\r\n\r\n# Function to inspect problematic files\r\ndef inspect_problematic_file(file_path):\r\n    try:\r\n        with open(file_path, 'r', encoding='utf-8') as f:\r\n            first_few_lines = [next(f) for _ in range(5)]\r\n        logging.info(f\"First few lines of {file_path}:\\n{''.join(first_few_lines)}\")\r\n    except Exception as e:\r\n        logging.error(f\"Error inspecting file {file_path}: {e}\")\r\n\r\n\r\ndef process_server_data(server_name, directory, threshold, file_type):\r\n    csv_files = glob.glob(os.path.join(directory, f\"*{server_name}*_edited.csv\"))\r\n    logging.info(f\"Processing {len(csv_files)} files for server {server_name}\")\r\n    dataframes = []\r\n\r\n    for csv_file in csv_files:\r\n        try:\r\n            df = pd.read_csv(csv_file, low_memory=False)\r\n            logging.info(f\"Successfully read file: {csv_file}\")\r\n\r\n            if df.empty:\r\n                logging.warning(f\"Empty DataFrame for file: {csv_file}\")\r\n                continue\r\n\r\n            # Determine the refresh rate column name based on the filename\r\n            file_prefix = 'ADS' if 'ads' in csv_file.lower() else 'ADH'\r\n            refresh_rate_col = f\"{file_prefix}_Total_Input_Refresh_Rate_1s_peak\"\r\n\r\n            cpu_columns = [col for col in df.columns if col.startswith(\"Processor\") and col.endswith(\"_Percent_Usage_1s_peak\")]\r\n            if not cpu_columns:\r\n                logging.warning(f\"No CPU usage columns found in {csv_file}\")\r\n                continue\r\n\r\n            df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%d/%m/%Y %H:%M:%S', errors='coerce')\r\n            df = df.dropna(subset=['Timestamp'])\r\n\r\n            df[cpu_columns] = df[cpu_columns].apply(pd.to_numeric, errors='coerce')\r\n            \r\n            # Convert refresh rate column to numeric\r\n            if refresh_rate_col in df.columns:\r\n                df[refresh_rate_col] = pd.to_numeric(df[refresh_rate_col], errors='coerce')\r\n            else:\r\n                logging.warning(f\"Refresh rate column {refresh_rate_col} not found in {csv_file}\")\r\n                df[refresh_rate_col] = np.nan\r\n\r\n            df['Filename'] = os.path.basename(csv_file)\r\n            df['Avg_CPU_Usage'] = df[cpu_columns].mean(axis=1)\r\n\r\n            df_filtered = df[df['Avg_CPU_Usage'] > threshold]\r\n\r\n            if df_filtered.empty:\r\n                logging.warning(f\"No data above {threshold}% CPU usage in {csv_file}\")\r\n                continue\r\n\r\n            columns_to_keep = ['Timestamp', 'Avg_CPU_Usage', 'Filename', refresh_rate_col] + cpu_columns\r\n            df_filtered = df_filtered[columns_to_keep]\r\n\r\n            dataframes.append(df_filtered)\r\n\r\n        except Exception as e:\r\n            logging.error(f\"Error processing file {csv_file}: {e}\")\r\n            logging.exception(\"Exception details:\")\r\n\r\n    if dataframes:\r\n        combined_df = pd.concat(dataframes).sort_values('Timestamp').reset_index(drop=True)\r\n        return combined_df, cpu_columns\r\n    else:\r\n        return None, []\r\n\r\ndef plot_server_graph(ax, server_name, data, cpu_columns):\r\n    for cpu in cpu_columns:\r\n        line = ax.plot(data['Timestamp'], data[cpu], label=cpu, linewidth=0.5)[0]\r\n        line.set_pickradius(5)\r\n        \r\n        # Get the refresh rate column name\r\n        refresh_rate_col = next((col for col in data.columns if col.endswith('_Total_Input_Refresh_Rate_1s_peak')), None)\r\n        \r\n        # Store data as a custom attribute\r\n        line_data = data[['Timestamp', cpu, 'Filename']].copy()\r\n        if refresh_rate_col:\r\n            line_data[refresh_rate_col] = data[refresh_rate_col]\r\n        line_data = line_data.to_dict('records')\r\n        \r\n        line.set_picker(5)\r\n        line.data_annotations = line_data\r\n\r\n    ax.set_title(f\"{server_name}\", fontsize=8)\r\n    ax.tick_params(axis='both', which='major', labelsize=6)\r\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\r\n\r\ndef create_detailed_view(server_name, data, cpu_columns, root):\r\n    detail_window = tk.Toplevel(root)\r\n    detail_window.title(f\"Detailed View: {server_name}\")\r\n    detail_window.geometry(\"1000x600\")\r\n\r\n    fig, ax = plt.subplots(figsize=(10, 6))\r\n    plot_server_graph(ax, server_name, data, cpu_columns)\r\n    ax.set_xlabel('Timestamp')\r\n    ax.set_ylabel('CPU Usage (%)')\r\n    ax.legend(loc='upper left', bbox_to_anchor=(1, 1), title=\"Processors\")\r\n\r\n    canvas = FigureCanvasTkAgg(fig, master=detail_window)\r\n    canvas_widget = canvas.get_tk_widget()\r\n    canvas_widget.pack(fill=tk.BOTH, expand=True)\r\n\r\n    toolbar = NavigationToolbar2Tk(canvas, detail_window)\r\n    toolbar.update()\r\n    canvas_widget.pack(fill=tk.BOTH, expand=True)\r\n\r\n    current_annotation = None\r\n\r\n    def on_pick(event):\r\n        nonlocal current_annotation\r\n        logging.info(\"Pick event triggered\")\r\n\r\n        if event.mouseevent.button != 1:\r\n            return\r\n\r\n        line = event.artist\r\n        if hasattr(line, 'data_annotations'):\r\n            data = line.data_annotations\r\n            ind = event.ind[0]\r\n            logging.info(f\"Picked point index: {ind}\")\r\n\r\n            if ind < len(data):\r\n                if current_annotation:\r\n                    current_annotation.remove()\r\n\r\n                point = data[ind]\r\n                timestamp = point['Timestamp']\r\n                cpu_usage = point[line.get_label()]\r\n                filename = point['Filename']\r\n                \r\n                # Get refresh rate value if available\r\n                refresh_rate_col = next((key for key in point.keys() if key.endswith('_Total_Input_Refresh_Rate_1s_peak')), None)\r\n                refresh_rate = point.get(refresh_rate_col, 'N/A') if refresh_rate_col else 'N/A'\r\n                \r\n                # Create annotation text with refresh rate\r\n                annotation_text = (\r\n                    f\"File: {filename}\\n\"\r\n                    f\"Time: {timestamp}\\n\"\r\n                    f\"CPU Usage: {cpu_usage:.2f}%\\n\"\r\n                    f\"Refresh Rate: {refresh_rate}\"\r\n                )\r\n\r\n                current_annotation = ax.annotate(\r\n                    annotation_text,\r\n                    xy=(mdates.date2num(pd.Timestamp(timestamp)), cpu_usage),\r\n                    xytext=(15, 15),\r\n                    textcoords='offset points',\r\n                    bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\r\n                    arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0')\r\n                )\r\n               \r\n                logging.info(f\"Annotation created for point: {annotation_text}\")\r\n                fig.canvas.draw_idle()\r\n\r\n    fig.canvas.mpl_connect('pick_event', on_pick)\r\n\r\n    def on_click(event):\r\n        nonlocal current_annotation\r\n        if event.inaxes is None or (event.xdata is None or event.ydata is None):\r\n            if current_annotation:\r\n                current_annotation.remove()\r\n                current_annotation = None\r\n                fig.canvas.draw_idle()\r\n\r\n    fig.canvas.mpl_connect('button_press_event', on_click)\r\n    \r\ndef create_main_window(server_data, threshold, file_type):\r\n    root = tk.Tk()\r\n    root.title(\"CPU Usage Visualization\")\r\n    root.geometry(\"1200x800\")\r\n\r\n    # Create a frame for the graphs\r\n    main_frame = ttk.Frame(root)\r\n    main_frame.pack(fill=tk.BOTH, expand=True)\r\n\r\n    # Create a canvas with both vertical and horizontal scrollbars\r\n    canvas = tk.Canvas(main_frame)\r\n    v_scrollbar = ttk.Scrollbar(main_frame, orient=\"vertical\", command=canvas.yview)\r\n    h_scrollbar = ttk.Scrollbar(main_frame, orient=\"horizontal\", command=canvas.xview)\r\n    scrollable_frame = ttk.Frame(canvas)\r\n\r\n    scrollable_frame.bind(\r\n        \"<Configure>\",\r\n        lambda e: canvas.configure(\r\n            scrollregion=canvas.bbox(\"all\")\r\n        )\r\n    )\r\n\r\n    canvas.create_window((0, 0), window=scrollable_frame, anchor=\"nw\")\r\n    canvas.configure(yscrollcommand=v_scrollbar.set, xscrollcommand=h_scrollbar.set)\r\n\r\n    # Calculate the number of columns and rows\r\n    num_columns = 3\r\n    num_rows = len(server_data) // num_columns + (1 if len(server_data) % num_columns else 0)\r\n    \r\n    fig, axes = plt.subplots(nrows=num_rows, ncols=num_columns, figsize=(15, 5 * num_rows))\r\n    axes = axes.flatten()\r\n\r\n    for i, (server_name, (data, cpu_columns)) in enumerate(server_data.items()):\r\n        if data is not None and not data.empty:\r\n            plot_server_graph(axes[i], server_name, data, cpu_columns)\r\n\r\n            def create_click_handler(sn, d, cc):\r\n                return lambda event: create_detailed_view(sn, d, cc, root)\r\n\r\n            axes[i].figure.canvas.mpl_connect('button_press_event', create_click_handler(server_name, data, cpu_columns))\r\n\r\n    # Remove any unused subplots\r\n    for j in range(i + 1, len(axes)):\r\n        fig.delaxes(axes[j])\r\n\r\n    plt.tight_layout()\r\n\r\n    canvas_widget = FigureCanvasTkAgg(fig, scrollable_frame)\r\n    canvas_widget.draw()\r\n    canvas_widget.get_tk_widget().pack(fill=tk.BOTH, expand=True)\r\n\r\n    canvas.pack(side=\"left\", fill=\"both\", expand=True)\r\n    v_scrollbar.pack(side=\"right\", fill=\"y\")\r\n    h_scrollbar.pack(side=\"bottom\", fill=\"x\")\r\n\r\n    return root\r\n    \r\n# Function to delete unnecessary files\r\ndef delete_unwanted_csv_files(directory):\r\n    csv_files = glob.glob(os.path.join(directory, \"*.csv\"))\r\n    for file in csv_files:\r\n        if \"_edited.csv\" not in os.path.basename(file):\r\n            try:\r\n                os.remove(file)\r\n            except Exception as e:\r\n                logging.error(f\"Error deleting {file}: {e}\")\r\n\r\ndef process_directory():\r\n    file_type = input(\"Enter 'ADS' or 'ADH': \").strip().upper()\r\n    if file_type not in ['ADS', 'ADH']:\r\n        print(\"Invalid input. Please enter 'ADS' or 'ADH'.\")\r\n        return\r\n\r\n    region = input(\"Enter 'APAC' or 'BON': \").strip().upper()\r\n    if region not in ['APAC', 'BON']:\r\n        print(\"Invalid input. Please enter 'APAC' or 'BON'.\")\r\n        return\r\n\r\n    base_directory = os.getcwd()\r\n    directory = os.path.join(base_directory, f\"{region}_{file_type}\")\r\n\r\n    if not os.path.exists(directory):\r\n        print(f\"Directory {directory} does not exist.\")\r\n        return\r\n\r\n    # Extract gzip files first\r\n    extract_gzip_to_csv(directory)\r\n\r\n    # Process each CSV in the directory\r\n    server_names = set()\r\n    csv_files = glob.glob(os.path.join(directory, \"*.csv\"))\r\n\r\n    # Dynamically clean CSV files by deleting lines before 'Timestamp'\r\n    for csv_file in csv_files:\r\n        output_file = csv_file.replace(\".csv\", \"_edited.csv\")\r\n        delete_lines_before_timestamp(csv_file, output_file)\r\n        try:\r\n            server_name = os.path.basename(csv_file).split('_')[1] # Extract server name\r\n            server_names.add(server_name)\r\n        except IndexError:\r\n            logging.error(f\"Error extracting server name from file: {csv_file}\")\r\n\r\n    if not server_names:\r\n        print(\"No valid server names extracted. Exiting.\")\r\n        return\r\n\r\n    threshold = 10 # CPU usage threshold\r\n    server_data = {}\r\n\r\n    for server_name in server_names:\r\n        data, cpu_columns = process_server_data(server_name, directory, threshold, file_type)\r\n        if data is not None and not data.empty:\r\n            server_data[server_name] = (data, cpu_columns)\r\n\r\n    # Create and run the main window\r\n    root = create_main_window(server_data, threshold, file_type)\r\n    \r\n    # Set up a proper exit mechanism\r\n    def on_closing():\r\n        root.quit()\r\n        root.destroy()\r\n\r\n    root.protocol(\"WM_DELETE_WINDOW\", on_closing)\r\n    root.mainloop()\r\n\r\n    # Delete all unwanted CSV files after processing\r\n    delete_unwanted_csv_files(directory)\r\n\r\nif __name__ == '__main__':\r\n    process_directory()",
      "language": "python",
      "functions": [
        {
          "name": "extract_gzip_to_csv",
          "line": 20,
          "args": [
            "directory",
            "seven_zip"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "delete_lines_before_timestamp",
          "line": 39,
          "args": [
            "input_file",
            "output_file"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "clean_numeric_columns",
          "line": 53,
          "args": [
            "df",
            "cpu_columns"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "cleanse_data",
          "line": 59,
          "args": [
            "df"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "inspect_problematic_file",
          "line": 66,
          "args": [
            "file_path"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "process_server_data",
          "line": 75,
          "args": [
            "server_name",
            "directory",
            "threshold",
            "file_type"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "plot_server_graph",
          "line": 134,
          "args": [
            "ax",
            "server_name",
            "data",
            "cpu_columns"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "create_detailed_view",
          "line": 155,
          "args": [
            "server_name",
            "data",
            "cpu_columns",
            "root"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "create_main_window",
          "line": 234,
          "args": [
            "server_data",
            "threshold",
            "file_type"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "delete_unwanted_csv_files",
          "line": 292,
          "args": [
            "directory"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "process_directory",
          "line": 301,
          "args": [],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "on_pick",
          "line": 176,
          "args": [
            "event"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "on_click",
          "line": 224,
          "args": [
            "event"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "on_closing",
          "line": 352,
          "args": [],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "create_click_handler",
          "line": 270,
          "args": [
            "sn",
            "d",
            "cc"
          ],
          "docstring": null,
          "decorators": []
        }
      ],
      "classes": [],
      "imports": [
        "pandas",
        "matplotlib.pyplot",
        "os",
        "numpy",
        "glob",
        "subprocess",
        "csv",
        "datetime.datetime",
        "matplotlib.dates",
        "logging",
        "tkinter",
        "tkinter.ttk",
        "matplotlib.backends.backend_tkagg.FigureCanvasTkAgg",
        "matplotlib.backends.backend_tkagg.NavigationToolbar2Tk"
      ],
      "complexity": "high",
      "timestamp": "2025-10-20T09:31:50.091948"
    },
    "5d2324e347065282240cc919fe367a3b": {
      "filename": "test_app.py",
      "code": "# test_app.py\nimport faiss\nimport json\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\n\n# -------------------\n# CONFIG\n# -------------------\nBUGS_FILE = \"data/dummy_bugs.jsonl\"   # your bug dataset\nMODEL_NAME = \"all-MiniLM-L6-v2\"       # same model used during indexing\nFAISS_DIM = 384                       # embedding dimension\nTOP_K = 10                            # number of results to retrieve\n\n# -------------------\n# Load dataset\n# -------------------\nbugs = []\nwith open(BUGS_FILE, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        bugs.append(json.loads(line))\n\n# -------------------\n# Load model & FAISS index\n# -------------------\nprint(\"\ud83d\udce5 Loading model & building FAISS index...\")\nmodel = SentenceTransformer(MODEL_NAME)\n\nbug_texts = [f\"{b['summary']} - {b['description']} ({b['platform']})\" for b in bugs]\nembeddings = model.encode(bug_texts, convert_to_numpy=True)\n\nindex = faiss.IndexFlatL2(FAISS_DIM)\nindex.add(embeddings)\nprint(f\"\u2705 FAISS index built with {len(bugs)} bug reports\")\n\n# -------------------\n# Search function\n# -------------------\ndef search_bugs(query, k=TOP_K, keyword_match=True):\n    query_vec = model.encode([query], convert_to_numpy=True)\n    D, I = index.search(np.array(query_vec), k)\n\n    results = [(bugs[idx], D[0][rank]) for rank, idx in enumerate(I[0])]\n\n    if keyword_match:\n        keyword_hits = [\n            (bug, 0.0) for bug in bugs\n            if query.lower() in bug[\"summary\"].lower() or query.lower() in bug[\"description\"].lower()\n        ]\n        results_dict = {r[0][\"summary\"]: r for r in results}\n        for bug, score in keyword_hits:\n            results_dict[bug[\"summary\"]] = (bug, score)\n        results = list(results_dict.values())\n\n    return results\n\n# -------------------\n# Severity & Priority heuristics\n# -------------------\ndef suggest_severity_priority(bug):\n    text = (bug[\"summary\"] + \" \" + bug[\"description\"]).lower()\n\n    if any(word in text for word in [\"crash\", \"data loss\", \"corruption\", \"unresponsive\", \"failover\"]):\n        return \"Severity: Critical\", \"Priority: High\"\n\n    if any(word in text for word in [\"latency\", \"delay\", \"slow\", \"high cpu\", \"memory leak\", \"timeout\"]):\n        return \"Severity: Major\", \"Priority: Medium\"\n\n    if any(word in text for word in [\"ui\", \"format\", \"glitch\", \"display\", \"alignment\"]):\n        return \"Severity: Minor\", \"Priority: Low\"\n\n    return \"Severity: Major\", \"Priority: Medium\"  # default\n\n# -------------------\n# Interactive loop\n# -------------------\nprint(\"\\n\ud83d\udca1 Type a query (or 'exit' to quit):\")\nwhile True:\n    query = input(\"Query: \")\n    if query.lower() in [\"exit\", \"quit\"]:\n        break\n\n    results = search_bugs(query, k=TOP_K, keyword_match=True)\n\n    if not results:\n        print(\"\u26a0\ufe0f No matches found.\")\n        continue\n\n    print(\"\\n\ud83d\udd0e Matches:\")\n    for i, (bug, score) in enumerate(results, 1):\n        print(f\"{i}. {bug['summary']} ({bug['platform']})\")\n        print(f\"   {bug['description'][:70]}...\")\n        print(f\"   Score: {score:.4f}\\n\")\n\n    try:\n        choice = int(input(\"Select issue number to view full defect: \")) - 1\n        if choice < 0 or choice >= len(results):\n            print(\"\u26a0\ufe0f Invalid choice.\")\n            continue\n    except ValueError:\n        print(\"\u26a0\ufe0f Please enter a valid number.\")\n        continue\n\n    selected = results[choice][0]\n\n    print(\"\\n\ud83d\udccc Full Defect Description:\")\n    print(selected[\"description\"])\n\n    severity, priority = suggest_severity_priority(selected)\n    print(severity)\n    print(priority)\n    print(\"-\" * 80)\n",
      "language": "python",
      "functions": [
        {
          "name": "search_bugs",
          "line": 39,
          "args": [
            "query",
            "k",
            "keyword_match"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "suggest_severity_priority",
          "line": 60,
          "args": [
            "bug"
          ],
          "docstring": null,
          "decorators": []
        }
      ],
      "classes": [],
      "imports": [
        "faiss",
        "json",
        "numpy",
        "sentence_transformers.SentenceTransformer"
      ],
      "complexity": "low",
      "timestamp": "2025-10-20T09:40:49.548118"
    },
    "7a43bdd85ee17662f7fdb0dcbbe80e2b": {
      "filename": "build_index.py",
      "code": "import os\nimport json\nimport argparse\nimport numpy as np\nimport faiss\nfrom sentence_transformers import SentenceTransformer\n\n\n\n\ndef iter_jsonl(path):\n\twith open(path, \"r\", encoding=\"utf-8\") as f:\n\t\tfor line in f:\n\t\t\tif line.strip():\n\t\t\t\tyield json.loads(line)\n\n\n\n\nif __name__ == \"__main__\":\n\tap = argparse.ArgumentParser()\n\tap.add_argument(\"--input\", required=True)\n\tap.add_argument(\"--outdir\", required=True)\n\tap.add_argument(\"--model\", default=\"sentence-transformers/all-MiniLM-L6-v2\")\n\targs = ap.parse_args()\n\n\tos.makedirs(args.outdir, exist_ok=True)\n\n\tmodel = SentenceTransformer(args.model)\n\ttexts, metas = [], []\n\tfor rec in iter_jsonl(args.input):\n\t\ttext = f\"{rec.get('summary','')}\\n\\n{rec.get('description','')}\"\n\t\ttexts.append(text)\n\t\tmetas.append({\"key\": rec.get(\"key\"), \"summary\": rec.get(\"summary\"), \"description\": rec.get(\"description\")})\n\n\tembs = model.encode(texts, convert_to_numpy=True, show_progress_bar=True, batch_size=64, normalize_embeddings=True)\n\tdim = embs.shape[1]\n\tindex = faiss.IndexFlatIP(dim)\n\tindex.add(embs)\n\n\tfaiss.write_index(index, os.path.join(args.outdir, \"faiss_index.bin\"))\n\tnp.save(os.path.join(args.outdir, \"embeddings.npy\"), embs)\n\twith open(os.path.join(args.outdir, \"meta.json\"), \"w\", encoding=\"utf-8\") as f:\n\t\tjson.dump(metas, f, ensure_ascii=False)\n\n\tprint(f\"Indexed {len(texts)} documents.\")",
      "language": "python",
      "functions": [
        {
          "name": "iter_jsonl",
          "line": 11,
          "args": [
            "path"
          ],
          "docstring": null,
          "decorators": []
        }
      ],
      "classes": [],
      "imports": [
        "os",
        "json",
        "argparse",
        "numpy",
        "faiss",
        "sentence_transformers.SentenceTransformer"
      ],
      "complexity": "low",
      "timestamp": "2025-10-20T09:40:49.548118"
    },
    "1d0d5d5e5649d8da5a57fb29cd3eb884": {
      "filename": "build_index_dummy.py",
      "code": "from sentence_transformers import SentenceTransformer\nimport faiss\nimport json\nimport numpy as np\nimport os\n\nDATA_PATH = \"data/dummy_bugs.jsonl\"\nINDEX_PATH = \"index/faiss_index.bin\"\nEMB_PATH = \"index/embeddings.npy\"\n\ndef main():\n    # Load model (small + CPU friendly)\n    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n    # Load dataset\n    bugs = []\n    with open(DATA_PATH, \"r\") as f:\n        for line in f:\n            bugs.append(json.loads(line))\n\n    # Build FAISS index\n    bug_texts = [f\"{b['summary']} - {b['description']} ({b['platform']})\" for b in bugs]\n    embeddings = model.encode(bug_texts, convert_to_numpy=True)\n\n    dimension = embeddings.shape[1]\n    index = faiss.IndexFlatL2(dimension)\n    index.add(embeddings)\n\n    # Save index + embeddings\n    os.makedirs(\"index\", exist_ok=True)\n    faiss.write_index(index, INDEX_PATH)\n    np.save(EMB_PATH, embeddings)\n\n    print(f\"FAISS index built with {len(bug_texts)} bug reports\")\n\nif __name__ == \"__main__\":\n    main()",
      "language": "python",
      "functions": [
        {
          "name": "main",
          "line": 11,
          "args": [],
          "docstring": null,
          "decorators": []
        }
      ],
      "classes": [],
      "imports": [
        "sentence_transformers.SentenceTransformer",
        "faiss",
        "json",
        "numpy",
        "os"
      ],
      "complexity": "low",
      "timestamp": "2025-10-20T09:40:49.548118"
    },
    "90c61a0bcfe6a4de787aec99d068ee8a": {
      "filename": "fine_tune_lora_t5.py",
      "code": "import json\nfrom datasets import load_dataset\nfrom transformers import T5ForConditionalGeneration, T5TokenizerFast, DataCollatorForSeq2Seq, Trainer, TrainingArguments\nfrom peft import LoraConfig, get_peft_model, TaskType\n\n\n\"\"\"\nLightweight contextual training with LoRA on T5-base.\nInput: vague summary; Target: structured description.\nThis is compute-friendly and good for a POC.\n\"\"\"\n\n\n\n\ndef jsonl_to_hf(path):\n\tdef gen():\n\t\twith open(path, \"r\", encoding=\"utf-8\") as f:\n\t\t\tfor line in f:\n\t\t\t\tif line.strip():\n\t\t\t\t\tj = json.loads(line)\n\t\t\t\t\tyield {\"input\": j[\"input\"], \"target\": j[\"target\"]}\n\treturn load_dataset(\"json\", data_files={\"train\": path}, split=\"train\").map(lambda _: _)\n\n\n\n\nif __name__ == \"__main__\":\n\timport argparse\n\tap = argparse.ArgumentParser()\n\tap.add_argument(\"--train\", required=True)\n\tap.add_argument(\"--eval\", required=True)\n\tap.add_argument(\"--out\", required=True)\n\tap.add_argument(\"--base\", default=\"google/flan-t5-base\")\n\targs = ap.parse_args()\n\n\ttokenizer = T5TokenizerFast.from_pretrained(args.base)\n\tmodel = T5ForConditionalGeneration.from_pretrained(args.base)\n\n\tlora_config = LoraConfig(\n\t\ttask_type=TaskType.SEQ_2_SEQ_LM,\n\t\tr=8, lora_alpha=16, lora_dropout=0.1,\n\t\ttarget_modules=[\"q\", \"v\"]\n\t)\n\tmodel = get_peft_model(model, lora_config)\n\n\tdef preprocess(ex):\n\t\tx = tokenizer(\n\t\t\t[f\"Enhance defect: {e}\" for e in ex[\"input\"]],\n\t\t\tmax_length=256, truncation=True\n\t\t)\n\t\ty = tokenizer(text_target=ex[\"target\"], max_length=512, truncation=True)\n\t\tx[\"labels\"] = y[\"input_ids\"]\n\t\treturn x\n\n\ttrain_ds = load_dataset(\"json\", data_files={\"train\": args.train})[\"train\"].map(preprocess, batched=True, remove_columns=[\"input\", \"target\"])\n\teval_ds = load_dataset(\"json\", data_files={\"eval\": args.eval})[\"eval\"].map(preprocess, batched=True, remove_columns=[\"input\", \"target\"])\n\n\tcollator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\n\ttargs = TrainingArguments(\n\t\toutput_dir=args.out,\n\t\tper_device_train_batch_size=4,\n\t\tper_device_eval_batch_size=4,\n\t\tgradient_accumulation_steps=4,\n\t\tlearning_rate=2e-4,\n\t\tnum_train_epochs=3,\n\t\tevaluation_strategy=\"epoch\",\n\t\tsave_strategy=\"epoch\",\n\t\tlogging_steps=50,\n\t\tfp16=True\n\t)\n\n\ttrainer = Trainer(model=model, args=targs, train_dataset=train_ds, eval_dataset=eval_ds, data_collator=collator)\n\ttrainer = Trainer(\n\t\tmodel=model,\n\t\targs=targs,\n\t\ttrain_dataset=train_ds,\n\t\teval_dataset=eval_ds,\n\t\tdata_collator=collator,\n\t\tpredict_with_generate=True\n\t)\n\ttrainer.train()\n\ttrainer.save_model(args.out)\n\ttokenizer.save_pretrained(args.out)\n\tprint(\"Saved LoRA model to\", args.out)",
      "language": "python",
      "functions": [
        {
          "name": "jsonl_to_hf",
          "line": 16,
          "args": [
            "path"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "gen",
          "line": 17,
          "args": [],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "preprocess",
          "line": 47,
          "args": [
            "ex"
          ],
          "docstring": null,
          "decorators": []
        }
      ],
      "classes": [],
      "imports": [
        "json",
        "datasets.load_dataset",
        "transformers.T5ForConditionalGeneration",
        "transformers.T5TokenizerFast",
        "transformers.DataCollatorForSeq2Seq",
        "transformers.Trainer",
        "transformers.TrainingArguments",
        "peft.LoraConfig",
        "peft.get_peft_model",
        "peft.TaskType",
        "argparse"
      ],
      "complexity": "low",
      "timestamp": "2025-10-20T09:40:49.549723"
    },
    "a2c05dc58db11a83734cb15e0499cee2": {
      "filename": "ingest_jira.py",
      "code": "import os\nimport json\nimport argparse\nimport requests\nfrom dotenv import load_dotenv\n\n\nload_dotenv()\n\n\nBASE = os.getenv(\"JIRA_BASE_URL\")\nEMAIL = os.getenv(\"JIRA_EMAIL\")\nTOKEN = os.getenv(\"JIRA_API_TOKEN\")\n\n\nHEADERS = {\"Accept\": \"application/json\"}\nAUTH = (EMAIL, TOKEN)\n\n\n\n\ndef fetch_issues(jql: str, limit: int = 500):\n\tstart = 0\n\tall_issues = []\n\twhile True:\n\t\tparams = {\n\t\t\t\"jql\": jql,\n\t\t\t\"startAt\": start,\n\t\t\t\"maxResults\": min(100, limit - start),\n\t\t\t\"fields\": [\n\t\t\t\t\"summary\",\n\t\t\t\t\"description\",\n\t\t\t\t\"issuetype\",\n\t\t\t\t\"created\",\n\t\t\t\t\"updated\",\n\t\t\t\t\"labels\",\n\t\t\t\t\"components\",\n\t\t\t\t\"priority\",\n\t\t\t\t\"environment\"\n\t\t\t]\n\t\t}\n\t\tr = requests.get(f\"{BASE}/rest/api/3/search\", headers=HEADERS, params=params, auth=AUTH)\n\t\tr.raise_for_status()\n\t\tdata = r.json()\n\t\tissues = data.get(\"issues\", [])\n\t\tfor it in issues:\n\t\t\tfields = it.get(\"fields\", {})\n\t\t\trec = {\n\t\t\t\t\"key\": it.get(\"key\"),\n\t\t\t\t\"summary\": fields.get(\"summary\") or \"\",\n\t\t\t\t\"description\": (fields.get(\"description\") or \"\"),\n\t\t\t\t\"labels\": fields.get(\"labels\") or [],\n\t\t\t\t\"components\": [c.get(\"name\") for c in (fields.get(\"components\") or [])],\n\t\t\t\t\"priority\": (fields.get(\"priority\") or {}).get(\"name\"),\n\t\t\t\t\"environment\": fields.get(\"environment\") or \"\",\n\t\t\t\t\"issuetype\": (fields.get(\"issuetype\") or {}).get(\"name\"),\n\t\t\t\t\"created\": fields.get(\"created\"),\n\t\t\t\t\"updated\": fields.get(\"updated\"),\n\t\t\t}\n\t\t\tall_issues.append(rec)\n\t\tstart += len(issues)\n\t\tif start >= limit or len(issues) == 0:\n\t\t\tbreak\n\treturn all_issues\n\n\n\n\nif __name__ == \"__main__\":\n\tap = argparse.ArgumentParser()\n\tap.add_argument(\"--jql\", required=True)\n\tap.add_argument(\"--limit\", type=int, default=500)\n\tap.add_argument(\"--out\", default=\"data/raw_jira_export.jsonl\")\n\targs = ap.parse_args()\n\n\tissues = fetch_issues(args.jql, args.limit)\n\twith open(args.out, \"w\", encoding=\"utf-8\") as f:\n\t\tfor issue in issues:\n\t\t\tf.write(json.dumps(issue, ensure_ascii=False) + \"\\n\")\n\n\tprint(f\"Saved {len(issues)} issues to {args.out}\")",
      "language": "python",
      "functions": [
        {
          "name": "fetch_issues",
          "line": 22,
          "args": [
            "jql",
            "limit"
          ],
          "docstring": null,
          "decorators": []
        }
      ],
      "classes": [],
      "imports": [
        "os",
        "json",
        "argparse",
        "requests",
        "dotenv.load_dotenv"
      ],
      "complexity": "low",
      "timestamp": "2025-10-20T09:40:49.549723"
    },
    "a4bed6001970c033925473d02f35504b": {
      "filename": "make_training_pairs.py",
      "code": "import json\nimport argparse\n\n\n\"\"\"\nBuild supervised pairs from historical tickets. Heuristic:\n- input = short/vague summary\n- target = cleaned structured description synthesized from description + priority + environment\nFor better results, curate manually or use a templater.\n\"\"\"\n\n\ndef iter_jsonl(path):\n\twith open(path, \"r\", encoding=\"utf-8\") as f:\n\t\tfor line in f:\n\t\t\tif line.strip():\n\t\t\t\tyield json.loads(line)\n\n\n\n\ndef to_structured(rec):\n\tdesc = rec.get(\"description\") or \"\"\n\tenv = rec.get(\"environment\") or \"\"\n\tpriority = rec.get(\"priority\") or \"Unspecified\"\n\tcomponents = \", \".join(rec.get(\"components\") or [])\n\n\ttemplate = f\"\"\"\n**Summary**: {rec.get('summary','')}\n\n\n**Steps to Reproduce**:\n1. [From historical context; may be incomplete]\n\n\n**Expected Result**:\n- [Fill based on module: {components or 'N/A'}]\n\n\n**Actual Result**:\n- {desc[:800]}\n\n\n**Environment**:\n- {env or 'Unknown'}\n\n\n**Impact/Severity**:\n- {priority}\n\"\"\"\n\treturn template.strip()\n\n\n\n\nif __name__ == \"__main__\":\n\tap = argparse.ArgumentParser()\n\tap.add_argument(\"--input\", required=True)\n\tap.add_argument(\"--output\", required=True)\n\tap.add_argument(\"--eval\", required=True)\n\targs = ap.parse_args()\n\n\ttrain, eval_ = [], []\n\tfor i, rec in enumerate(iter_jsonl(args.input)):\n\t\tpair = {\n\t\t\t\"input\": rec.get(\"summary\") or rec.get(\"key\"),\n\t\t\t\"target\": to_structured(rec)\n\t\t}\n\t\t(eval_ if i % 10 == 0 else train).append(pair)\n\n\twith open(args.output, \"w\", encoding=\"utf-8\") as f:\n\t\tfor p in train:\n\t\t\tf.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n\twith open(args.eval, \"w\", encoding=\"utf-8\") as f:\n\t\tfor p in eval_:\n\t\t\tf.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n\tprint(f\"Train: {len(train)}, Eval: {len(eval_)}\")",
      "language": "python",
      "functions": [
        {
          "name": "iter_jsonl",
          "line": 13,
          "args": [
            "path"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "to_structured",
          "line": 22,
          "args": [
            "rec"
          ],
          "docstring": null,
          "decorators": []
        }
      ],
      "classes": [],
      "imports": [
        "json",
        "argparse"
      ],
      "complexity": "low",
      "timestamp": "2025-10-20T09:40:49.549723"
    },
    "f16230720fc80a1447662ee72b457b93": {
      "filename": "test_lora_generation.py",
      "code": "import os\nfrom transformers import T5ForConditionalGeneration, T5TokenizerFast\n\n\nmodel_path = os.getenv(\"HF_MODEL_PATH\", \"models/t5-lora-domain\")\n\n\nmodel = T5ForConditionalGeneration.from_pretrained(model_path)\ntokenizer = T5TokenizerFast.from_pretrained(model_path)\n\n\nprompt = \"Enhance defect: payment page error after clicking pay\"\nids = tokenizer(prompt, return_tensors=\"pt\").input_ids\nout = model.generate(ids, max_new_tokens=300)\nprint(tokenizer.decode(out[0], skip_special_tokens=True))",
      "language": "python",
      "functions": [],
      "classes": [],
      "imports": [
        "os",
        "transformers.T5ForConditionalGeneration",
        "transformers.T5TokenizerFast"
      ],
      "complexity": "low",
      "timestamp": "2025-10-20T09:40:49.549723"
    },
    "461c0dd3749392dd7179d9ab33fa8d01": {
      "filename": "test_search.py",
      "code": "# scripts/test_search.py\nimport faiss\nimport json\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\n\nDATA_PATH = \"data/dummy_bugs.jsonl\"   # or your LSE dataset\nINDEX_PATH = \"index/faiss_index.bin\"  # built by build_index_dummy.py\nEMB_PATH = \"index/embeddings.npy\"\n\n# ---------------- Load dataset ----------------\nwith open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n    bugs = [json.loads(line) for line in f if line.strip()]\n\n# Load FAISS index + embeddings\nindex = faiss.read_index(INDEX_PATH)\nembeddings = np.load(EMB_PATH)\n\n# Model for embeddings\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n\n# ---------------- Search ----------------\ndef search_bugs(query, k=10, keyword_match=True):\n    \"\"\"Search FAISS, optionally augment with keyword matches.\"\"\"\n    query_vec = model.encode([query], convert_to_numpy=True)\n    D, I = index.search(np.array(query_vec), k)\n\n    results = [(bugs[idx], D[0][rank]) for rank, idx in enumerate(I[0])]\n\n    if keyword_match:\n        keyword_hits = [\n            (bug, 0.0) for bug in bugs\n            if query.lower() in bug[\"summary\"].lower() or query.lower() in bug[\"description\"].lower()\n        ]\n        # Merge FAISS + keyword results (avoid duplicates)\n        results_dict = {r[0][\"summary\"]: r for r in results}\n        for bug, score in keyword_hits:\n            results_dict[bug[\"summary\"]] = (bug, score)\n        results = list(results_dict.values())\n\n    return results\n\n\n# ---------------- Severity & Priority Heuristics ----------------\ndef suggest_severity_priority(bug):\n    \"\"\"Heuristic rules to guess severity and priority.\"\"\"\n    text = (bug[\"summary\"] + \" \" + bug[\"description\"]).lower()\n\n    if any(word in text for word in [\"crash\", \"data loss\", \"not triggered\", \"unresponsive\", \"corruption\", \"failover\"]):\n        return \"Severity: Critical\", \"Priority: High\"\n\n    if any(word in text for word in [\"latency\", \"delay\", \"slow\", \"high cpu\", \"memory leak\", \"timeout\"]):\n        return \"Severity: Major\", \"Priority: Medium\"\n\n    if any(word in text for word in [\"ui\", \"format\", \"glitch\", \"display\", \"alignment\"]):\n        return \"Severity: Minor\", \"Priority: Low\"\n\n    return \"Severity: Major\", \"Priority: Medium\"  # default fallback\n\n\n# ---------------- Main Loop ----------------\ndef main():\n    print(\"\ud83d\udca1 Type a query (or 'exit' to quit):\")\n    while True:\n        query = input(\"Query: \")\n        if query.lower() in [\"exit\", \"quit\"]:\n            break\n\n        results = search_bugs(query, k=10, keyword_match=True)\n\n        if not results:\n            print(\"\u26a0\ufe0f No matches found.\")\n            continue\n\n        print(\"\\n\ud83d\udd0e Matches:\")\n        for i, (bug, score) in enumerate(results, 1):\n            print(f\"{i}. {bug['summary']} ({bug['platform']})\")\n            print(f\"   {bug['description'][:70]}...\")\n            print(f\"   Score: {score:.4f}\\n\")\n\n        try:\n            choice = int(input(\"Select issue number to view full defect: \")) - 1\n            if choice < 0 or choice >= len(results):\n                print(\" Invalid choice.\")\n                continue\n        except ValueError:\n            print(\" Please enter a valid number.\")\n            continue\n\n        selected = results[choice][0]\n\n        print(\"\\n\ud83d\udccc Full Defect Description:\")\n        print(selected[\"description\"])\n\n        severity, priority = suggest_severity_priority(selected)\n        print(severity)\n        print(priority)\n        print(\"-\" * 80)\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "language": "python",
      "functions": [
        {
          "name": "search_bugs",
          "line": 24,
          "args": [
            "query",
            "k",
            "keyword_match"
          ],
          "docstring": "Search FAISS, optionally augment with keyword matches.",
          "decorators": []
        },
        {
          "name": "suggest_severity_priority",
          "line": 46,
          "args": [
            "bug"
          ],
          "docstring": "Heuristic rules to guess severity and priority.",
          "decorators": []
        },
        {
          "name": "main",
          "line": 63,
          "args": [],
          "docstring": null,
          "decorators": []
        }
      ],
      "classes": [],
      "imports": [
        "faiss",
        "json",
        "numpy",
        "sentence_transformers.SentenceTransformer"
      ],
      "complexity": "low",
      "timestamp": "2025-10-20T09:40:49.549723"
    },
    "7ec0f28af40d8ac16b63490487b63c88": {
      "filename": "app.py",
      "code": "import os, json, tomli\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom dotenv import load_dotenv\n\n\nfrom .schemas import EnhanceRequest, EnhanceResponse\nfrom .rag import Retriever\nfrom .llm import render_prompt, generate\nfrom .jira_api import update_issue_description\n\n\nload_dotenv()\n\n\nwith open(\"config.toml\", \"rb\") as f:\n\tCFG = tomli.load(f)\n\n\nretriever = Retriever(CFG)\napp = FastAPI(title=\"Defect Enhancer POC\")\n\n\n\n\n@app.post(\"/enhance\", response_model=EnhanceResponse)\ndef enhance(req: EnhanceRequest):\n\tctx = retriever.search(req.vague_text, k=req.top_k)\n\tprompt = render_prompt(\n\t\tCFG[\"prompt\"][\"template_path\"],\n\t\tvague_text=req.vague_text,\n\t\tproject_key=req.project_key,\n\t\tcontext_docs=ctx\n\t)\n\tenhanced = generate(prompt)\n\n\tupdated_key = None\n\tif req.update_jira and req.issue_key:\n\t\ttry:\n\t\t\tupdate_issue_description(req.issue_key, enhanced)\n\t\t\tupdated_key = req.issue_key\n\t\texcept Exception as e:\n\t\t\tenhanced += f\"\\n\\n[Note: Failed to update Jira: {e}]\"\n\n\treturn EnhanceResponse(enhanced=enhanced, context=ctx, updated_issue_key=updated_key)",
      "language": "python",
      "functions": [
        {
          "name": "enhance",
          "line": 27,
          "args": [
            "req"
          ],
          "docstring": null,
          "decorators": []
        }
      ],
      "classes": [],
      "imports": [
        "os",
        "json",
        "tomli",
        "fastapi.FastAPI",
        "pydantic.BaseModel",
        "dotenv.load_dotenv",
        "schemas.EnhanceRequest",
        "schemas.EnhanceResponse",
        "rag.Retriever",
        "llm.render_prompt",
        "llm.generate",
        "jira_api.update_issue_description"
      ],
      "complexity": "low",
      "timestamp": "2025-10-20T09:40:49.549723"
    },
    "6cfce0b246085759049ee73add1af01d": {
      "filename": "jira_api.py",
      "code": "import os\nimport requests\nfrom dotenv import load_dotenv\n\n\nload_dotenv()\n\n\nBASE = os.getenv(\"JIRA_BASE_URL\")\nEMAIL = os.getenv(\"JIRA_EMAIL\")\nTOKEN = os.getenv(\"JIRA_API_TOKEN\")\n\n\nHEADERS = {\"Accept\": \"application/json\", \"Content-Type\": \"application/json\"}\nAUTH = (EMAIL, TOKEN)\n\n\n\n\ndef update_issue_description(issue_key: str, description: str):\n\turl = f\"{BASE}/rest/api/3/issue/{issue_key}\"\n\tpayload = {\"fields\": {\"description\": description}}\n\tr = requests.put(url, headers=HEADERS, auth=AUTH, json=payload)\n\tr.raise_for_status()\n\treturn True",
      "language": "python",
      "functions": [
        {
          "name": "update_issue_description",
          "line": 20,
          "args": [
            "issue_key",
            "description"
          ],
          "docstring": null,
          "decorators": []
        }
      ],
      "classes": [],
      "imports": [
        "os",
        "requests",
        "dotenv.load_dotenv"
      ],
      "complexity": "low",
      "timestamp": "2025-10-20T09:40:49.549723"
    },
    "1bd4f7c0f7c8d0181e1db678c1b446e8": {
      "filename": "llm.py",
      "code": "import os\n# return t.render(vague_text=vague_text, project_key=project_key, context_docs=context_docs)\n\n\n# ---------- Providers ----------\n\n\ndef gen_openai(prompt: str):\n\timport requests, json\n\tbase = os.getenv(\"OPENAI_BASE_URL\", \"https://api.openai.com/v1\")\n\tmodel = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n\theaders = {\"Authorization\": f\"Bearer {os.getenv('OPENAI_API_KEY')}\", \"Content-Type\": \"application/json\"}\n\tpayload = {\"model\": model, \"messages\": [{\"role\": \"user\", \"content\": prompt}], \"temperature\": 0.2}\n\tr = requests.post(f\"{base}/chat/completions\", headers=headers, json=payload)\n\tr.raise_for_status()\n\treturn r.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n\n\n\n\ndef gen_ollama(prompt: str):\n\timport requests\n\tbase = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n\tmodel = os.getenv(\"OLLAMA_MODEL\", \"llama3\")\n\tr = requests.post(f\"{base}/api/generate\", json={\"model\": model, \"prompt\": prompt, \"options\": {\"temperature\": 0.2}})\n\tr.raise_for_status()\n\ttxt = \"\"\n\tfor line in r.iter_lines():\n\t\tif not line:\n\t\t\tcontinue\n\t\ttry:\n\t\t\tpart = line.decode(\"utf-8\")\n\t\texcept Exception:\n\t\t\tpart = line\n\t\ttxt += part\n\t# Ollama returns JSONL; a simpler approach is to call without stream, but many builds stream by default\n\t# For POC simplicity, call non-stream:\n\tr = requests.post(f\"{base}/api/generate\", json={\"model\": model, \"prompt\": prompt, \"stream\": False, \"options\": {\"temperature\": 0.2}})\n\tr.raise_for_status()\n\treturn r.json().get(\"response\", \"\").strip()\n\n\n\n\ndef gen_huggingface_t5(prompt: str):\n\tfrom transformers import T5ForConditionalGeneration, T5TokenizerFast\n\tpath = os.getenv(\"HF_MODEL_PATH\", \"models/t5-lora-domain\")\n\ttok = T5TokenizerFast.from_pretrained(path)\n\tmdl = T5ForConditionalGeneration.from_pretrained(path)\n\tids = tok(prompt, return_tensors=\"pt\").input_ids\n\tout = mdl.generate(ids, max_new_tokens=400)\n\treturn tok.decode(out[0], skip_special_tokens=True).strip()\n\n\n\n\ndef generate(prompt: str) -> str:\n\tif PROVIDER == \"openai\":\n\t\treturn gen_openai(prompt)\n\telif PROVIDER == \"huggingface\":\n\t\treturn gen_huggingface_t5(prompt)\n\telse:\n\t\treturn gen_ollama(prompt)",
      "language": "python",
      "functions": [
        {
          "name": "gen_openai",
          "line": 8,
          "args": [
            "prompt"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "gen_ollama",
          "line": 21,
          "args": [
            "prompt"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "gen_huggingface_t5",
          "line": 45,
          "args": [
            "prompt"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "generate",
          "line": 57,
          "args": [
            "prompt"
          ],
          "docstring": null,
          "decorators": []
        }
      ],
      "classes": [],
      "imports": [
        "os",
        "requests",
        "json",
        "requests",
        "transformers.T5ForConditionalGeneration",
        "transformers.T5TokenizerFast"
      ],
      "complexity": "low",
      "timestamp": "2025-10-20T09:40:49.549723"
    },
    "41c4798301d388ee78193f8f191752f4": {
      "filename": "rag.py",
      "code": "import os, json\nimport numpy as np\nimport faiss\nfrom sentence_transformers import SentenceTransformer\n\n\nclass Retriever:\n\tdef __init__(self, cfg):\n\t\tself.model = SentenceTransformer(cfg.get(\"index\", {}).get(\"embed_model\", \"sentence-transformers/all-MiniLM-L6-v2\"))\n\t\tself.index = faiss.read_index(cfg[\"index\"][\"faiss_path\"]) if os.path.exists(cfg[\"index\"][\"faiss_path\"]) else None\n\t\twith open(\"index/meta.json\", \"r\", encoding=\"utf-8\") as f:\n\t\t\tself.meta = json.load(f)\n\n\tdef search(self, query: str, k: int = 5):\n\t\tif self.index is None:\n\t\t\treturn []\n\t\tq = self.model.encode([query], normalize_embeddings=True)\n\t\tD, I = self.index.search(q, k)\n\t\tresults = []\n\t\tfor score, idx in zip(D[0].tolist(), I[0].tolist()):\n\t\t\tif idx == -1:\n\t\t\t\tcontinue\n\t\t\tm = self.meta[idx]\n\t\t\tm[\"score\"] = float(score)\n\t\t\tresults.append(m)\n\t\treturn results",
      "language": "python",
      "functions": [
        {
          "name": "__init__",
          "line": 8,
          "args": [
            "self",
            "cfg"
          ],
          "docstring": null,
          "decorators": []
        },
        {
          "name": "search",
          "line": 14,
          "args": [
            "self",
            "query",
            "k"
          ],
          "docstring": null,
          "decorators": []
        }
      ],
      "classes": [
        {
          "name": "Retriever",
          "line": 7,
          "methods": [
            "__init__",
            "search"
          ],
          "docstring": null,
          "bases": []
        }
      ],
      "imports": [
        "os",
        "json",
        "numpy",
        "faiss",
        "sentence_transformers.SentenceTransformer"
      ],
      "complexity": "low",
      "timestamp": "2025-10-20T09:40:49.549723"
    },
    "6b8abf84533dc30d89f3c82cddd6b0b9": {
      "filename": "schemas.py",
      "code": "from pydantic import BaseModel\nfrom typing import List, Optional\n\n\nclass EnhanceRequest(BaseModel):\n\tproject_key: str\n\tvague_text: str\n\tissue_key: Optional[str] = None\n\tupdate_jira: bool = False\n\ttop_k: int = 5\n\n\nclass EnhanceResponse(BaseModel):\n\tenhanced: str\n\tcontext: List[dict]\n\tupdated_issue_key: Optional[str] = None",
      "language": "python",
      "functions": [],
      "classes": [
        {
          "name": "EnhanceRequest",
          "line": 5,
          "methods": [],
          "docstring": null,
          "bases": [
            "BaseModel"
          ]
        },
        {
          "name": "EnhanceResponse",
          "line": 13,
          "methods": [],
          "docstring": null,
          "bases": [
            "BaseModel"
          ]
        }
      ],
      "imports": [
        "pydantic.BaseModel",
        "typing.List",
        "typing.Optional"
      ],
      "complexity": "low",
      "timestamp": "2025-10-20T09:40:49.549723"
    }
  },
  "embeddings": {
    "e1ff932aca5433c0e1544c59d1e72036": {
      "config": 17,
      "py": 2,
      "create": 3,
      "directories": 3,
      "get": 15,
      "model": 3,
      "file": 3,
      "rag": 3,
      "privacy": 3,
      "validate": 3,
      "environment": 3,
      "info": 3,
      "medicalassistantconfig": 3,
      "os": 1,
      "typing": 2,
      "dict": 1,
      "any": 1,
      "pathlib": 1,
      "path": 1,
      "platform": 1,
      "torch": 1,
      "requests": 1,
      "python": 5
    },
    "22b3c9dae6596bc00887f277c24aa3ce": {
      "main": 5,
      "py": 2,
      "render": 15,
      "data": 3,
      "upload": 3,
      "tab": 15,
      "analysis": 3,
      "rag": 4,
      "chat": 3,
      "add": 3,
      "helper": 3,
      "methods": 3,
      "to": 3,
      "assistant": 3,
      "status": 3,
      "init": 3,
      "initialize": 3,
      "components": 3,
      "auto": 3,
      "detect": 3,
      "content": 3,
      "type": 3,
      "generate": 15,
      "contextual": 3,
      "query": 3,
      "intelligent": 3,
      "response": 12,
      "skin": 3,
      "lesion": 3,
      "image": 3,
      "document": 3,
      "extract": 3,
      "key": 3,
      "findings": 3,
      "multimodalmedicalassistant": 3,
      "streamlit": 1,
      "os": 1,
      "tempfile": 1,
      "pandas": 1,
      "pathlib": 1,
      "path": 1,
      "time": 1,
      "json": 1,
      "datetime": 2,
      "src": 6,
      "models": 3,
      "llama_processor": 1,
      "llamaprocessor": 1,
      "image_processor": 1,
      "medicalimageprocessor": 1,
      "text_processor": 1,
      "medicaltextprocessor": 1,
      "retrieval_system": 1,
      "medicalragsystem": 1,
      "utils": 2,
      "privacy_utils": 1,
      "datadeidentifier": 1,
      "file_handler": 1,
      "filehandler": 1,
      "python": 5
    },
    "c6ee1dc677572dc360c1da2498e4d449": {
      "setup": 11,
      "py": 2,
      "main": 3,
      "init": 3,
      "print": 6,
      "header": 3,
      "check": 3,
      "python": 5,
      "version": 3,
      "install": 3,
      "base": 3,
      "requirements": 3,
      "ollama": 3,
      "pull": 3,
      "llama": 3,
      "model": 3,
      "download": 3,
      "spacy": 4,
      "models": 6,
      "medical": 3,
      "create": 9,
      "directories": 3,
      "sample": 3,
      "data": 3,
      "environment": 3,
      "file": 3,
      "verify": 3,
      "installation": 3,
      "next": 3,
      "steps": 3,
      "run": 3,
      "medicalassistantsetup": 3,
      "subprocess": 1,
      "sys": 1,
      "os": 1,
      "platform": 1,
      "pathlib": 1,
      "path": 1,
      "requests": 1,
      "zipfile": 1,
      "shutil": 1,
      "time": 1
    },
    "d3e206534c409ee71071ccb97a3cde4f": {
      "skin": 20,
      "lesion": 8,
      "optimizer": 2,
      "py": 2,
      "optimize": 3,
      "chat": 3,
      "handle": 3,
      "timeout": 3,
      "init": 3,
      "quick": 9,
      "analysis": 3,
      "extract": 3,
      "features": 3,
      "simplify": 3,
      "query": 6,
      "create": 3,
      "default": 3,
      "generate": 6,
      "response": 6,
      "fallback": 3,
      "skinlesionoptimizer": 3,
      "streamlit": 1,
      "time": 1,
      "datetime": 2,
      "asyncio": 1,
      "threading": 1,
      "python": 5
    },
    "7af70ab94ac0a14e91d47c8f89ff768a": {
      "test": 29,
      "medical": 2,
      "assistant": 2,
      "py": 2,
      "main": 3,
      "init": 3,
      "setup": 3,
      "logging": 4,
      "create": 3,
      "sample": 3,
      "data": 3,
      "configuration": 3,
      "llama": 3,
      "processor": 9,
      "text": 3,
      "image": 3,
      "rag": 4,
      "system": 3,
      "privacy": 3,
      "utils": 5,
      "file": 3,
      "handler": 3,
      "integration": 3,
      "generate": 3,
      "report": 3,
      "run": 3,
      "all": 3,
      "tests": 3,
      "medicalassistanttester": 3,
      "os": 1,
      "sys": 1,
      "tempfile": 1,
      "pathlib": 1,
      "path": 1,
      "datetime": 2,
      "json": 1,
      "src": 6,
      "models": 3,
      "llama_processor": 1,
      "llamaprocessor": 1,
      "image_processor": 1,
      "medicalimageprocessor": 1,
      "text_processor": 1,
      "medicaltextprocessor": 1,
      "retrieval_system": 1,
      "medicalragsystem": 1,
      "privacy_utils": 1,
      "datadeidentifier": 1,
      "file_handler": 1,
      "filehandler": 1,
      "config": 1,
      "medicalassistantconfig": 1,
      "python": 5
    },
    "0f32d523c0e87884b26c69c256d1a617": {
      "test": 2,
      "app": 2,
      "py": 2,
      "search": 3,
      "bugs": 3,
      "suggest": 3,
      "severity": 3,
      "priority": 3,
      "faiss": 1,
      "json": 1,
      "numpy": 1,
      "sentence_transformers": 1,
      "sentencetransformer": 1,
      "python": 5
    },
    "190471e9f58719cc2d1232c96bbe014a": {
      "reporting": 2,
      "py": 2,
      "extract": 3,
      "gzip": 3,
      "to": 3,
      "csv": 7,
      "delete": 6,
      "lines": 3,
      "before": 3,
      "timestamp": 3,
      "clean": 3,
      "numeric": 3,
      "columns": 3,
      "cleanse": 3,
      "data": 6,
      "inspect": 3,
      "problematic": 3,
      "file": 3,
      "process": 6,
      "server": 6,
      "plot": 3,
      "graph": 3,
      "create": 9,
      "detailed": 3,
      "view": 3,
      "main": 3,
      "window": 3,
      "unwanted": 3,
      "files": 3,
      "directory": 3,
      "on": 9,
      "pick": 3,
      "click": 6,
      "closing": 3,
      "handler": 3,
      "pandas": 1,
      "matplotlib": 4,
      "pyplot": 1,
      "os": 1,
      "numpy": 1,
      "glob": 1,
      "subprocess": 1,
      "datetime": 2,
      "dates": 1,
      "logging": 1,
      "tkinter": 2,
      "ttk": 1,
      "backends": 2,
      "backend_tkagg": 2,
      "figurecanvastkagg": 1,
      "navigationtoolbar2tk": 1,
      "python": 5
    },
    "947b6a089925a763355ebf51a881ae7d": {
      "reporting": 2,
      "n": 2,
      "py": 2,
      "extract": 3,
      "gzip": 3,
      "to": 3,
      "csv": 7,
      "delete": 6,
      "lines": 3,
      "before": 3,
      "timestamp": 3,
      "clean": 3,
      "numeric": 3,
      "columns": 3,
      "cleanse": 3,
      "data": 6,
      "inspect": 3,
      "problematic": 3,
      "file": 3,
      "process": 6,
      "server": 6,
      "plot": 3,
      "graph": 3,
      "create": 9,
      "detailed": 3,
      "view": 3,
      "main": 3,
      "window": 3,
      "unwanted": 3,
      "files": 3,
      "directory": 3,
      "on": 9,
      "pick": 3,
      "click": 6,
      "closing": 3,
      "handler": 3,
      "pandas": 1,
      "matplotlib": 4,
      "pyplot": 1,
      "os": 1,
      "numpy": 1,
      "glob": 1,
      "subprocess": 1,
      "datetime": 2,
      "dates": 1,
      "logging": 1,
      "tkinter": 2,
      "ttk": 1,
      "backends": 2,
      "backend_tkagg": 2,
      "figurecanvastkagg": 1,
      "navigationtoolbar2tk": 1,
      "python": 5
    },
    "5d2324e347065282240cc919fe367a3b": {
      "test": 2,
      "app": 2,
      "py": 2,
      "search": 3,
      "bugs": 3,
      "suggest": 3,
      "severity": 3,
      "priority": 3,
      "faiss": 1,
      "json": 1,
      "numpy": 1,
      "sentence_transformers": 1,
      "sentencetransformer": 1,
      "python": 5
    },
    "7a43bdd85ee17662f7fdb0dcbbe80e2b": {
      "build": 2,
      "index": 2,
      "py": 2,
      "iter": 3,
      "jsonl": 3,
      "os": 1,
      "json": 1,
      "argparse": 1,
      "numpy": 1,
      "faiss": 1,
      "sentence_transformers": 1,
      "sentencetransformer": 1,
      "python": 5
    },
    "1d0d5d5e5649d8da5a57fb29cd3eb884": {
      "build": 2,
      "index": 2,
      "dummy": 2,
      "py": 2,
      "main": 3,
      "sentence_transformers": 1,
      "sentencetransformer": 1,
      "faiss": 1,
      "json": 1,
      "numpy": 1,
      "os": 1,
      "python": 5
    },
    "90c61a0bcfe6a4de787aec99d068ee8a": {
      "fine": 2,
      "tune": 2,
      "lora": 2,
      "t5": 2,
      "py": 2,
      "jsonl": 3,
      "to": 3,
      "hf": 3,
      "gen": 3,
      "preprocess": 3,
      "json": 1,
      "datasets": 1,
      "load_dataset": 1,
      "transformers": 5,
      "t5forconditionalgeneration": 1,
      "t5tokenizerfast": 1,
      "datacollatorforseq2seq": 1,
      "trainer": 1,
      "trainingarguments": 1,
      "peft": 3,
      "loraconfig": 1,
      "get_peft_model": 1,
      "tasktype": 1,
      "argparse": 1,
      "python": 5
    },
    "a2c05dc58db11a83734cb15e0499cee2": {
      "ingest": 2,
      "jira": 2,
      "py": 2,
      "fetch": 3,
      "issues": 3,
      "os": 1,
      "json": 1,
      "argparse": 1,
      "requests": 1,
      "dotenv": 1,
      "load_dotenv": 1,
      "python": 5
    },
    "a4bed6001970c033925473d02f35504b": {
      "make": 2,
      "training": 2,
      "pairs": 2,
      "py": 2,
      "iter": 3,
      "jsonl": 3,
      "to": 3,
      "structured": 3,
      "json": 1,
      "argparse": 1,
      "python": 5
    },
    "f16230720fc80a1447662ee72b457b93": {
      "test": 2,
      "lora": 2,
      "generation": 2,
      "py": 2,
      "os": 1,
      "transformers": 2,
      "t5forconditionalgeneration": 1,
      "t5tokenizerfast": 1,
      "python": 5
    },
    "461c0dd3749392dd7179d9ab33fa8d01": {
      "test": 2,
      "search": 5,
      "py": 2,
      "bugs": 3,
      "suggest": 3,
      "severity": 3,
      "priority": 3,
      "main": 3,
      "faiss": 1,
      "json": 1,
      "numpy": 1,
      "sentence_transformers": 1,
      "sentencetransformer": 1,
      "python": 5
    },
    "7ec0f28af40d8ac16b63490487b63c88": {
      "app": 2,
      "py": 2,
      "enhance": 3,
      "os": 1,
      "json": 1,
      "tomli": 1,
      "fastapi": 2,
      "pydantic": 1,
      "basemodel": 1,
      "dotenv": 1,
      "load_dotenv": 1,
      "schemas": 2,
      "enhancerequest": 1,
      "enhanceresponse": 1,
      "rag": 1,
      "retriever": 1,
      "llm": 2,
      "render_prompt": 1,
      "generate": 1,
      "jira_api": 1,
      "update_issue_description": 1,
      "python": 5
    },
    "6cfce0b246085759049ee73add1af01d": {
      "jira": 2,
      "api": 2,
      "py": 2,
      "update": 3,
      "issue": 3,
      "description": 3,
      "os": 1,
      "requests": 1,
      "dotenv": 1,
      "load_dotenv": 1,
      "python": 5
    },
    "1bd4f7c0f7c8d0181e1db678c1b446e8": {
      "llm": 2,
      "py": 2,
      "gen": 9,
      "openai": 3,
      "ollama": 3,
      "huggingface": 3,
      "t5": 3,
      "generate": 3,
      "os": 1,
      "requests": 2,
      "json": 1,
      "transformers": 2,
      "t5forconditionalgeneration": 1,
      "t5tokenizerfast": 1,
      "python": 5
    },
    "41c4798301d388ee78193f8f191752f4": {
      "rag": 2,
      "py": 2,
      "init": 3,
      "search": 3,
      "retriever": 3,
      "os": 1,
      "json": 1,
      "numpy": 1,
      "faiss": 1,
      "sentence_transformers": 1,
      "sentencetransformer": 1,
      "python": 5
    },
    "6b8abf84533dc30d89f3c82cddd6b0b9": {
      "schemas": 2,
      "py": 2,
      "enhancerequest": 3,
      "enhanceresponse": 3,
      "pydantic": 1,
      "basemodel": 1,
      "typing": 2,
      "list": 1,
      "optional": 1,
      "python": 5
    }
  },
  "metadata": {
    "e1ff932aca5433c0e1544c59d1e72036": {
      "filename": "config.py",
      "language": "python",
      "num_functions": 7,
      "num_classes": 1,
      "loc": 263
    },
    "22b3c9dae6596bc00887f277c24aa3ce": {
      "filename": "main.py",
      "language": "python",
      "num_functions": 16,
      "num_classes": 1,
      "loc": 883
    },
    "c6ee1dc677572dc360c1da2498e4d449": {
      "filename": "setup.py",
      "language": "python",
      "num_functions": 15,
      "num_classes": 1,
      "loc": 406
    },
    "d3e206534c409ee71071ccb97a3cde4f": {
      "filename": "skin_lesion_optimizer.py",
      "language": "python",
      "num_functions": 9,
      "num_classes": 1,
      "loc": 201
    },
    "7af70ab94ac0a14e91d47c8f89ff768a": {
      "filename": "test_medical_assistant.py",
      "language": "python",
      "num_functions": 14,
      "num_classes": 1,
      "loc": 640
    },
    "0f32d523c0e87884b26c69c256d1a617": {
      "filename": "test_app.py",
      "language": "python",
      "num_functions": 2,
      "num_classes": 0,
      "loc": 113
    },
    "190471e9f58719cc2d1232c96bbe014a": {
      "filename": "reporting.py",
      "language": "python",
      "num_functions": 15,
      "num_classes": 0,
      "loc": 344
    },
    "947b6a089925a763355ebf51a881ae7d": {
      "filename": "reporting_n.py",
      "language": "python",
      "num_functions": 15,
      "num_classes": 0,
      "loc": 363
    },
    "5d2324e347065282240cc919fe367a3b": {
      "filename": "test_app.py",
      "language": "python",
      "num_functions": 2,
      "num_classes": 0,
      "loc": 113
    },
    "7a43bdd85ee17662f7fdb0dcbbe80e2b": {
      "filename": "build_index.py",
      "language": "python",
      "num_functions": 1,
      "num_classes": 0,
      "loc": 46
    },
    "1d0d5d5e5649d8da5a57fb29cd3eb884": {
      "filename": "build_index_dummy.py",
      "language": "python",
      "num_functions": 1,
      "num_classes": 0,
      "loc": 37
    },
    "90c61a0bcfe6a4de787aec99d068ee8a": {
      "filename": "fine_tune_lora_t5.py",
      "language": "python",
      "num_functions": 3,
      "num_classes": 0,
      "loc": 86
    },
    "a2c05dc58db11a83734cb15e0499cee2": {
      "filename": "ingest_jira.py",
      "language": "python",
      "num_functions": 1,
      "num_classes": 0,
      "loc": 81
    },
    "a4bed6001970c033925473d02f35504b": {
      "filename": "make_training_pairs.py",
      "language": "python",
      "num_functions": 2,
      "num_classes": 0,
      "loc": 77
    },
    "f16230720fc80a1447662ee72b457b93": {
      "filename": "test_lora_generation.py",
      "language": "python",
      "num_functions": 0,
      "num_classes": 0,
      "loc": 15
    },
    "461c0dd3749392dd7179d9ab33fa8d01": {
      "filename": "test_search.py",
      "language": "python",
      "num_functions": 3,
      "num_classes": 0,
      "loc": 104
    },
    "7ec0f28af40d8ac16b63490487b63c88": {
      "filename": "app.py",
      "language": "python",
      "num_functions": 1,
      "num_classes": 0,
      "loc": 45
    },
    "6cfce0b246085759049ee73add1af01d": {
      "filename": "jira_api.py",
      "language": "python",
      "num_functions": 1,
      "num_classes": 0,
      "loc": 25
    },
    "1bd4f7c0f7c8d0181e1db678c1b446e8": {
      "filename": "llm.py",
      "language": "python",
      "num_functions": 4,
      "num_classes": 0,
      "loc": 63
    },
    "41c4798301d388ee78193f8f191752f4": {
      "filename": "rag.py",
      "language": "python",
      "num_functions": 2,
      "num_classes": 1,
      "loc": 26
    },
    "6b8abf84533dc30d89f3c82cddd6b0b9": {
      "filename": "schemas.py",
      "language": "python",
      "num_functions": 0,
      "num_classes": 2,
      "loc": 16
    }
  }
}